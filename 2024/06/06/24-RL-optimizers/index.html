<!DOCTYPE html>
<html lang="en">

  
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>The Sample Complexity of RL-based Optimizers for Financial Applications (Part I) · Arnab Sircar</title>

<link rel="stylesheet" href="https://a-sircar1.github.io/assets/main.css">
<link rel="alternate" type="application/atom+xml" title="Arnab Sircar" href="https://a-sircar1.github.io/feed.xml">


<script type="text/javascript" async
  src="https://polyfill.io/v3/polyfill.min.js?features=es6">
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
</script>


<style>
  .hero {
    display: flex;
    flex-wrap: wrap;
    align-items: center;
    justify-content: space-between;
    gap: 1.75rem;
    margin-bottom: 1.75rem;
  }

  .hero-text {
    flex: 1 1 280px;
  }

  .hero-photo {
    flex: 0 0 320px;    
    display: flex;
    justify-content: center;
  } 

  .hero-photo img {
    max-width: 320px;
    width: 100%;
    height: auto;
    border-radius: 8px;
    border: 3px solid #e0e0e0;
  }

  @media (max-width: 640px) {
    .hero-photo { flex-basis: 200px; }
    .hero-photo img { max-width: 200px; }
  }


  /* Card-style boxes ------------------------------------------------------ */

  .card {
    background-color: #fafafa;
    border-radius: 8px;
    border: 1px solid #e0e0e0;
    padding: 1.1rem 1.6rem;   /* slightly tighter than before */
    margin-bottom: 1.6rem;
  }

  .card h2 {
    margin-top: 0;
  }

  .section-divider {
    margin: 2rem 0 1.6rem;
    border: 0;
    border-top: 1px solid #dddddd;
  }

  /* News / Recent updates ------------------------------------------------- */

  .news-updates {
    background-color: #f9f9f9;
  }

  .news-list {
    list-style: none;
    padding-left: 0;
    margin: 0;
  }

  .news-list li + li {
    margin-top: 0.6rem;
  }

  .news-meta {
    font-size: 0.9em;
    color: #777;
    margin: 0.05rem 0;
  }

  .news-excerpt {
    margin: 0.15rem 0 0;
    font-size: 0.9em;
    color: #555;
  }

  /* Research section ------------------------------------------------------ */

    /* Research section ------------------------------------------------------ */

    .research-section {
    background-color: #ffffff;
  }

  /* Each entry is tighter, with a subtle divider between items */
  .research-item {
    padding: 0.25rem 0;
  }

  .research-item + .research-item {
    margin-top: 0.4rem;
    padding-top: 0.55rem;
    border-top: 1px solid #e5e5e5;
  }

  .research-title {
    margin: 0 0 0.05rem;
    font-size: 1.02rem;
  }

  .research-authors {
    margin: 0;
    font-size: 0.9em;
    color: #666;
  }

  .research-meta {
    margin: 0.1rem 0 0;
    font-size: 0.85em;
    color: #777;
  }

  .award-badge {
    font-weight: 600;
  }

  .award-badge .award-text {
    color: #b8860b;  /* dark gold */
  }

  .research-abstract {
    margin-top: 0.25rem;
  }

  .research-abstract summary {
    cursor: pointer;
    display: inline-block;
    padding: 0.18rem 0.65rem;
    border-radius: 999px;
    background-color: #005f99;
    color: #ffffff;
    font-size: 0.78rem;
    font-weight: 600;
    list-style: none;
  }

  .research-abstract[open] summary {
    background-color: #004474;
  }

  .research-abstract summary::-webkit-details-marker {
    display: none;
  }

  .abstract-body {
    margin-top: 0.55rem;
    font-size: 0.95em;
    color: #444;
  }

  #research {
    scroll-margin-top: 60px;
  }


    /* Recent posts ---------------------------------------------------------- */

    .recent-posts {
    background-color: #ffffff;
    padding-top: 0.9rem;
    padding-bottom: 0.9rem;   /* tighter vertical padding */
  }

  .recent-posts h2 {
    margin-bottom: 0.6rem;    /* less gap under "Recent Blog Posts" */
  }

  .recent-posts .post-list {
    list-style: none;
    padding: 0;
    margin: 0;
  }

  .recent-posts .post-list li {
    margin: 0;
  }

  /* spacing + subtle divider between posts */
  .recent-posts .post-list li + li {
    border-top: 1px solid #e5e5e5;
    margin-top: 0.4rem;
    padding-top: 0.4rem;
  }

  .recent-post-item {
    padding: 0.2rem 0;        /* less padding in each item */
  }

  .recent-post-title {
    font-size: 1em;
    font-weight: 600;
    margin: 0;
    color: #333;
  }

  .recent-post-link {
    text-decoration: none;
    color: #333;
    transition: color 0.2s;
  }

  .recent-post-link:hover {
    color: #007acc;
    text-decoration: underline;
  }

  .recent-post-excerpt {
    font-size: 0.9em;
    color: #555;
    margin: 0.08rem 0;        /* tighter gap above/below excerpt */
    line-height: 1.25;
  }

  .recent-post-meta {
    font-size: 0.85em;
    color: #777;
    margin: 0.02rem 0 0;      /* almost no extra space under excerpt */
    line-height: 1.1;
  }

  .more-posts {
    margin-top: 0.5rem;
    font-size: 0.9em;
  }

  .more-posts a {
    text-decoration: none;
    color: #007acc;
  }

  .more-posts a:hover {
    text-decoration: underline;
  }

  .award-badge {
    font-weight: 600;
  }

  .award-badge .award-text {
    color: #b8860b;  /* dark gold (DarkGoldenRod) */
  }

</style>
<style>
  /* table of contents style */
.toc-wrap { background:#fafafa; border:1px solid #e0e0e0; border-radius:8px; padding:0.75rem 1rem; margin:1rem 0; }
.toc-wrap summary { cursor:pointer; margin-bottom:0.5rem; }
.toc-wrap ul { margin:0.3rem 0 0 1rem; }
.toc-wrap a { text-decoration:none; }
.toc-wrap a:hover { text-decoration:underline; }
</style>

<style>
  /* image caption format */
  .img-center { text-align:center; margin: 1rem auto; }
  .img-center img { display:block; margin:0 auto; max-width:100%; height:auto; }
  .img-center figcaption { margin-top:.5rem; font-size:.9em; color:#666; }
</style>


<style>
  sup.proof-inline {
    font-size:.75em; vertical-align:super; margin-left:.25rem; white-space:nowrap;
  }
  sup.proof-inline a { text-decoration:none; border-bottom:1px dotted currentColor; }
  sup.proof-inline a:hover { text-decoration:underline; }

  details.proof-box {
    margin:.6rem 0 1rem;
    padding:0;           
    border:0;             
    background:transparent;
  }
  details.proof-box[open] {
    padding:.9rem 1rem;
    border:1px solid #e0e0e0;
    border-radius:8px;
    background:#fafafa;
  }

  details.proof-box > summary {
    position:absolute !important;
    width:1px; height:1px; padding:0; margin:-1px;
    overflow:hidden; clip:rect(0,0,0,0); white-space:nowrap; border:0;
  }
</style>


<script>
  document.addEventListener('DOMContentLoaded', function () {
    document.querySelectorAll('a[data-proof]').forEach(function (link) {
      var id = link.getAttribute('data-proof');
      var box = document.getElementById(id);
      if (!box) return;

      if (box.tagName.toLowerCase() === 'details') box.open = false;

      link.setAttribute('aria-expanded', 'false');
      link.textContent = 'See proof';

      link.addEventListener('click', function (e) {
        e.preventDefault();
        if (box.tagName.toLowerCase() === 'details') {
          var isOpen = box.hasAttribute('open');
          box.open = !isOpen;
          link.textContent = isOpen ? 'See proof' : 'Hide proof';
          link.setAttribute('aria-expanded', String(!isOpen));
          if (!isOpen) {
            try { box.scrollIntoView({ behavior: 'smooth', block: 'center' }); } catch (_) {}
          }
        }
      });
    });
  });
</script>



    
    

  
  

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Arnab Sircar</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/#research">Projects</a>
      
        
        <a class="page-link" href="/archives/">Blog</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">The Sample Complexity of RL-based Optimizers for Financial Applications (Part I)</h1>
    
    <p class="post-meta"><time datetime="2024-06-06T06:10:56+00:00" itemprop="datePublished">Jun 6, 2024</time> •
  
    
    
      
    
      
        <a href="/categories/finance/">finance</a>,
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
    
      
        <a href="/categories/optimization/">optimization</a>,
      
    
      
    
      
    
      
    
  
    
    
      
        <a href="/categories/machine-learning/">machine-learning</a>
      
    
      
    
      
    
      
    
      
    
      
    
  



</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Earlier this year in the spring, I got the chance to take part in Penn’s Directed Reading Program (DRP) in the Math department. I wanted to explore machine learning for time-series data and sequential learning, and eventually settled on reinforcement learning (RL)– a topic I had heard a lot about but knew very little. I knew quantitative researchers and practioners were attempting to leverage reinforcement learning for all sorts of problems in financial forecasting and optimization, but that’s about all I had to go off. So, I decided to spend sometime and take the opportunity to obtain valuable guidance in my efforts to understand RL.</p>

<p>I read through a couple of chapters in Barto and Sutton’s textbook and then decided to take a look at some of the papers for applied RL in finance. These were interesting, but I kept gravitating back to more theory-oriented RL work– not really because I see myself as a theory person, but because that perspective made it a lot easier for me to connect the algorithms to the kinds of questions I care about in financial economics, like robustness, risk, and when the model’s assumptions actually line up with market data or real-world market problems. In my literature search, a piece that stood out to me was a BAIR article on “Learning to Optimize with RL,” which frames optimization algorithms themselves as policies learned via RL.<sup id="fnref:bair" role="doc-noteref"><a href="#fn:bair" class="footnote" rel="footnote">1</a></sup> In parallel, I worked through parts of an online RL theory book that develops the foundations of Markov decision processes, value functions, and convergence guarantees in a clean, abstract way.<sup id="fnref:rlbook" role="doc-noteref"><a href="#fn:rlbook" class="footnote" rel="footnote">2</a></sup></p>

<p>Together, these are the main readings that gave me a mental picture of RL that actually felt natural from a finance perspective: markets as sequential decision problems under uncertainty, algorithms as dynamic trading rules, and theory as a way to check on when those rules should behave sensibly. For the rest of this note, I build the model using the tools and inspiration from those works.</p>

<h3>Introduction</h3>

<p>In financial settings, reinforcement learning is often introduced as a way to learn trading strategies or hedging rules. Here I use the same language to think instead about learning an optimizer (as in Li et al.) for a family of financial optimization problems.<sup id="fnref:bair:1" role="doc-noteref"><a href="#fn:bair" class="footnote" rel="footnote">1</a></sup> I intentionally design a minimal set-up: (1) start from a one-period stochastic portfolio problem, (2) reinterpret a finite family of optimizers as actions of an RL agent, (3) derive a simple sample-complexity bound for selecting a nearly optimal optimizer, (4) and finally connect this to a small mean-variance optimization experiment where I empirically validate the theoretical result.</p>

<h3>Portfolio Setup and Mean-variance Utility</h3>

<p>Let’s take \(d \ge 1\) assets, and let the random vector of one-period returns be \(R \in \mathbb{R}^d\). A portfolio is a weight vector \(w \in \mathbb{R}^d\) that satisfies the budget and non-negativity constraints like so:</p>

\[\sum_{j=1}^d w_j = 1\]

\[w_j \ge 0 \quad  \forall j\]

<p>The random portfolio return is then \(X(w) = w^{\top} R\). The mean vector and covariance matrix of returns can be written as \(\mu = \mathbb{E}[R] \in \mathbb{R}^d\) and \(\Sigma = \operatorname{Cov}(R) \in \mathbb{R}^{d \times d}\).</p>

<p>Consider the standard mean-variance objective with risk-aversion parameter \(\lambda &gt; 0\):</p>

\[U(w) = \mathbb{E}[X(w)] - \frac{\lambda}{2} \operatorname{Var}(X(w))\]

<p>The expectation can be written explicitly as</p>

\[\begin{aligned}
\mathbb{E}[X(w)]
&amp;= \mathbb{E}[w^{\top} R] \\
&amp;= w^{\top} \mathbb{E}[R] \\
&amp;= w^{\top} \mu
\end{aligned}\]

<p>because \(w\) is deterministic. For the variance, start from the covariance definition \(\Sigma = \mathbb{E}[(R - \mu)(R - \mu)^{\top}]\). Define the centered scalar \(Y = w^{\top} R - w^{\top} \mu = w^{\top}(R - \mu)\), so that</p>

\[\operatorname{Var}(X(w)) = \mathbb{E}[Y^2]\]

<p>Now expand \(Y^2\) and rewrite it as a quadratic form like so:</p>

\[\begin{aligned}
Y^2
&amp;= \big(w^{\top}(R - \mu)\big)^2 \\
&amp;= \big(w^{\top}(R - \mu)\big)\big(w^{\top}(R - \mu)\big) \\
&amp;= w^{\top}(R - \mu)(R - \mu)^{\top} w
\end{aligned}\]

<p>Taking expectations and pulling out the deterministic weights gives the following:</p>

\[\mathbb{E}[Y^2]
= \mathbb{E}\!\big[w^{\top}(R - \mu)(R - \mu)^{\top} w\big]
= w^{\top} \mathbb{E}\!\big[(R - \mu)(R - \mu)^{\top}\big] w
= w^{\top} \Sigma w\]

<p>Thus the variance is \(\operatorname{Var}(X(w)) = w^{\top} \Sigma w\). Plugging in both of the pieces into the utility, I get:</p>

\[U(w) = w^{\top} \mu - \frac{\lambda}{2} w^{\top} \Sigma w\]

<p>For a fixed pair \((\mu,\Sigma)\), this is a concave quadratic function of \(w\) on the simplex of admissible portfolios \(\mathcal{W}\). Many portfolio problems can generally be seen as variants of this kind of form. For convenience, write \(U(w, \mu, \Sigma) = w^{\top} \mu - \frac{\lambda}{2} w^{\top} \Sigma w\) and think of \((\mu,\Sigma)\) as the parameters that define a particular optimization parameterization \(p = (\mu, \Sigma)\). The corresponding optimal utility is</p>

\[U^{\star}(p) = \max_{w \in \mathcal{W}} U(w, \mu, \Sigma)\]

<p>In practice, \(\mu\) and \(\Sigma\) are unknown as only samples can be obtained and not the population-level values. Different universes or constraints give different parameterizations \(p\). A meta-optimizer should perform well on average across a distribution of these kinds of parameterizations.</p>

<h3>A Finite Family of Optimizers as Actions</h3>

<p>Instead of tuning a single algorithm by hand, now imagine a finite family of \(K\) candidate optimizers,</p>

\[\mathcal{O} = \{O_1, O_2, \dots, O_K\}\]

<p>where each of the optimizers \(O_k\) takes as input data for a given parameterization \(p\) and outputs a portfolio \(w_k\). For example, \(O_1\) could be projected gradient ascent with a small step size, \(O_2\) could be the same algorithm with a larger step size, and this can go on. To connect this to RL cleanly, consider a simple sequential environment. Say there is an unknown “meta-distribution” \(\mathcal{P}\) over problem parameterizations \(p = (\mu,\Sigma)\). In each case \(i\), nature draws a fresh parameterization</p>

\[p_i = (\mu_i, \Sigma_i) \sim \mathcal{P}\]

<p>the agent chooses an action \(A_i \in \{1,\dots,K\}\), and action \(A_i = k\) means the optimizer \(O_k\) is applied to problem \(p_i\). Optimizer \(O_k\) outputs a portfolio \(w_{k,i} = O_k(p_i)\), and then out-of-sample (OOS) performance \(R_i = R_{k}(p_i)\) can be evaluated, where \(R_k(p)\) is some reward based on \(U(w_k, \mu, \Sigma)\).</p>

<p>For the analysis, the rewards are assumed bounded in \([0,1]\):</p>

\[0 \le R_k(p) \le 1 \quad \forall k, p\]

<p>Note that this is not restrictive– any utility that is real-values should be able to be clipped to \([a,b]\) and then rescaled linearly to \([0,1]\) without changing the ordering of the optimizers. The mean performance of the optimizer \(k\) under the meta-distribution \(\mathcal{P}\) is \(\mu_k = \mathbb{E}_{p \sim \mathcal{P}}[R_k(p)]\), and an index of a best optimizer is \(\mu_{k^{\star}} = \max_{1 \le k \le K} \mu_k\).</p>

<p>The meta-learning problem is then to use samples from cases across \(i\) to select an index \(\hat{k}\) such that \(\mu_{\hat{k}}\) is close to \(\mu_{k^{\star}}\). This is exactly a stochastic multi-armed bandit in the exploration setting. Each optimizer is an arm and then the reward of arm \(k\) is the random value \(R_k(p)\) when \(p \sim \mathcal{P}\), and \(\mu_k\) is its mean. The only difference from the classical bandit model is that there is a financial interpretation of the reward.</p>

<h3>A Simple Meta-algorithm and its Uniform Deviation Bound</h3>

<p>For the analysis, consider the following strategy. For each optimizer \(k\), collect \(n\) independent rewards \(R_{k,1}, R_{k,2}, \dots, R_{k,n}\), where each of the rewards \(R_{k,j}\) is generated by drawing a fresh problem parameterization \(p_{k,j} \sim \mathcal{P}\), running optimizer \(O_k\), and computing its bounded reward. The empirical mean reward of optimizer \(k\) is then the following:</p>

\[\hat{\mu}_k = \frac{1}{n} \sum_{j=1}^n R_{k,j}\]

<p>and at the end choose \(\hat{k} = \arg\max_{1 \le k \le K} \hat{\mu}_k\) with and break ties arbitarily. The question then is how large \(n\) must be so that we can be sure that, with high probability, the gap \(\mu_{k^{\star}} - \mu_{\hat{k}}\) is small. Use Hoeffding’s inequality for bounded random variables. For a fixed optimizer \(k\), the rewards \(R_{k,1},\dots,R_{k,n}\) are independent and lie in \([0,1]\). Writing \(\hat{\mu}_k = \frac{1}{n} \sum_{j=1}^n R_{k,j}\) and \(\mu_k = \mathbb{E}[R_{k,1}]\), Hoeffding’s inequality states that for every \(\varepsilon &gt; 0\),</p>

\[\mathbb{P}\big(\|\hat{\mu}_k - \mu_k\| \ge \varepsilon\big) \le 2 \exp(-2 n \varepsilon^2)\]

<p>For each \(k\), then define the deviation event \(E_k = \{ \|\hat{\mu}_k - \mu_k\| \ge \varepsilon \}\), so that \(\mathbb{P}(E_k) \le 2 \exp(-2 n \varepsilon^2)\). The event that some optimizer has a deviation at least \(\varepsilon\) is the union \(\bigcup_{k=1}^K E_k\), and by the union bound,</p>

\[\mathbb{P}\!\left(\bigcup_{k=1}^K E_k\right) \le 2 K \exp(-2 n \varepsilon^2)\]

<p>Equivalently, with probability at least \(1 - 2 K \exp(-2 n \varepsilon^2)\) there is the event</p>

\[\mathcal{E} = \{ \|\hat{\mu}_k - \mu_k\| &lt; \varepsilon \quad \forall k = \{1,\dots,K \}\]

<p>meaning that every empirical mean is within \(\varepsilon\) of its true mean.</p>

<p>On event \(\mathcal{E}\), the optimizer chosen by empirical means is at most \(2 \varepsilon\) worse than the best optimizer in terms of the true mean reward. To see this, fix an outcome where \(\mathcal{E}\) holds. By definition of \(k^{\star}\), \(\mu_{k^{\star}} = \max_{1 \le k \le K} \mu_k\). On \(\mathcal{E}\), the empirical mean of optimizer \(k^{\star}\) satisfies \(\|\hat{\mu}_{k^{\star}} - \mu_{k^{\star}}\| &lt; \varepsilon\), and so \(\mu_{k^{\star}} - \hat{\mu}_{k^{\star}} \le \varepsilon\). For any other optimizer \(k\), the same event then gives \(\|\hat{\mu}_k - \mu_k\| &lt; \varepsilon \Rightarrow \mu_k \le \hat{\mu}_k + \varepsilon\). By construction, \(\hat{k}\) maximizes the empirical means, so \(\hat{\mu}_{\hat{k}} \ge \hat{\mu}_{k^{\star}}\).</p>

<p>Now decompose the performance gap as</p>

\[\mu_{k^{\star}} - \mu_{\hat{k}} = (\mu_{k^{\star}} - \hat{\mu}_{k^{\star}}) + (\hat{\mu}_{k^{\star}} - \hat{\mu}_{\hat{k}}) + (\hat{\mu}_{\hat{k}} - \mu_{\hat{k}})\]

<p>On \(\mathcal{E}\), the three terms satisfy \(\mu_{k^{\star}} - \hat{\mu}_{k^{\star}} \le \varepsilon\), \(\hat{\mu}_{k^{\star}} - \hat{\mu}_{\hat{k}} \le 0\), and \(\hat{\mu}_{\hat{k}} - \mu_{\hat{k}} \le \varepsilon\), so</p>

\[\mu_{k^{\star}} - \mu_{\hat{k}} \le \varepsilon + 0 + \varepsilon = 2 \varepsilon\]

<p>Therefore</p>

\[\mathbb{P}\big(\mu_{k^{\star}} - \mu_{\hat{k}} &gt; 2 \varepsilon\big)
\le 2 K \exp(-2 n \varepsilon^2)\]

<p>This inequality gives a clean probabilistic guarantee in that the probability that the optimizer selected by empirical mean performance is more than \(2 \varepsilon\) worse than the best optimizer is at most \(2 K \exp(-2 n \varepsilon^2)\).</p>

<h3>Sample Complexity in Terms of \(\varepsilon\) and \(\delta\)</h3>

<p>Next, to invert the bound, fix a target confidence level \(1 - \delta\) with \(0 &lt; \delta &lt; 1\).  The goal is to get \(2 K \exp(-2 n \varepsilon^2) \le \delta\). First, start with \(2 K \exp(-2 n \varepsilon^2) \le \delta \Rightarrow \exp(-2 n \varepsilon^2) \le \delta/(2 K)\). Taking natural logs and using the fact that the exponential is monotonic, find that \(-2 n \varepsilon^2 \le \log(\delta/(2 K))\), and multiplying by \(-1\), the inequality gets reversed so \(2 n \varepsilon^2 \ge -\log(\delta/(2 K)) = \log(2 K/\delta)\). Dividing both sides by \(2 \varepsilon^2\) finally gives:</p>

\[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)\]

<p>Thus, if</p>

\[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)\]

<p>cases per optimizer and the optimizer with largest empirical mean is selected, then with probability at least \(1 - \delta\) the chosen optimizer \(\hat{k}\) satisfies \(\mu_{k^{\star}} - \mu_{\hat{k}} \le 2 \varepsilon\). The total number of episodes is \(K n\), and the dependence on the number of optimizers is only logarithmic through the factor \(\log(2 K / \delta)\).</p>

<p>For a small numerical example, consider \(K = 4\), \(\delta = 0.05\), \(\varepsilon = 0.1\). Calculate each component explicitly like so: \(2 K = 2 \times 4 = 8\), \(\frac{2 K}{\delta} = \frac{8}{0.05} = 160\), \(\log\!\left(\frac{2 K}{\delta}\right) = \log(160) \approx 5.0752\). Next, \(\varepsilon^2 = (0.1)^2 = 0.01\), \(2 \varepsilon^2 = 2 \times 0.01 = 0.02\), \(\frac{1}{2 \varepsilon^2} = \frac{1}{0.02} = 50\). Then, putting this together:</p>

\[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)
\approx 50 \times 5.0752
\approx 253.76\]

<p>so it should be enough to take \(n = 254\) cases per optimizer. The resulting total sample size is \(K n = 4 \times 254 = 1016\). Under such a choice, the bound guarantees that the optimizer selected by the empirical mean performance is within about \(0.2\) of the best possible mean reward, with probability at least \(95\%\).</p>

<h3>Mean-variance Experiment and Empirical Results</h3>

<p>To experiment, I implement the meta-optimization set-up synthetically in a world with \(d=5\) assets and four candidate optimizers that are given by projected gradient ascent with step sizes \(\eta \in \{0.01, 0.03, 0.08, 0.15\}\). For each of the problems \(p = (\mu,\Sigma)\), all four of the optimizers are run, their mean–variance utilities are computed, and then I linearly rescale them to \([0,1]\) so that the best optimizer on that problem receives a reward of \(1\), and the worst receives a reward of \(0\). To avoid a degenerate and basically deterministic situation where the best optimizer is almost always identified perfectly, and to better mimic noisy OOS evaluation during meta-learning, I add a small Gaussian perturbation to this rescaled utility and then clip it back to \([0,1]\). This maintains the assumption that rewards are bounded in the analysis with the Hoeffding bound, but it introduces a nontrivial chance of not selecting the best optimizer when \(n\) is small.</p>

<p>On a large independent test set of problems, the estimated mean rewards of the four optimizers are \((\mu_0,\mu_1,\mu_2,\mu_3) \approx (0.24, 0.47, 0.69, 0.76)\), so the \(\eta = 0.15\) optimizer is best but not far better than \(\eta = 0.08\). For each sample size \(n\), I draw \(n\) new problems per optimizer, compute the empirical means \(\hat{\mu}*k\), choose \(\hat{k} = \arg\max_k \hat{\mu}*k\), and write down the realized performance gap \(\mu*{k^\star} - \mu*{\hat{k}}\). Repeating this 60 times per \(n\) produces an empirical distribution of the gap, which I then compare to the tolerance obtained via the Hoeffding bound:</p>

\[2\varepsilon_n = 2\sqrt{\frac{1}{2n}\log\left(\frac{2K}{\delta}\right)}
\quad\text{with } K = 4,\ \delta = 0.05\]

<p>The figure below plots the mean gap, its 95th percentile, and the theoretical \(2\varepsilon_n\) curve as functions of \(n\) on a semi-logarithmic scale:</p>

<p><img src="/assets/img/RL-optimizers-plot1.png" alt="Meta-optimizer Selection Experiment in the Toy Market" /></p>

<p>The exact values are in the table below:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Episodes per optimizer \(n\)</th>
      <th style="text-align: right">Mean gap \(\mathbb{E}[\mu_{k^\star} - \mu_{\hat{k}}]\)</th>
      <th style="text-align: right">95th pct. gap</th>
      <th style="text-align: right">Theoretical \(2\varepsilon_n\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">4</td>
      <td style="text-align: right">0.0496</td>
      <td style="text-align: right">0.2900</td>
      <td style="text-align: right">1.5930</td>
    </tr>
    <tr>
      <td style="text-align: right">8</td>
      <td style="text-align: right">0.0290</td>
      <td style="text-align: right">0.0726</td>
      <td style="text-align: right">1.1264</td>
    </tr>
    <tr>
      <td style="text-align: right">16</td>
      <td style="text-align: right">0.0278</td>
      <td style="text-align: right">0.0726</td>
      <td style="text-align: right">0.7965</td>
    </tr>
    <tr>
      <td style="text-align: right">32</td>
      <td style="text-align: right">0.0157</td>
      <td style="text-align: right">0.0726</td>
      <td style="text-align: right">0.5632</td>
    </tr>
    <tr>
      <td style="text-align: right">64</td>
      <td style="text-align: right">0.0060</td>
      <td style="text-align: right">0.0726</td>
      <td style="text-align: right">0.3982</td>
    </tr>
    <tr>
      <td style="text-align: right">128</td>
      <td style="text-align: right">0.0012</td>
      <td style="text-align: right">0.0000</td>
      <td style="text-align: right">0.2816</td>
    </tr>
    <tr>
      <td style="text-align: right">256</td>
      <td style="text-align: right">0.0000</td>
      <td style="text-align: right">0.0000</td>
      <td style="text-align: right">0.1991</td>
    </tr>
    <tr>
      <td style="text-align: right">512</td>
      <td style="text-align: right">0.0000</td>
      <td style="text-align: right">0.0000</td>
      <td style="text-align: right">0.1408</td>
    </tr>
  </tbody>
</table>

<p>Even with the noisy rewards, the mean gap goes toward zero as \(n\) increases, and the 95th-percentile gap decays on the same \(1/\sqrt{n}\) scale as the theoretical \(2\varepsilon_n\) line while staying below it. This indicates that the Hoeffding bound is conservative but also accurate for this meta-optimization problem. More broadly, it can be seen that classical concentration inequalities already give useful sample-complexity guarantees for optimizer selection using RL, as demonstrated in the stylized financial setting here.</p>

<h3>Notes and References</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:bair" role="doc-endnote">
      <p>BAIR Blog, “Learning to Optimize with RL” (2017). Available at <a href="https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/">https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/</a>. <a href="#fnref:bair" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:bair:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:rlbook" role="doc-endnote">
      <p><em>Reinforcement Learning and Stochastic Optimization</em> (RL Theory Book). Available at <a href="https://rltheorybook.github.io/">https://rltheorybook.github.io/</a>. <a href="#fnref:rlbook" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  

</article>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Arnab Sircar - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://a-sircar1.github.io/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
