<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <generator uri="http://jekyllrb.com" version="4.3.4">Jekyll</generator>
  
  
  <link href="https://a-sircar1.github.io/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://a-sircar1.github.io/" rel="alternate" type="text/html" hreflang="en" />
  <updated>2026-02-03T01:46:21+00:00</updated>
  <id>https://a-sircar1.github.io/</id>

  
    <title type="html">Arnab Sircar</title>
  

  
    <subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle>
  

  
    <author>
        <name>Arnab Sircar</name>
      
      
    </author>
  

  
  
    <entry>
      
      <title type="html">Hard Problems in Disguise: Distribution-Preserving Subsampling via Multiscale Trees, and a Brief Introduction to Optimal Transport</title>
      
      
      <link href="https://a-sircar1.github.io/2026/01/29/Downsampling-Dataset/" rel="alternate" type="text/html" title="Hard Problems in Disguise: Distribution-Preserving Subsampling via Multiscale Trees, and a Brief Introduction to Optimal Transport" />
      
      <published>2026-01-29T06:10:56+00:00</published>
      <updated>2026-01-29T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2026/01/29/Downsampling-Dataset</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2026/01/29/Downsampling-Dataset/">&lt;p&gt;I’ve had the pleasure of being a teaching assistant for one of Penn’s flagship machine learning courses for the past three semesters now, and I’ve enjoyed the experience so much. There are many amazing parts of the job I can speak to, but I believe the most fruitful and memorable ones that keep me coming back semester after semester have all been from the relationships I’ve gotten to form with students, faculty, and my peer TAs alike.&lt;/p&gt;

&lt;p&gt;This semester, we added a final project submission to the course curriculum, in addition to the usual homeworks and exams. And along with the final project requirements came a schedule of checkpoints throughout the semester in which project groups were meant to meet with their assigned TAs. Effectively, this gave the TAs the opportunity to serve as mentors for project groups.&lt;/p&gt;

&lt;p&gt;I was able to work with many amazing groups and was so happy to see the amount of commitment students were putting toward their projects. I was particularly impressed with one of my groups that was working on building and evaluating an ML-based approach toward the joint prediction of the occurence and magnitude of flight delays from a large U.S. domestic flights dataset. The group was not only diligent in the completion of the project, but they also asked insightful and interesting questions that clearly showed they were thinking carefully about the nuances of the problem they were tackling.&lt;/p&gt;

&lt;p&gt;In fact, one of their questions was especially fascinating to me, as it was a very real, but complex problem that popped up in a seemingly simple setting. They had the following situation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The dataset is simply too big to work with locally on laptops. Training models on the data takes way too long. So, take a sample from the dataset such that the label distribution in the sample remains the same as that of the complete data. However, what issues does this cause for training now? What if there are fundamental differences in the feature distributions of the rows in the sample? And crucially, how can a sample be selected such that feature distributions from the original dataset are jointly preserved across all features?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After our meeting, I dug deeper into the problem and found out it was a documented challenge. I was hooked, and so, the general setup of the problem has sat in the back of my head for some weeks. In this post, I detail some of the thoughts I’ve had and findings I’ve made.&lt;/p&gt;

&lt;h1&gt;Roadmap and Setup&lt;/h1&gt;

&lt;p&gt;Informally, we can think of this problem as “sampling without changing the dataset.” But what does this mean? How should we think about this more carefully?&lt;/p&gt;

&lt;p&gt;A dataset is an empirical distribution, and a sample is another empirical distribution. Once we re-orient our thinking in this way, the whole problem becomes clearer, and, as we’ll soon see, much more delicate than it looks.&lt;/p&gt;

&lt;p&gt;Let me fix notation in a way that doesn’t assume anything special about the split, and doesn’t even assume binary labels. Suppose we have a dataset&lt;/p&gt;

\[D={(x_i,y_i)}_{i=1}^N\]

&lt;p&gt;where each \(y_i\) belongs to a finite label set \(\mathcal Y\) (for classification), and each \(x_i\) is a vector of features in some product space \(\mathcal X\) (think \(\mathbb R^d\) for numerical features crossed with a finite product of categorical alphabets for discrete features). There is no “true distribution” in the background here. At least, we don’t need to assume one. All that matters is the empirical distribution induced by the observed dataset:&lt;/p&gt;

\[P_D =\frac1N\sum_{i=1}^N \delta_{(x_i,y_i)}\]

&lt;p&gt;where \(\delta_z\) denotes a point mass at \(z\). Interpret this simply as \(P_D(A)\) for a set \(A\subseteq \mathcal X\times\mathcal Y\) is simply “the fraction of rows whose \((x_i,y_i)\) land in \(A\).”&lt;/p&gt;

&lt;p&gt;Now imagine we want to keep only \(n\ll N\) rows. We pick an index subset \(S \subseteq {1,\dots,N}\) with \(\lvert S \rvert=n\), and form the sampled dataset \(D_S = {(x_i,y_i): i\in S}\). This induces its own empirical distribution&lt;/p&gt;

\[P_S =\frac1n\sum_{i\in S}\delta_{(x_i,y_i)}\]

&lt;p&gt;So the vague problem of “pick a representative sample” can be stated as the following mathematical objective: choose (S) so that \(P_S\) is close to \(P_D\).&lt;/p&gt;

&lt;p&gt;Where do labels come in? Usually, the first constraint that people impose is to preserve the label proportions. If \(\widehat p_y = P_D(Y=y)\) denotes the empirical label frequencies of the full dataset, we would like the sample to satisfy \(P_S(Y=y) \approx \widehat p_y\) for all \(y\in\mathcal Y\). Most of the time we can enforce this approximately by rounding, or exactly by committing to target sample sizes where \(n_y \in \mathbb Z_{\ge 0}\) and \(\sum_{y\in\mathcal Y} n_y = n\), and requiring \(\lvert S\cap I_y \rvert=n_y\), where \(I_y=\{i:y_i=y\}\). This is the simplest way to formalize the idea of “keeping the same label split.”&lt;/p&gt;

&lt;p&gt;A small but crucial observation is that this reduces the whole task to the conditional distributions. So, the empirical distribution factorizes as \(P_D(x,y)=P_D(y),P_D(x\mid y)\) and \(P_S(x,y)=P_S(y),P_S(x\mid y)\).&lt;/p&gt;

&lt;p&gt;If we force \(P_S(y)=P_D(y)\) (by enforcing the \(n_y\)), then the only remaining question is how close we can make each conditional \(P_S(x\mid y)\) to \(P_D(x\mid y)\). In other words, the “dataset reduction” problem becomes a family of independent problems, one for each label \(y\): inside the class-\(y\) points, choose \(n_y\) rows so that the feature distribution for that class is preserved. Once you solve that for each class, you combine the results and you automatically preserve the global label distribution.&lt;/p&gt;

&lt;p&gt;So we’ve reduced the problem to this: for each label \(y\), from the multiset of feature vectors \({x_i: i\in I_y}\) of size \(N_y\), select a subset \(S_y\subseteq I_y\) of size \(n_y\) so that&lt;/p&gt;

\[P_{S_y}\approx P_{I_y}
\quad\text{where}\quad
P_{I_y}=\frac1{N_y}\sum_{i\in I_y}\delta_{x_i},
\quad
P_{S_y}=\frac1{n_y}\sum_{i\in S_y}\delta_{x_i}\]

&lt;p&gt;Everything now depends on what “\(\approx\)” means. This is the point where there’s a fork in the road, and the roadmap of this post is basically about exploring two particularly enlightening branches toward getting us as close as possible to preserving the label distributions.&lt;/p&gt;

&lt;p&gt;One branch is what I’ll call the “questions you want your sample to answer” viewpoint. A dataset is valuable because you ask it questions like what’s the fraction of flights with departure delay exceeding 30 minutes? what’s the distribution of carrier codes? what is the conditional distribution of delay given month and origin airport? If we decide in advance on a large family of such “questions,” then preserving the dataset means that the full dataset and the sample should give nearly the same answers to all questions in that family. Mathematically, these “questions” are test functions \(f\) on \(\mathcal X\), and “having the same answers” means matching expectations:&lt;/p&gt;

\[\mathbb E_{P_{S_y}}[f(X)] \approx \mathbb E_{P_{I_y}}[f(X)]
\quad\text{for all }f\text{ in some function class }\mathcal F\]

&lt;p&gt;The challenge is to choose \(\mathcal F\) so that it is both rich (so that the guarantee we provide is actually ubiquitous and somewhat useful) and tractable (so that we can design an algorithm and prove that it works). My own approach in this post is based on choosing \(\mathcal F\) to be indicator functions of cells in a multiscale partition of the feature space. Briefly, the idea is to build a recursive partition (a tree) of the dataset into cells that correspond to coarse-to-fine “regions” in the feature space, and you ask that the sample preserves the mass of every such region. This produces a discrepancy measure \(d_{\mathcal T}(P_{S_y},P_{I_y})\) for the tree \(\mathcal T\), and then the algorithm’s task is to construct \(S_y\) so that this discrepancy is provably small. The attractive part here is that the statement “every region has almost the right mass” is something you can prove with induction, and it gives a pretty teachable correctness guarantee.&lt;/p&gt;

&lt;p&gt;The other branch is to use optimal transport (OT). Here the philosophical question is not “do these two datasets answer my pre-selected family of questions similarly?” but rather “how expensive is it to morph one of the empirical distributions into the other?” Optimal transport formalizes the notion of moving probability mass through a feature space with minimal cost. It produces a distance between distributions (like the Wasserstein distance), which is geometric, in that, if two distributions differ only by a small shift in a continuous feature, OT sees that as small, whereas histogram metrics can see it as large if your bins are not properly designed. OT is therefore a natural conceptual baseline for what it means to preserve “shape” of a distribution. In one dimension it becomes exactly quantiles, and in higher dimensions it becomes computationally harder but still interpretable. In this post I’ll use OT as a teachable interlude, partly because it’s intrinsically beautiful, and partly because it clarifies what any distribution-preserving sampling scheme is trying to approximate. To be honest, my mind immediately went to OT when I first heard this problem, and this exercise gave me the perfect excuse to teach myself the fundamentals of OT.&lt;/p&gt;

&lt;p&gt;So the plan here is as follows: first, I’ll explain why this problem is hard in a very concrete sense– hard even before you worry about fancy models– because it looks like a “simple sampling” question but is actually a constrained approximation problem over measures. Then I’ll show a construction that gives a rigorous and explicit guarantee for a meaningful notion of preservation of distributions. Finally, I’ll step back and show how OT offers a second way to view the same goal, with a complete and simple rule/theorem in one dimension and a story for tackling the general case.&lt;/p&gt;

&lt;p&gt;At the end, the hope is that you’ll walk away with two things: one “hands on” algorithm you could implement for your own project that comes with a proof of correctness, and one geometric mental model– using optimal transport– that changes how you think about what it means for a sample to be representative.&lt;/p&gt;

&lt;h1&gt;Why is This a Hard Problem?&lt;/h1&gt;

&lt;p&gt;At first glance, the students’ question sounds like it should have a straightforward answer, as it seems like it would be pretty commonly asked. Sampling while preserving label proportions is a built-in option in many libraries. So why does feature drift happen at all, and why can’t we just fix it by sampling more carefully?&lt;/p&gt;

&lt;p&gt;There are three distinct difficulties here, and they show up even before you think about neural networks, fancy objectives, or hyperparameter tuning. They are: (i) the object you’re trying to preserve is infinite-dimensional, (ii) exact preservation quickly becomes a combinatorial optimization problem, and (iii) high-dimensional structure forces a tradeoff between resolution and sample size.&lt;/p&gt;

&lt;h2&gt;1. Distribution Preservation is an Infinite-dimensional Constraint&lt;/h2&gt;

&lt;p&gt;Fix a label \(y\in\mathcal Y\) and focus only on the class-conditional empirical feature distribution&lt;/p&gt;

\[P_{I_y}=\frac1{N_y}\sum_{i\in I_y}\delta_{x_i}\]

&lt;p&gt;and its sampled counterpart&lt;/p&gt;

\[P_{S_y}=\frac1{n_y}\sum_{i\in S_y}\delta_{x_i}\]

&lt;p&gt;What would it mean to “preserve the feature distribution” exactly? The strongest statement would be&lt;/p&gt;

\[P_{S_y}=P_{I_y}\]

&lt;p&gt;as measures on \(\mathcal X\). But if \(n_y&amp;lt;N_y\) and the feature vectors are not massively duplicated, this is impossible, as \(P_{S_y}\) has a support size of at most \(n_y\), while \(P_{I_y}\) has support size \(N_y\). So we are forced into an approximation.&lt;/p&gt;

&lt;p&gt;Abstractly, a distribution can be thought of as an operator that assigns probabilities to all measurable sets, or equivalently a tool that assigns expectations to all bounded measurable functions. One standard way to make this precise is using an integral probability metric. Given a class of test functions \(\mathcal F\), define&lt;/p&gt;

\[d_{\mathcal F}(P,Q)=\sup_{f\in\mathcal F}\left|\mathbb E_P[f(X)]-\mathbb E_Q[f(X)]\right|\]

&lt;p&gt;If \(\mathcal F\) is “too big,” then matching becomes either impossible or sample-inefficient (e.g. choosing \(\mathcal F\) as all bounded measurable functions corresponds essentially to total variation). If \(\mathcal F\) is “too small,” then the guarantee is meaningless, as you can match a few moments and still have a wildly different distribution. So the difficulty is not that we lack distances between distributions, but it’s that the notion of “preserving the distribution” necessarily requires choosing which family of distributional questions you care about.&lt;/p&gt;

&lt;p&gt;This is why the path we select in the roadmap section matters. Any solution must choose some structured class \(\mathcal F\) (or some structured metric like Wasserstein distance) that is rich enough to capture what models that we care about but simple enough that we can design algorithms and prove guarantees.&lt;/p&gt;

&lt;h2&gt;2. Even Simple Exact Matching Becomes Combinatorial (and can be NP-hard)&lt;/h2&gt;

&lt;p&gt;A natural response to the previous section is, “Fine. I’ll just preserve a finite list of feature summaries like means, maybe quantiles, maybe histograms.” This is reasonable in practice, but there this is theoretically problematic. As soon as you impose exact matching constraints under a fixed sample size, you are solving a discrete feasibility problem, and those problems can be representative of classic NP-hard problems.&lt;/p&gt;

&lt;p&gt;Here is a clean reduction that captures the essence. Consider the simplest possible setting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;there is a single label (so ignore stratification)&lt;/li&gt;
  &lt;li&gt;there is a single numerical feature \(x_i\in\mathbb Z\)&lt;/li&gt;
  &lt;li&gt;we want to choose exactly half the points&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Suppose we demand that the sample mean equals the full-data mean. That is, with \(n=N/2\), we want a subset \(S\subseteq{1,\dots,N}\) with \(\|S\|=n\) such that&lt;/p&gt;

\[\frac1n\sum_{i\in S}x_i = \frac1N\sum_{i=1}^N x_i\]

&lt;p&gt;Now take an instance of the PARTITION problem: given integers \(a_1,\dots,a_N\) with \(N\) even, decide whether there exists a subset of exactly \(N/2\) of them whose sum is half the total. Construct data points \(x_i=a_i\). Let&lt;/p&gt;

\[A=\sum_{i=1}^N a_i\]

&lt;p&gt;Then the full-data mean is \(A/N\). A subset \(S\) of size \(n=N/2\) satisfies the mean constraint iff&lt;/p&gt;

\[\frac1n\sum_{i\in S}a_i = \frac AN
\implies
\sum_{i\in S}a_i = \frac nN A
\implies
\sum_{i\in S}a_i = \frac12 A\]

&lt;p&gt;So deciding whether there exists a half-sample whose mean matches the full mean is exactly PARTITION. In particular, the decision version of this “perfect mean-preserving subsample” problem is NP-complete.&lt;/p&gt;

&lt;p&gt;This is a tiny example, but it makes the key point. Once you start demanding that a subset match distributional properties exactly, you are often solving a hard combinatorial selection problem. And since realistic notions of “preserve feature distributions” typically involve matching many summaries simultaneously, exact matching is not only impossible for measure-theoretic reasons (as seen in the previous section), but also computationally intractable even when it is not impossible.&lt;/p&gt;

&lt;p&gt;The practical conclusion is that “distribution-preserving sampling” should be seen as an approximation problem with some kind of error control or minimization.&lt;/p&gt;

&lt;h2&gt;3. High-dimensional Distributions Force a Resolution vs. Sample-size Tradeoff&lt;/h2&gt;

&lt;p&gt;Even if we ignore computational hardness and focus only on approximation, there is a geometric barrier that blocks the way. High-dimensional distributions cannot be preserved at fine resolution without huge sample sizes. This is essentially the curse of dimensionality, but it’s worth seeing it in a specific way.&lt;/p&gt;

&lt;p&gt;Imagine discretizing each feature into a small number of bins. Now, suppose, for simplicity, we discretize each of \(d\) numerical features into \(m\) bins (quantiles, for example). If we tried to preserve the full joint histogram over all \(d\) features, we would be working with \(m^d\) cells. Even for modest numbers like \(m=10\) and \(d=20\), this is \(10^{20}\) cells– astronomically many. Most cells are empty even in the full dataset, so the “true” empirical joint histogram is extremely sparse and unstable. A small sample cannot possibly reproduce it reliably.&lt;/p&gt;

&lt;p&gt;This is why approaches that try to preserve the full joint distribution directly are doomed except in very low dimensions or enormous sample sizes. The only viable strategy here is to choose a structured approximation to the distribution that avoids an exponential complexity. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;you might preserve all one-dimensional marginals (feature-wise histograms)&lt;/li&gt;
  &lt;li&gt;you might preserve selected pairwise interactions (a small set of correlations or conditional tables)&lt;/li&gt;
  &lt;li&gt;you might preserve masses of cells in a multiscale partition (a tree), which captures coarse structure globally and finer structure locally where data supports it&lt;/li&gt;
  &lt;li&gt;or you might use a geometric metric like Wasserstein distance, which implicitly balances resolution and geometry without committing to a fixed binning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these are different ways of acknowledging the same fundamental reality in that distribution preservation is always relative to a choice of resolution (explicitly, as in partitions and histograms). The hard part is choosing a notion of similarity that is both mathematically meaningful and practically aligned with the learning problem further down the pipeline.&lt;/p&gt;

&lt;p&gt;This is exactly why the roadmap split into two branches is useful. The multiscale-partition approach makes the resolution explicit and gives clean discrepancy bounds. The optimal transport approach provides an elegant “best possible” story in one dimension, with principled approximations in higher dimensions.&lt;/p&gt;

&lt;p&gt;In the next section, I’ll start with the multiscale viewpoint, and I’ll build our approximation algorithm.&lt;/p&gt;

&lt;h1&gt;The Target: Preserve Mass on a Multiscale Partition (label conditionally)&lt;/h1&gt;

&lt;p&gt;We now need to make the approximation target \(P_{S_y}\approx P_{I_y}\) precise in a way that is both mathematically meaningful and algorithmically tractable.&lt;/p&gt;

&lt;p&gt;Let’s fix a finite label set \(\mathcal Y\). For each \(y\in\mathcal Y\) let \(I_y=\{i:y_i=y\}\) and \(N_y=\lvert I_y\rvert\), and choose target sample sizes \(n_y\in\mathbb Z_{\ge 0}\) with \(\sum_{y\in\mathcal Y} n_y=n\). As discussed above, we will construct the sample label conditionally, which means, for each \(y\) we select a subset \(S_y\subseteq I_y\) with \(\lvert S_y\rvert=n_y\), and then return&lt;/p&gt;

\[S=\bigcup_{y\in\mathcal Y} S_y\]

&lt;p&gt;This enforces the label proportions exactly:&lt;/p&gt;

\[P_S(Y=y)=\frac{\lvert S_y\rvert}{\lvert S\rvert}=\frac{n_y}{n}\]

&lt;p&gt;So here it suffices to focus on a single label \(y\) and suppress it in notation. We now have a finite population \(I\) of size \(N\) and want to select a subset \(S\subseteq I\) of size \(n\) whose empirical feature distribution is close to that of the population.&lt;/p&gt;

&lt;p&gt;Instead of trying to compare \(P_S\) and \(P_I\) on all measurable sets, which is too strong, we compare them on a structured family of sets coming from a multiscale partition of feature space. Specifically, we build a binary &lt;em&gt;partition tree&lt;/em&gt; \(\mathcal T\) over the indices in \(I\).&lt;/p&gt;

&lt;p&gt;A partition tree \(\mathcal T\) consists of nodes \(u\), each associated with a subset \(A_u\subseteq I\), satisfying:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The root node corresponds to the full class: \(A_{\mathrm{root}}=I\)&lt;/li&gt;
  &lt;li&gt;Every internal node \(u\) has two children \(u_L,u_R\) such that \(A_u = A_{u_L}\sqcup A_{u_R}\) and \(A_{u_L}\cap A_{u_R}=\varnothing\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Intuitively, each node corresponds to a “cell” in feature space, that is obtained by recursively splitting on numerical thresholds or grouping categorical values. Near the root, cells are coarse. Deeper down, cells are finer. The tree is therefore a multiscale description of the feature distribution.&lt;/p&gt;

&lt;p&gt;Given such a tree, we measure how well a subset \(S\) matches the population by comparing the masses of all cells in the tree. For each node \(u\in\mathcal T\) define its population mass \(p_u = \frac{\lvert A_u\rvert}{N}\) and sample mass \(q_u = \frac{\lvert S\cap A_u\rvert}{n}\). The associated discrepancy is&lt;/p&gt;

\[d_{\mathcal T}(S) = \max_{u\in\mathcal T} \lvert q_u-p_u\rvert\]

&lt;p&gt;If \(d_{\mathcal T}(S)\) is small, then for every region of feature space represented anywhere in the tree, the fraction of sampled points landing in that region is close to the fraction in the full dataset. This is a checkable notion of “distribution preservation,” and from here, we can support a rigorous proof.&lt;/p&gt;

&lt;p&gt;At this point the problem becomes algorithmic: can we construct a subset \(S\) of size \(n\) with provably small \(d_{\mathcal T}(S)\)?&lt;/p&gt;

&lt;p&gt;The answer is yes. In the next section I’ll give a multiscale balancing construction (deterministic and randomized variants) that produces this kind of \(S\) with explicit error bounds, and the proofs are short enough to hopefully make the point as clear as possible.&lt;/p&gt;

&lt;h2&gt;Multiscale Balanced Rounding on a Tree&lt;/h2&gt;

&lt;p&gt;Fix a label \(y\in\mathcal Y\) and suppress notation. So we have a finite population index set \(I\) of size \(N=\lvert I\rvert\) and a partition tree \(\mathcal T\) whose nodes \(u\) correspond to subsets \(A_u\subseteq I\) satisfying&lt;/p&gt;

\[\begin{aligned}
A_{\mathrm{root}} &amp;amp;= I\\
A_u &amp;amp;= A_{u_L}\sqcup A_{u_R}\ \ \text{for internal nodes }u
\end{aligned}\]

&lt;p&gt;Our goal is to choose a subset \(S\subseteq I\) with \(\lvert S\rvert=n\) so that, for every node \(u\in\mathcal T\), the sample mass of the cell \(A_u\) is close to the population mass:&lt;/p&gt;

\[\begin{aligned}
p_u &amp;amp;= \frac{\lvert A_u\rvert}{N} \\
q_u &amp;amp;= \frac{\lvert S\cap A_u\rvert}{n} \\
d_{\mathcal T}(S) &amp;amp;= \max_{u\in\mathcal T} \lvert q_u-p_u\rvert
\end{aligned}\]

&lt;p&gt;The key idea is to separate the problem into two steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Allocate integer sample counts to every node of the tree in a way that respects the tree structure and tracks the population proportions&lt;/li&gt;
  &lt;li&gt;Once the leaves know how many points they should contribute, sample that many points from each leaf and take the union&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This works because, if the leaf counts are consistent, then every internal node automatically receives the correct count. This is because internal nodes are unions of leaves. So the “distribution preservation” problem reduces to this tree rounding problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3&gt;Preliminaries: Notation for Counts&lt;/h3&gt;

&lt;p&gt;For each node \(u\) define the population count&lt;/p&gt;

\[N_u = \lvert A_u\rvert\]

&lt;p&gt;We will construct integers \(n_u\) (sample counts) for every node \(u\) such that&lt;/p&gt;

\[\begin{aligned}
n_{\mathrm{root}} &amp;amp;= n\\
n_u &amp;amp;= n_{u_L}+n_{u_R}\ \ \text{for every internal node }u
\end{aligned}\]

&lt;p&gt;Once this is done, we will sample exactly \(n_\ell\) indices from each leaf \(\ell\), and define&lt;/p&gt;

\[\begin{aligned}
S &amp;amp;= \bigcup_{\ell\ \text{leaf}} S_\ell\\
S_\ell &amp;amp;\subseteq A_\ell\\
\lvert S_\ell\rvert &amp;amp;= n_\ell
\end{aligned}\]

&lt;p&gt;The following lemma formalizes the “consistency implies exact internal counts” statement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma (internal node counts are forced by leaf counts).&lt;/strong&gt;&lt;br /&gt;
Suppose that for every leaf \(\ell\) we have chosen a subset \(S_\ell\subseteq A_\ell\) with \(\lvert S_\ell\rvert=n_\ell\), and define \(S=\bigcup_\ell S_\ell\). Suppose also that the integers \(n_u\) satisfy the additivity constraints \(n_u=n_{u_L}+n_{u_R}\) for all internal nodes and \(n_{\mathrm{root}}=n\). Then for every node \(u\),&lt;/p&gt;

\[\lvert S\cap A_u\rvert = n_u\]

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Fix a node \(u\). The set \(A_u\) is a disjoint union of the leaf cells under it:&lt;/p&gt;

\[A_u = \bigsqcup_{\ell\in\mathrm{Leaves}(u)} A_\ell\]

&lt;p&gt;Since the \(A_\ell\) are disjoint and \(S_\ell\subseteq A_\ell\), we have&lt;/p&gt;

\[S\cap A_u
= \left(\bigcup_{\ell} S_\ell\right)\cap A_u
= \bigcup_{\ell\in\mathrm{Leaves}(u)} S_\ell\]

&lt;p&gt;and this union is disjoint. Therefore&lt;/p&gt;

\[\begin{aligned}
\lvert S\cap A_u\rvert
&amp;amp;= \sum_{\ell\in\mathrm{Leaves}(u)} \lvert S_\ell\rvert\\
&amp;amp;= \sum_{\ell\in\mathrm{Leaves}(u)} n_\ell
\end{aligned}\]

&lt;p&gt;On the other hand, the additivity constraints imply that the node count \(n_u\) equals the sum of the leaf counts under \(u\) (this is proved by a simple induction down the tree: each internal node’s count is the sum of its children’s counts, so unrolling yields the sum of all descendant leaves). Hence&lt;/p&gt;

\[n_u = \sum_{\ell\in\mathrm{Leaves}(u)} n_\ell
= \lvert S\cap A_u\rvert\]

&lt;p&gt;This holds for every node \(u\). \(\square\)&lt;/p&gt;

&lt;p&gt;So all discrepancy guarantees can be proved purely at the level of the integer allocation rule for the counts \(n_u\).&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Version A: Deterministic Rounding&lt;/h2&gt;

&lt;p&gt;At each internal node \(u\) we want the left child to receive approximately the population fraction&lt;/p&gt;

\[\alpha_u = \frac{N_{u_L}}{N_u}\in[0,1]\]

&lt;p&gt;If the parent has sample count \(n_u\), the “ideal” non-integer count for the left child is \(n_u\alpha_u\). Deterministic rounding sets&lt;/p&gt;

\[\begin{aligned}
n_{u_L} &amp;amp;= \operatorname{Round}(n_u\alpha_u)\\
n_{u_R} &amp;amp;= n_u - n_{u_L}
\end{aligned}\]

&lt;p&gt;Here \(\operatorname{Round}(\cdot)\) means nearest integer, where ties are broken arbitrarily. By construction, \(n_{u_L}+n_{u_R}=n_u\) exactly at every split, so additivity holds globally.&lt;/p&gt;

&lt;p&gt;At leaves we sample:&lt;/p&gt;

\[S_\ell \sim \text{UniformWithoutReplacement}(A_\ell, n_\ell)\]

&lt;p&gt;(Notice here that the choice within a leaf can be randomized or deterministic. It does not affect the tree-mass discrepancy, only which specific points we keep.)&lt;/p&gt;

&lt;p&gt;A pseudocode description is seen below:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Algorithm: DeterministicTreeBalance(I, T, n)
Input:  index set I of size N, partition tree T with node sets A_u, target sample size n
Output: subset S ⊆ I with |S| = n

1. For every node u in T compute N_u = |A_u|
2. Set n_root = n
3. For each internal node u in top-down order:
       alpha_u = N_{u_L} / N_u
       n_{u_L} = Round(n_u * alpha_u)
       n_{u_R} = n_u - n_{u_L}
4. For each leaf ell:
       Choose S_ell ⊆ A_ell uniformly without replacement with |S_ell| = n_ell
5. Return S = ⋃_{leaves ell} S_ell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we prove the discrepancy bound carefully.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1 (deterministic multiscale balance).&lt;/strong&gt;&lt;br /&gt;
Let \(L\) be the depth of the tree \(\mathcal T\) (the maximum root-to-node distance). The subset \(S\) that is produced by the deterministic algorithm satisfies, for every node \(u\),&lt;/p&gt;

\[\left\lvert\frac{\lvert S\cap A_u\rvert}{n} - \frac{\lvert A_u\rvert}{N}\right\rvert
\le \frac{\mathrm{depth}(u)}{2n}\]

&lt;p&gt;In particular,&lt;/p&gt;

\[d_{\mathcal T}(S)\le \frac{L}{2n}\]

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; By the previous lemma, for every node \(u\) we have \(\lvert S\cap A_u\rvert=n_u\), so it suffices to bound&lt;/p&gt;

\[\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert\]

&lt;p&gt;Define the “ideal” real valued target count&lt;/p&gt;

\[\mu_u = n\cdot\frac{N_u}{N}\]

&lt;p&gt;and define the allocation error&lt;/p&gt;

\[e_u = n_u - \mu_u\]

&lt;p&gt;Then&lt;/p&gt;

\[\begin{aligned}
\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert
&amp;amp;= \left\lvert\frac{n_u}{n}-\frac{\mu_u}{n}\right\rvert\\
&amp;amp;= \frac{\lvert n_u-\mu_u\rvert}{n}\\
&amp;amp;= \frac{\lvert e_u\rvert}{n}
\end{aligned}\]

&lt;p&gt;So it is enough to prove&lt;/p&gt;

\[\lvert e_u\rvert \le \frac{\mathrm{depth}(u)}{2}
\quad\text{for all nodes }u\]

&lt;p&gt;We prove this by induction on depth.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Base case.&lt;/strong&gt; For the root,&lt;/p&gt;

\[\begin{aligned}
n_{\mathrm{root}} &amp;amp;= n\\
\mu_{\mathrm{root}} &amp;amp;= n\cdot\frac{N_{\mathrm{root}}}{N} = n\cdot\frac{N}{N}=n
\end{aligned}\]

&lt;p&gt;so&lt;/p&gt;

\[e_{\mathrm{root}} = n_{\mathrm{root}}-\mu_{\mathrm{root}} = n-n = 0\]

&lt;p&gt;Since \(\mathrm{depth}(\mathrm{root})=0\), the base case holds.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inductive step.&lt;/strong&gt; Fix an internal node \(u\) and assume&lt;/p&gt;

\[\lvert e_u\rvert\le \frac{\mathrm{depth}(u)}{2}\]

&lt;p&gt;Let its children be \(u_L,u_R\) and define \(\alpha_u = N_{u_L}/N_u\). Deterministic rounding gives&lt;/p&gt;

\[n_{u_L} = \operatorname{Round}(n_u\alpha_u)\]

&lt;p&gt;Therefore there exists a rounding error term \(\delta_u\) such that&lt;/p&gt;

\[n_{u_L} = n_u\alpha_u + \delta_u\]

&lt;p&gt;where \(\lvert\delta_u\rvert\le \frac12\). Also,&lt;/p&gt;

\[\begin{aligned}
\mu_{u_L}
&amp;amp;= n\cdot\frac{N_{u_L}}{N}\\
&amp;amp;= n\cdot\frac{N_u}{N}\cdot\frac{N_{u_L}}{N_u}\\
&amp;amp;= \mu_u\alpha_u
\end{aligned}\]

&lt;p&gt;So the child error is&lt;/p&gt;

\[\begin{aligned}
e_{u_L}
&amp;amp;= n_{u_L}-\mu_{u_L}\\
&amp;amp;= (n_u\alpha_u+\delta_u) - (\mu_u\alpha_u)\\
&amp;amp;= \alpha_u(n_u-\mu_u) + \delta_u\\
&amp;amp;= \alpha_u e_u + \delta_u
\end{aligned}\]

&lt;p&gt;Next, taking absolute values and using \(0\le \alpha_u\le 1\), we get:&lt;/p&gt;

\[\begin{aligned}
\lvert e_{u_L}\rvert
&amp;amp;\le \lvert\alpha_u\rvert\,\lvert e_u\rvert + \lvert\delta_u\rvert\\
&amp;amp;\le \lvert e_u\rvert + \frac12\\
&amp;amp;\le \frac{\mathrm{depth}(u)}{2} + \frac12\\
&amp;amp;= \frac{\mathrm{depth}(u)+1}{2}\\
&amp;amp;= \frac{\mathrm{depth}(u_L)}{2}
\end{aligned}\]

&lt;p&gt;For the right child, use \(n_{u_R}=n_u-n_{u_L}\) and \(\mu_{u_R}=\mu_u-\mu_{u_L}=\mu_u(1-\alpha_u)\). Since&lt;/p&gt;

\[n_{u_R} = n_u - (n_u\alpha_u+\delta_u) = n_u(1-\alpha_u)-\delta_u,\]

&lt;p&gt;we get&lt;/p&gt;

\[\begin{aligned}
e_{u_R}
&amp;amp;= n_{u_R}-\mu_{u_R}\\
&amp;amp;= \bigl(n_u(1-\alpha_u)-\delta_u\bigr) - \mu_u(1-\alpha_u)\\
&amp;amp;= (1-\alpha_u)(n_u-\mu_u) - \delta_u\\
&amp;amp;= (1-\alpha_u)e_u - \delta_u
\end{aligned}\]

&lt;p&gt;and so,&lt;/p&gt;

\[\begin{aligned}
\lvert e_{u_R}\rvert
&amp;amp;\le \lvert1-\alpha_u\rvert\,\lvert e_u\rvert + \lvert\delta_u\rvert\\
&amp;amp;\le \lvert e_u\rvert + \frac12\\
&amp;amp;\le \frac{\mathrm{depth}(u)+1}{2}\\
&amp;amp;= \frac{\mathrm{depth}(u_R)}{2}
\end{aligned}\]

&lt;p&gt;Thus the bound holds for both children, completing the induction. Therefore, for every node \(u\),&lt;/p&gt;

\[\lvert e_u\rvert\le \frac{\mathrm{depth}(u)}{2},\]

&lt;p&gt;and substituting into \(\left\lvert n_u/n - N_u/N\right\rvert = \lvert e_u\rvert/n\) produces the theorem. \(\square\)&lt;/p&gt;

&lt;p&gt;This is a worst-case bound with no probability and no consideration for the “typical case.” Its conceptual limitation is also clear in that the bound grows linearly with depth. That motivates a randomized variant that removes systematic drift and produces a concentration like \(\sqrt{\mathrm{depth}}/n\) for any fixed node.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;Version B: Randomized Rounding&lt;/h2&gt;

&lt;p&gt;The deterministic algorithm rounds to the nearest integer, which is stable but can accumulate small biases along a path in the tree. Randomized rounding replaces that with an unbiased decision at each of the splits.&lt;/p&gt;

&lt;p&gt;Fix an internal node \(u\). Let \(t_u = n_u\alpha_u\) and say&lt;/p&gt;

\[t_u = \lfloor t_u\rfloor + \theta_u,
\quad
\theta_u \in [0,1)\]

&lt;p&gt;Define&lt;/p&gt;

\[n_{u_L} =
\begin{cases}
\lfloor t_u\rfloor + 1 &amp;amp; \text{with probability }\theta_u\\
\lfloor t_u\rfloor &amp;amp; \text{with probability }1-\theta_u
\end{cases}\]

&lt;p&gt;and \(n_{u_R} = n_u - n_{u_L}\). Everything else is the same.&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Algorithm: RandomizedTreeBalance(I, T, n)
Input:  index set I of size N, partition tree T with node sets A_u, target sample size n
Output: subset S ⊆ I with |S| = n

1. Compute N_u = |A_u| for all nodes u
2. Set n_root = n
3. For each internal node u in top-down order:
       alpha_u = N_{u_L} / N_u
       t = n_u * alpha_u
       theta = t - floor(t)
       With prob theta set n_{u_L} = floor(t) + 1; otherwise n_{u_L} = floor(t)
       Set n_{u_R} = n_u - n_{u_L}
4. For each leaf ell:
       Choose S_ell ⊆ A_ell uniformly without replacement with |S_ell| = n_ell
5. Return S = ⋃_{leaves ell} S_ell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We now prove the two key properties (1) unbiasedness and (2) concentration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 2 (unbiased node counts).&lt;/strong&gt;&lt;br /&gt;
For every node \(u\in\mathcal T\), the random count \(n_u = \lvert S\cap A_u\rvert\) satisfies&lt;/p&gt;

\[\mathbb E[n_u] = n\cdot \frac{N_u}{N}\]

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; As above, \(\lvert S\cap A_u\rvert=n_u\) deterministically once the allocation is fixed, so it suffices to prove the statement for the allocation random variables \(n_u\).&lt;/p&gt;

&lt;p&gt;We prove by induction on depth. For the root,&lt;/p&gt;

\[\mathbb E[n_{\mathrm{root}}] = n = n\cdot\frac{N}{N} = n\cdot\frac{N_{\mathrm{root}}}{N}\]

&lt;p&gt;Assume the claim holds for a node \(u\). Conditional on \(n_u\), define \(t_u=n_u\alpha_u\). By construction,&lt;/p&gt;

\[n_{u_L} =
\begin{cases}
\lfloor t_u\rfloor + 1 &amp;amp; \text{with probability }\theta_u\\
\lfloor t_u\rfloor &amp;amp; \text{with probability }1-\theta_u
\end{cases}\]

&lt;p&gt;and \(\theta_u = t_u-\lfloor t_u\rfloor\). Therefore&lt;/p&gt;

\[\begin{aligned}
\mathbb E[n_{u_L}\mid n_u]
&amp;amp;= (\lfloor t_u\rfloor + 1)\theta_u + (\lfloor t_u\rfloor)(1-\theta_u)\\
&amp;amp;= \lfloor t_u\rfloor + \theta_u\\
&amp;amp;= t_u\\
&amp;amp;= n_u\alpha_u
\end{aligned}\]

&lt;p&gt;Taking expectations,&lt;/p&gt;

\[\begin{aligned}
\mathbb E[n_{u_L}]
&amp;amp;= \mathbb E\big[\mathbb E[n_{u_L}\mid n_u]\big]\\
&amp;amp;= \mathbb E[n_u\alpha_u]\\
&amp;amp;= \alpha_u\,\mathbb E[n_u]\\
&amp;amp;= \frac{N_{u_L}}{N_u}\cdot n\frac{N_u}{N}\\
&amp;amp;= n\frac{N_{u_L}}{N}
\end{aligned}\]

&lt;p&gt;For the right child, \(n_{u_R}=n_u-n_{u_L}\), so&lt;/p&gt;

\[\mathbb E[n_{u_R}]
= \mathbb E[n_u]-\mathbb E[n_{u_L}]
= n\frac{N_u}{N} - n\frac{N_{u_L}}{N}
= n\frac{N_{u_R}}{N}\]

&lt;p&gt;Thus the claim holds for both children, completing the induction. \(\square\)&lt;/p&gt;

&lt;p&gt;So the randomized construction is exactly correct in expectation*for every cell in the multiscale partition. Next we quantify deviations for a fixed node (the cleanest statement), and afterwards one can apply a union bound if one wants a statement holding simultaneously for all nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3 (concentration for a fixed node).&lt;/strong&gt;&lt;br /&gt;
Let us fix a node \(u\) at depth \(d\). Then for all \(t&amp;gt;0\),&lt;/p&gt;

\[\Pr\left(\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert\ge \frac{t}{n}\right)
\le 2\exp\left(-\frac{t^2}{2d}\right)\]

&lt;p&gt;Equivalently, we can say for any \(\delta\in(0,1)\), with probability at least \(1-\delta\),&lt;/p&gt;

\[\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert
\le \frac{\sqrt{2d\log(2/\delta)}}{n}\]

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Let \(v_0,v_1,\dots,v_d\) be the unique path from the root \(v_0\) to the node \(v_d=u\). For each step \(k=1,\dots,d\), let \(\beta_k\in[0,1]\) denote the population fraction of the chosen child along the path (i.e., if \(v_k\) is the left child of \(v_{k-1}\), then \(\beta_k = N_{v_k}/N_{v_{k-1}}\); if it is the right child, \(\beta_k = N_{v_k}/N_{v_{k-1}}\) as well). With this definition we always have \(\frac{N_{v_k}}{N_{v_{k-1}}} = \beta_k\) and thus, \(\frac{N_{v_k}}{N} = \left(\prod_{j=1}^k \beta_j\right)\). Next, define the ideal real valued target counts&lt;/p&gt;

\[\mu_k = n\cdot\frac{N_{v_k}}{N}\]

&lt;p&gt;and define the errors&lt;/p&gt;

\[e_k = n_{v_k}-\mu_k\]

&lt;p&gt;We will express \(e_d\) as a sum of bounded martingale differences.&lt;/p&gt;

&lt;p&gt;At step \(k\), conditional on the past (i.e. on \(n_{v_{k-1}}\)), the algorithm sets&lt;/p&gt;

\[n_{v_k} = n_{v_{k-1}}\beta_k + \rho_k\]

&lt;p&gt;where the rounding noise&lt;/p&gt;

\[\rho_k = n_{v_k} - n_{v_{k-1}}\beta_k\]

&lt;p&gt;satisfies \(\mathbb E[\rho_k\mid \mathcal F_{k-1}] = 0\) and \(\lvert\rho_k\rvert\le 1\). The expectation is zero because randomized rounding is unbiased. It rounds \(t=n_{v_{k-1}}\beta_k\) up with probability equal to its fractional part. The magnitude bound holds because \(\rho_k\in\{-\theta,1-\theta\}\) for some \(\theta\in[0,1)\).&lt;/p&gt;

&lt;p&gt;Also, by definition of \(\mu_k\) and the fact that \(N_{v_k}/N_{v_{k-1}}=\beta_k\),&lt;/p&gt;

\[\begin{aligned}
\mu_k
&amp;amp;= n\cdot\frac{N_{v_k}}{N}\\
&amp;amp;= n\cdot\frac{N_{v_{k-1}}}{N}\cdot\frac{N_{v_k}}{N_{v_{k-1}}}\\
&amp;amp;= \mu_{k-1}\beta_k
\end{aligned}\]

&lt;p&gt;Subtracting \(\mu_k\) from \(n_{v_k}\) produces&lt;/p&gt;

\[\begin{aligned}
e_k
&amp;amp;= n_{v_k}-\mu_k\\
&amp;amp;= \bigl(n_{v_{k-1}}\beta_k+\rho_k\bigr) - \mu_{k-1}\beta_k\\
&amp;amp;= \beta_k(n_{v_{k-1}}-\mu_{k-1}) + \rho_k\\
&amp;amp;= \beta_k e_{k-1} + \rho_k
\end{aligned}\]

&lt;p&gt;Now unroll the recursion. Since \(e_0 = n-\mu_0 = n-n = 0\), repeated substitution gives&lt;/p&gt;

\[\begin{aligned}
e_d
&amp;amp;= \beta_d e_{d-1} + \rho_d\\
&amp;amp;= \beta_d(\beta_{d-1} e_{d-2}+\rho_{d-1}) + \rho_d\\
&amp;amp;= \beta_d\beta_{d-1}e_{d-2} + \beta_d\rho_{d-1} + \rho_d\\
&amp;amp;\ \ \vdots\\
&amp;amp;= \sum_{k=1}^d \rho_k \prod_{j=k+1}^d \beta_j
\end{aligned}\]

&lt;p&gt;Define&lt;/p&gt;

\[X_k = \rho_k \prod_{j=k+1}^d \beta_j\]

&lt;p&gt;Then \(e_d = \sum_{k=1}^d X_k\). Moreover, \(\prod_{j=k+1}^d \beta_j\) is measurable with respect to \(\mathcal F_{k-1}\) (it depends only on the fixed tree and the path, not on future randomness), so&lt;/p&gt;

\[\mathbb E[X_k\mid \mathcal F_{k-1}]
= \left(\prod_{j=k+1}^d \beta_j\right)\mathbb E[\rho_k\mid\mathcal F_{k-1}]
= 0\]

&lt;p&gt;Thus \(X_k\) are martingale differences. Finally, since each \(\beta_j\in[0,1]\) and \(\lvert\rho_k\rvert\le 1\),&lt;/p&gt;

\[\lvert X_k\rvert
\le \lvert\rho_k\rvert\cdot \prod_{j=k+1}^d \lvert\beta_j\rvert
\le 1\]

&lt;p&gt;Azuma–Hoeffding for martingales with bounded increments now gives, for all \(t&amp;gt;0\),&lt;/p&gt;

\[\Pr\left(\lvert e_d\rvert\ge t\right)
\le 2\exp\left(-\frac{t^2}{2\sum_{k=1}^d 1^2}\right)
= 2\exp\left(-\frac{t^2}{2d}\right)\]

&lt;p&gt;Finally, observe&lt;/p&gt;

\[\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert
= \left\lvert\frac{n_{v_d}}{n}-\frac{\mu_d}{n}\right\rvert
= \frac{\lvert e_d\rvert}{n}\]

&lt;p&gt;So&lt;/p&gt;

\[\Pr\left(\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert\ge \frac{t}{n}\right)
= \Pr\left(\lvert e_d\rvert\ge t\right)
\le 2\exp\left(-\frac{t^2}{2d}\right)\]

&lt;p&gt;which is the first claim. The second follows by setting \(t=\sqrt{2d\log(2/\delta)}\) and rearranging. \(\square\)&lt;/p&gt;

&lt;p&gt;This should be the Aha moment! Deterministic rounding produces a worst-case guarantee of order \(\mathrm{depth}(u)/n\), while randomized rounding produces deviations of order \(\sqrt{\mathrm{depth}(u)}/n\) for a fixed node, with the tail bounds.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;An Interlude: Why Optimal Transport Shows Up Inevitably&lt;/h1&gt;

&lt;p&gt;At this point we have a completely explicit construction where we choose a multiscale partition tree \(\mathcal T\), then build a sample whose mass in every tree cell matches the population mass up to a provable error. In other words, we chose a family of “questions” and forced the sample to answer them almost the same way as the full dataset.&lt;/p&gt;

&lt;p&gt;There is, however, a downside to any partition-based notion of similarity. It is coordinate-dependent and bin-dependent. If I shift a continuous feature by a tiny amount, the “shape” of the distribution hasn’t changed much, but a histogram can change a lot if points cross a bin boundary. The tree discrepancy \(d_{\mathcal T}\) is not “wrong,” but it is answers only a particular kind of question.”How much mass lies in each region?” But there is another question you might ask that is more geometric in nature.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If I imagine the full dataset distribution as a pile of sand and the sampled dataset distribution as another pile of sand, how expensive is it to reshape one pile into the other, if moving sand by distance \(r\) costs \(r\)?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That question is optimal transport. The conceptual leap is that, instead of choosing a function class \(\mathcal F\) or a partition \(\mathcal T\) up front, we choose a ground metric on feature space (a notion of distance between rows), and we measure distributional mismatch by the minimal cost of moving probability mass through this geometric landscape.&lt;/p&gt;

&lt;p&gt;I think this viewpoint is really teachable because it has (i) a clean variational definition, (ii) an exact closed form in one dimension that can be proved, and (iii) a direct algorithmic “coreset” formulation that matches our original downsampling goal.&lt;/p&gt;

&lt;h3&gt;Optimal Transport and Wasserstein Distance&lt;/h3&gt;

&lt;p&gt;Let \(\mu\) and \(\nu\) be probability measures on a metric space \((\mathcal X, d)\). A &lt;strong&gt;coupling&lt;/strong&gt; of \(\mu\) and \(\nu\) is a joint distribution \(\pi\) on \(\mathcal X\times\mathcal X\) whose first marginal is \(\mu\) and second marginal is \(\nu\). The set of all couplings is denoted \(\Pi(\mu,\nu)\).&lt;/p&gt;

&lt;p&gt;For \(p\ge 1\), the &lt;strong&gt;Wasserstein-\(p\) distance&lt;/strong&gt; is&lt;/p&gt;

\[W_p(\mu,\nu) =
\left(
\inf_{\pi\in\Pi(\mu,\nu)}
\int_{\mathcal X\times\mathcal X} d(x,x&apos;)^p \, d\pi(x,x&apos;)
\right)^{1/p}\]

&lt;p&gt;For the purposes of this post, the case \(p=1\) is already very informative:&lt;/p&gt;

\[W_1(\mu,\nu) =
\inf_{\pi\in\Pi(\mu,\nu)}
\int_{\mathcal X\times\mathcal X} d(x,x&apos;) \, d\pi(x,x&apos;)\]

&lt;p&gt;When \(\mu\) and \(\nu\) are empirical measures, this becomes a finite-dimensional linear program. Specifically, suppose&lt;/p&gt;

\[\mu = \sum_{i=1}^N a_i\,\delta_{x_i}
\quad
\nu = \sum_{j=1}^m b_j\,\delta_{z_j}\]

&lt;p&gt;where \(a_i\ge 0\), \(b_j\ge 0\), \(\sum_i a_i=1\), \(\sum_j b_j=1\). A coupling is then a nonnegative matrix \(\Pi\in\mathbb R_{\ge 0}^{N\times m}\) whose row/column sums match \(a\) and \(b\):&lt;/p&gt;

\[\sum_{j=1}^m \Pi_{ij} = a_i \quad \forall i,
\quad
\sum_{i=1}^N \Pi_{ij} = b_j \quad \forall j\]

&lt;p&gt;The transport cost is&lt;/p&gt;

\[\sum_{i=1}^N\sum_{j=1}^m \Pi_{ij}\, d(x_i,z_j)\]

&lt;p&gt;so&lt;/p&gt;

\[W_1(\mu,\nu)
=
\min_{\Pi\ge 0}
\left\{
\sum_{i=1}^N\sum_{j=1}^m \Pi_{ij}\, d(x_i,z_j)
\ :\ 
\Pi\mathbf 1=a,\ \Pi^\top \mathbf 1=b
\right\}\]

&lt;p&gt;This is the first important connection to downsampling. OT is naturally a problem of replacing a big empirical measure by a smaller one while keeping them close in the geometric sense.&lt;/p&gt;

&lt;h3&gt;OT as a Downsampling Objective and the Wasserstein Coreset Problem&lt;/h3&gt;

&lt;p&gt;Return to our dataset distribution&lt;/p&gt;

\[P_I = \frac1N\sum_{i\in I}\delta_{x_i}\]

&lt;p&gt;Suppose we want a summary measure supported on only \(n\) points with equal weights:&lt;/p&gt;

\[\nu = \frac1n\sum_{k=1}^n \delta_{z_k}\]

&lt;p&gt;If we allow the support points \(z_k\) to be anywhere in \(\mathcal X\), one natural best-summary problem is:&lt;/p&gt;

\[\min_{z_1,\dots,z_n\in\mathcal X}
W_1\left(P_I,\ \frac1n\sum_{k=1}^n\delta_{z_k}\right)\]

&lt;p&gt;If we require that the summary to be a subset of the original points (which is what downsampling refers to), we add the constraint: \(z_k \in \{x_i : i\in I\}\). This becomes a Wasserstein-type of facility location problem: Choose a set of “representative points” so that transporting mass from each original point to representatives is low-cost. And if we also want to enforce label proportions, we do exactly what we did before. Solve this problem separately within each label class, which would produce \(\nu_y\), which would be supported on \(n_y\) points for each class, then union the supports.&lt;/p&gt;

&lt;p&gt;At this stage, the definition is clear but it might still feel abstract. I’m going to boil this down to one dimension so that my point becomes clear.&lt;/p&gt;

&lt;h2&gt;The One-dimensional Case Where OT Becomes Quantiles&lt;/h2&gt;

&lt;p&gt;Assume \(\mathcal X=\mathbb R\) with distance \(d(x,x&apos;)=\lvert x-x&apos;\rvert\), and let \(\mu\) be any probability measure on \(\mathbb R\) with CDF \(F\). Let \(\nu\) be any probability measure on \(\mathbb R\) with CDF \(G\).&lt;/p&gt;

&lt;p&gt;A foundational fact is that in one dimension, the optimal coupling is the monotone one, and the Wasserstein-1 distance has a closed form:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 4 (quantile representation of \(W_1\) on \(\mathbb R\)).&lt;/strong&gt;
Let \(F^{-1}\) and \(G^{-1}\) denote the generalized inverse CDFs. Then&lt;/p&gt;

\[W_1(\mu,\nu) = \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\]
&lt;/blockquote&gt;

&lt;p&gt;I’ll prove this because it provides us with the link between the geometric quality of transport and the notion of “sampling via quantiles,” and it is also the statement we will use to design an optimal \(n\)-point approximation.&lt;/p&gt;

&lt;h3&gt;Proof of Theorem 4&lt;/h3&gt;

&lt;p&gt;Define the quantile functions&lt;/p&gt;

\[F^{-1}(t) = \inf\{x\in\mathbb R : F(x)\ge t\}\]

\[G^{-1}(t) = \inf\{x\in\mathbb R : G(x)\ge t\}\]

&lt;p&gt;Let \(U\sim \mathrm{Unif}[0,1]\) and define random variables \(X = F^{-1}(U)\) and \(Y = G^{-1}(U)\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: \(X\sim\mu\) and \(Y\sim\nu\).&lt;/strong&gt;&lt;br /&gt;
We show \(\mathbb P(X\le x)=F(x)\). Indeed,&lt;/p&gt;

\[\begin{aligned}
\mathbb P(X\le x)
&amp;amp;= \mathbb P\bigl(F^{-1}(U)\le x\bigr)\\
&amp;amp;= \mathbb P\left(U \le F(x)\right)
\end{aligned}\]

&lt;p&gt;where the last step is a standard property of generalized inverses: \(F^{-1}(u)\le x\) iff \(u\le F(x)\). Since \(U\) is uniform,&lt;/p&gt;

\[\mathbb P(U\le F(x)) = F(x)\]

&lt;p&gt;Thus \(X\) has CDF \(F\), so \(X\sim\mu\). The same argument gives \(Y\sim\nu\).&lt;/p&gt;

&lt;p&gt;Therefore, the joint law of \((X,Y)\) defines a coupling \(\pi\) of \(\mu\) and \(\nu\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2: the induced transport cost equals the integral.&lt;/strong&gt;&lt;br /&gt;
By definition of \((X,Y)\),&lt;/p&gt;

\[\begin{aligned}
\mathbb E\left[\lvert X-Y\rvert\right]
&amp;amp;= \mathbb E\left[\left\lvert F^{-1}(U)-G^{-1}(U)\right\rvert\right]\\
&amp;amp;= \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt
\end{aligned}\]

&lt;p&gt;So we have produced a coupling with cost equal to the RHS, and therefore&lt;/p&gt;

\[W_1(\mu,\nu)\le \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\]

&lt;p&gt;&lt;strong&gt;Step 3: optimality of monotone coupling.&lt;/strong&gt;&lt;br /&gt;
Let \(\pi\) be any coupling of \(\mu\) and \(\nu\), and let \((X,Y)\sim\pi\). Consider the function class of 1-Lipschitz functions \(\varphi:\mathbb R\to\mathbb R\), i.e.&lt;/p&gt;

\[\lvert\varphi(x)-\varphi(x&apos;)\rvert\le \lvert x-x&apos;\rvert\]

&lt;p&gt;For any such \(\varphi\),&lt;/p&gt;

\[\begin{aligned}
\mathbb E[\varphi(X)]-\mathbb E[\varphi(Y)]
&amp;amp;= \mathbb E[\varphi(X)-\varphi(Y)]\\
&amp;amp;\le \mathbb E\left[\lvert\varphi(X)-\varphi(Y)\rvert\right]\\
&amp;amp;\le \mathbb E\left[\lvert X-Y\rvert\right]
\end{aligned}\]

&lt;p&gt;Taking the supremum over all 1-Lipschitz \(\varphi\) produces&lt;/p&gt;

\[\sup_{\mathrm{Lip}(\varphi)\le 1} \left(\mathbb E_\mu[\varphi]-\mathbb E_\nu[\varphi]\right)
\le \mathbb E_\pi\left[\lvert X-Y\rvert\right]\]

&lt;p&gt;Since this holds for every coupling \(\pi\),&lt;/p&gt;

\[\sup_{\mathrm{Lip}(\varphi)\le 1} \left(\mathbb E_\mu[\varphi]-\mathbb E_\nu[\varphi]\right)
\le W_1(\mu,\nu)\]

&lt;p&gt;Now, in one dimension, one can verify that&lt;/p&gt;

\[\sup_{\mathrm{Lip}(\varphi)\le 1} \left(\mathbb E_\mu[\varphi]-\mathbb E_\nu[\varphi]\right)
=
\int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\]

&lt;p&gt;Combining with the upper bound from Step 2 gives equality, proving the theorem. \(\square\)&lt;/p&gt;

&lt;h2&gt;The quantile coreset is optimal for 1D \(W_1\)&lt;/h2&gt;

&lt;p&gt;Now we use Theorem 4 to solve the downsampling problem in 1D, fully. First, fix \(n\ge 1\). Consider the class of measures supported on exactly \(n\) points with equal weights:&lt;/p&gt;

\[\nu = \frac1n\sum_{k=1}^n \delta_{z_k}\]

&lt;p&gt;Such a \(\nu\) has a quantile function that is a step function: if we order the support \(z_{(1)}\le \cdots\le z_{(n)}\), then&lt;/p&gt;

\[G^{-1}(t) = z_{(k)}\qquad\text{for }t\in\left(\frac{k-1}{n},\frac{k}{n}\right]\]

&lt;p&gt;Plugging this into Theorem 4 provides us with&lt;/p&gt;

\[\begin{aligned}
W_1(\mu,\nu)
&amp;amp;= \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\\
&amp;amp;= \sum_{k=1}^n \int_{(k-1)/n}^{k/n} \left\lvert F^{-1}(t)-z_{(k)}\right\rvert\,dt
\end{aligned}\]

&lt;p&gt;Crucially, the RHS separates across intervals, so the minimization over \(z_{(k)}\) decouples.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Lemma 5 (best constant on an interval is a median).&lt;/strong&gt;
Let \(g:[a,b]\to\mathbb R\) be integrable. Define&lt;/p&gt;

\[\Phi(z)=\int_a^b \lvert g(t)-z\rvert\,dt\]

  &lt;p&gt;Then any minimizer of \(\Phi(z)\) is a median of the pushforward distribution of \(g(t)\) under uniform measure on \([a,b]\). In particular, if \(g\) is monotone, a minimizer is \(g((a+b)/2)\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Fix \(z\) and consider the derivative of \(\Phi\) where it exists. For any \(z\) that is not equal to a value attained by \(g(t)\) on a set of positive measure, we can differentiate under the integral sign:&lt;/p&gt;

\[\begin{aligned}
\Phi(z)
&amp;amp;= \int_a^b \lvert g(t)-z\rvert\,dt\\
&amp;amp;= \int_{g(t)\ge z} (g(t)-z)\,dt + \int_{g(t)&amp;lt;z} (z-g(t))\,dt
\end{aligned}\]

&lt;p&gt;Differentiating with respect to \(z\) gives&lt;/p&gt;

\[\begin{aligned}
\Phi&apos;(z)
&amp;amp;= \int_{g(t)\ge z} (-1)\,dt + \int_{g(t)&amp;lt;z} (1)\,dt\\
&amp;amp;= \lambda(\{t: g(t)&amp;lt;z\}) - \lambda(\{t: g(t)\ge z\})
\end{aligned}\]

&lt;p&gt;where \(\lambda\) denotes the Lebesgue measure on \([a,b]\). A minimizer must satisfy \(0\in \partial \Phi(z)\), equivalently that the measure of the set where \(g(t)\le z\) is at least half and the measure where \(g(t)\ge z\) is at least half. This is exactly the median condition. If \(g\) is monotone, then the median is attained at the midpoint \(t=(a+b)/2\). \(\square\)&lt;/p&gt;

&lt;p&gt;Apply this lemma with \(g(t)=F^{-1}(t)\) on each interval \(((k-1)/n,k/n]\). Since \(F^{-1}\) is monotone nondecreasing, the optimal choice on that interval is the midpoint quantile:&lt;/p&gt;

\[z_{(k)}^\star = F^{-1}\left(\frac{(k-1)/n + k/n}{2}\right) = F^{-1}\left(\frac{2k-1}{2n}\right)\]

&lt;p&gt;We have proved:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 6 (optimal 1D OT coreset via quantiles).&lt;/strong&gt;
Let \(\mu\) be a probability measure on \(\mathbb R\) with the CDF \(F\). Across all of the measures of the form \(\nu=\frac1n\sum_{k=1}^n\delta_{z_k}\), the minimizer of \(W_1(\mu,\nu)\) is achieved by taking&lt;/p&gt;

\[z_k^\star = F^{-1}\left(\frac{2k-1}{2n}\right),\quad k=1,\dots,n\]
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Combine the quantile representation in Theorem 4, the interval decomposition above, and Lemma 5 applied on each interval. Each term is minimized by the midpoint quantile, and so the sum is minimized. \(\square\)&lt;/p&gt;

&lt;p&gt;This result is both rigorous and pedagogically pretty nice. In one dimension, the best downsampling in Wasserstein-1 is essentially equivalent to keeping evenly spaced quantiles.&lt;/p&gt;

&lt;p&gt;And it explains, why OT feels like the right notion of distribution preservation for continuous features. Shifting all points by a small amount changes the Wasserstein distance by that small amount.&lt;/p&gt;

&lt;h2&gt;From the Theorem to an Algorithm (in the 1D case)&lt;/h2&gt;

&lt;p&gt;If you have data points \(x_1,\dots,x_N\in\mathbb R\) and you want an \(n\)-point OT coreset with equal weights, you can implement Theorem 6 directly using order statistics.&lt;/p&gt;

&lt;p&gt;Let \(x_{(1)}\le \cdots\le x_{(N)}\) be the sorted data. The empirical CDF has quantile function&lt;/p&gt;

\[F_N^{-1}(t)=x_{(\lceil Nt\rceil)}\]

&lt;p&gt;So the OT-optimal equal weight summary points are&lt;/p&gt;

\[z_k = x_{\left(\left\lceil N\cdot \frac{2k-1}{2n}\right\rceil\right)},\quad k=1,\dots,n\]

&lt;p&gt;A simple implementation is:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Algorithm: 1D-OT-Quantile-Coreset(x[1..N], n)
1. Sort x to obtain x_(1) &amp;lt;= ... &amp;lt;= x_(N)
2. For k = 1..n:
       idx = ceil( N * (2k - 1) / (2n) )
       z_k = x_(idx)
3. Return {z_1, ..., z_n}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This gives an exact minimizer of \(W_1(P_I,\nu)\) among all equal weight \(n\)-point measures supported on the real line, where \(P_I\) is the empirical distribution of the data.&lt;/p&gt;

&lt;h2&gt;What about Higher Dimensions?&lt;/h2&gt;

&lt;p&gt;In \(d&amp;gt;1\), the same objective&lt;/p&gt;

\[\min_{\nu:\ \lvert\mathrm{supp}(\nu)\rvert=n} W_1(P_I,\nu)\]

&lt;p&gt;is still meaningful, but no longer has a simple quantile formula. Nonetheless, the OT viewpoint remains valuable for three reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It gives a coordinate free notion of distributional similarity&lt;/li&gt;
  &lt;li&gt;It directly rewards summaries that preserve the geometry and clustering structure in feature space&lt;/li&gt;
  &lt;li&gt;It connects to a large algorithmic toolbox of methods from linear programming, entropic regularization, and OT-inspired clustering/medoid selection&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A common practical restriction is to insist that \(\nu\) is supported on a subset of the observed points (true downsampling), and to take equal weights. Then we are choosing representative points that minimize the transport cost from the full empirical distribution. This is related in spirit to facility location and \(k\)-medoids, but with the framework of optimal transport&lt;/p&gt;

&lt;p&gt;For the purposes of this post, the key conceptual takeaway is that multiscale tree construction preserved distribution by matching masses on a chosen hierarchy of regions. OT preserves distribution by minimizing the geometric cost of moving mass. They are two different “languages” for the same core objective which is to replace a huge dataset by a smaller one without changing what a learner would perceive as the underlying world.&lt;/p&gt;

&lt;p&gt;Finally, to fold OT back into our original supervised setting, we can apply OT label conditionally. For each \(y\in\mathcal Y\), compute an OT based summary of the empirical conditional distribution&lt;/p&gt;

\[P_{I_y}=\frac1{N_y}\sum_{i\in I_y}\delta_{x_i}\]

&lt;p&gt;using \(n_y\) points, then union the supports across \(y\). This enforces the label proportions exactly while using OT to preserve the features of each class-conditional feature distribution.&lt;/p&gt;

&lt;p&gt;In the next section, I’ll connect this OT idea back to the multiscale tree idea a bit more concretely. One can view Wasserstein distance as controlling discrepancies against 1-Lipschitz test functions, while tree discrepancy controls discrepancies against indicator functions of tree cells. That comparison is a clear way to understand what each method preserves and why they complement each other.&lt;/p&gt;

&lt;h2&gt;A Brief Connection between Two Distances and Two Question Sets&lt;/h2&gt;

&lt;p&gt;The multiscale tree sampler and optimal transport are best thought of as answering the same meta question with different choices of “what counts as evidence that two distributions differ.”&lt;/p&gt;

&lt;p&gt;Fix a label \(y\) and suppress it in notation. We are comparing the empirical measures&lt;/p&gt;

\[P_I=\frac1N\sum_{i\in I}\delta_{x_i},
\quad
P_S=\frac1n\sum_{i\in S}\delta_{x_i}\]

&lt;p&gt;The tree discrepancy tied to a partition tree \(\mathcal T\) is&lt;/p&gt;

\[d_{\mathcal T}(S)
=\max_{u\in\mathcal T}\left\lvert\frac{\lvert S\cap A_u\rvert}{n}-\frac{\lvert A_u\rvert}{N}\right\rvert\]

&lt;p&gt;But each node set \(A_u\) is really just an indicator query. Define&lt;/p&gt;

\[f_u(x)=\mathbf 1\{x\in\mathcal C_u\}\]

&lt;p&gt;where \(\mathcal C_u\subseteq\mathcal X\) is the corresponding region of feature space. Then&lt;/p&gt;

\[\begin{aligned}
\mathbb E_{P_S}[f_u(X)]
&amp;amp;=\frac1n\sum_{i\in S}\mathbf 1\{x_i\in\mathcal C_u\}
=\frac{\lvert S\cap A_u\rvert}{n}\\
\mathbb E_{P_I}[f_u(X)]
&amp;amp;=\frac1N\sum_{i\in I}\mathbf 1\{x_i\in\mathcal C_u\}
=\frac{\lvert A_u\rvert}{N}
\end{aligned}\]

&lt;p&gt;So, exactly,&lt;/p&gt;

\[d_{\mathcal T}(S)
=
\max_{u\in\mathcal T}\left\lvert\mathbb E_{P_S}[f_u(X)]-\mathbb E_{P_I}[f_u(X)]\right\rvert\]

&lt;p&gt;In words, this means that tree balancing controls discrepancies against a large, multiscale family of indicator functions (cell membership queries).&lt;/p&gt;

&lt;p&gt;Optimal transport controls a different family. For \(W_1\) on a metric space \((\mathcal X,d)\), the Kantorovich–Rubinstein duality says&lt;/p&gt;

\[W_1(P_I,P_S)
=
\sup_{\mathrm{Lip}(f)\le 1}\left\lvert\mathbb E_{P_I}[f(X)]-\mathbb E_{P_S}[f(X)]\right\rvert\]

&lt;p&gt;so, OT controls discrepancies against all 1-Lipschitz functions.&lt;/p&gt;

&lt;p&gt;One small proposition makes the guarantee for our tree-based method clear.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Proposition (tree discrepancy controls piecewise-constant observables).&lt;/strong&gt;
Let \(\mathcal T\) be a partition tree with leaf set \(\mathcal L\). Suppose a function \(g:\mathcal X\to\mathbb R\) is constant on leaves, i.e. there exist constants \(c_\ell\) such that \(g(x)=c_\ell\) whenever \(x\in\mathcal C_\ell\). Then&lt;/p&gt;

\[\left\lvert\mathbb E_{P_S}[g(X)]-\mathbb E_{P_I}[g(X)]\right\rvert
\le
\left(\max_{\ell\in\mathcal L}\left\lvert\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right\rvert\right)
\sum_{\ell\in\mathcal L}\lvert c_\ell\rvert\]
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Since \(g\) is leaf-piecewise-constant,&lt;/p&gt;

\[g(x)=\sum_{\ell\in\mathcal L} c_\ell\,\mathbf 1\{x\in\mathcal C_\ell\}\]

&lt;p&gt;Therefore&lt;/p&gt;

\[\begin{aligned}
\mathbb E_{P_S}[g(X)]-\mathbb E_{P_I}[g(X)]
&amp;amp;=\sum_{\ell\in\mathcal L} c_\ell\left(\mathbb E_{P_S}[\mathbf 1\{X\in\mathcal C_\ell\}] - \mathbb E_{P_I}[\mathbf 1\{X\in\mathcal C_\ell\}]\right)\\
&amp;amp;=\sum_{\ell\in\mathcal L} c_\ell\left(\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right)
\end{aligned}\]

&lt;p&gt;Taking absolute values and applying the triangle inequality,&lt;/p&gt;

\[\begin{aligned}
\left\lvert\mathbb E_{P_S}[g(X)]-\mathbb E_{P_I}[g(X)]\right\rvert
&amp;amp;\le \sum_{\ell\in\mathcal L} \lvert c_\ell\rvert\left\lvert\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right\rvert\\
&amp;amp;\le \left(\max_{\ell\in\mathcal L}\left\lvert\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right\rvert\right)\sum_{\ell\in\mathcal L}\lvert c_\ell\rvert
\end{aligned}\]

\[\square\]

&lt;p&gt;This is the simplest statement of what the guarantee buys you. If your model or a piece of it behaves like a piecewise-constant function on your chosen multiscale partition, then the tree balancing forces its empirical averages to be stable under downsampling. OT, on the other hand, controls all averages of 1-Lipschitz functions, and so, it is naturally robust to small geometric perturbations.&lt;/p&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;One of the things I’ve come to appreciate the most about teaching is how often genuinely difficult problems can pop up disguised as mundane ones. Here, it was simply the case that a laptop couldn’t handle a dataset, which I suppose, is a pretty common problem (as complexity theory has been around for a while). But, when the problem was first posed to me in office hours, I truly did think a straightforward solution could exist somewhere in the documentation of Sci-kit Learn. This problem– and these kinds of problems in general– sound like logistical issues until you try to make them precise. And then, you realize you’ve been handed a real mathematical question that can only really be tackled using makeshift techniques and approximation tools. That shift in perspective is, to me, the main reward and most fascinating part of problem solving. It’s a reminder that there are technical problems that can show up all over the place yet remain relatively hidden. They show up wherever someone is trying to do something simple and quickly discover that the simplicity actually covers hidden structure that is much more complex.&lt;/p&gt;

&lt;p&gt;In this post I dove into one such problem. The multiscale tree construction gave a checkable notion of what it means to preserve a dataset, and it gave an provably-correct algorithm that actually produces such samples. Optimal transport offered a different lens that dependend more on the geometry of distributions. In one dimension it turned an abstract idea toward specifics, leveraging quantiles.&lt;/p&gt;

&lt;p&gt;But the real point here is broader than the downsampling problem. If you’re lucky, you’ll keep running into problems like this that are practical, slightly annoying, but amazingly elegant once you pause long enough to chalk out the problem. You don’t need to solve them perfectly on the first pass. You just need to take a stab, write down a definition that you believe in, and see the power that mathematics and modeling gives to you.&lt;/p&gt;

&lt;h1&gt;Recommended Readings / Works that Served as Inspiration for this Post&lt;/h1&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="statistics" />
      

      

      
        <summary type="html">I’ve had the pleasure of being a teaching assistant for one of Penn’s flagship machine learning courses for the past three semesters now, and I’ve enjoyed the experience so much. There are many amazing parts of the job I can speak to, but I believe the most fruitful and memorable ones that keep me coming back semester after semester have all been from the relationships I’ve gotten to form with students, faculty, and my peer TAs alike. This semester, we added a final project submission to the course curriculum, in addition to the usual homeworks and exams. And along with the final project requirements came a schedule of checkpoints throughout the semester in which project groups were meant to meet with their assigned TAs. Effectively, this gave the TAs the opportunity to serve as mentors for project groups. I was able to work with many amazing groups and was so happy to see the amount of commitment students were putting toward their projects. I was particularly impressed with one of my groups that was working on building and evaluating an ML-based approach toward the joint prediction of the occurence and magnitude of flight delays from a large U.S. domestic flights dataset. The group was not only diligent in the completion of the project, but they also asked insightful and interesting questions that clearly showed they were thinking carefully about the nuances of the problem they were tackling. In fact, one of their questions was especially fascinating to me, as it was a very real, but complex problem that popped up in a seemingly simple setting. They had the following situation: The dataset is simply too big to work with locally on laptops. Training models on the data takes way too long. So, take a sample from the dataset such that the label distribution in the sample remains the same as that of the complete data. However, what issues does this cause for training now? What if there are fundamental differences in the feature distributions of the rows in the sample? And crucially, how can a sample be selected such that feature distributions from the original dataset are jointly preserved across all features? After our meeting, I dug deeper into the problem and found out it was a documented challenge. I was hooked, and so, the general setup of the problem has sat in the back of my head for some weeks. In this post, I detail some of the thoughts I’ve had and findings I’ve made. Roadmap and Setup Informally, we can think of this problem as “sampling without changing the dataset.” But what does this mean? How should we think about this more carefully? A dataset is an empirical distribution, and a sample is another empirical distribution. Once we re-orient our thinking in this way, the whole problem becomes clearer, and, as we’ll soon see, much more delicate than it looks. Let me fix notation in a way that doesn’t assume anything special about the split, and doesn’t even assume binary labels. Suppose we have a dataset \[D={(x_i,y_i)}_{i=1}^N\] where each \(y_i\) belongs to a finite label set \(\mathcal Y\) (for classification), and each \(x_i\) is a vector of features in some product space \(\mathcal X\) (think \(\mathbb R^d\) for numerical features crossed with a finite product of categorical alphabets for discrete features). There is no “true distribution” in the background here. At least, we don’t need to assume one. All that matters is the empirical distribution induced by the observed dataset: \[P_D =\frac1N\sum_{i=1}^N \delta_{(x_i,y_i)}\] where \(\delta_z\) denotes a point mass at \(z\). Interpret this simply as \(P_D(A)\) for a set \(A\subseteq \mathcal X\times\mathcal Y\) is simply “the fraction of rows whose \((x_i,y_i)\) land in \(A\).” Now imagine we want to keep only \(n\ll N\) rows. We pick an index subset \(S \subseteq {1,\dots,N}\) with \(\lvert S \rvert=n\), and form the sampled dataset \(D_S = {(x_i,y_i): i\in S}\). This induces its own empirical distribution \[P_S =\frac1n\sum_{i\in S}\delta_{(x_i,y_i)}\] So the vague problem of “pick a representative sample” can be stated as the following mathematical objective: choose (S) so that \(P_S\) is close to \(P_D\). Where do labels come in? Usually, the first constraint that people impose is to preserve the label proportions. If \(\widehat p_y = P_D(Y=y)\) denotes the empirical label frequencies of the full dataset, we would like the sample to satisfy \(P_S(Y=y) \approx \widehat p_y\) for all \(y\in\mathcal Y\). Most of the time we can enforce this approximately by rounding, or exactly by committing to target sample sizes where \(n_y \in \mathbb Z_{\ge 0}\) and \(\sum_{y\in\mathcal Y} n_y = n\), and requiring \(\lvert S\cap I_y \rvert=n_y\), where \(I_y=\{i:y_i=y\}\). This is the simplest way to formalize the idea of “keeping the same label split.” A small but crucial observation is that this reduces the whole task to the conditional distributions. So, the empirical distribution factorizes as \(P_D(x,y)=P_D(y),P_D(x\mid y)\) and \(P_S(x,y)=P_S(y),P_S(x\mid y)\). If we force \(P_S(y)=P_D(y)\) (by enforcing the \(n_y\)), then the only remaining question is how close we can make each conditional \(P_S(x\mid y)\) to \(P_D(x\mid y)\). In other words, the “dataset reduction” problem becomes a family of independent problems, one for each label \(y\): inside the class-\(y\) points, choose \(n_y\) rows so that the feature distribution for that class is preserved. Once you solve that for each class, you combine the results and you automatically preserve the global label distribution. So we’ve reduced the problem to this: for each label \(y\), from the multiset of feature vectors \({x_i: i\in I_y}\) of size \(N_y\), select a subset \(S_y\subseteq I_y\) of size \(n_y\) so that \[P_{S_y}\approx P_{I_y} \quad\text{where}\quad P_{I_y}=\frac1{N_y}\sum_{i\in I_y}\delta_{x_i}, \quad P_{S_y}=\frac1{n_y}\sum_{i\in S_y}\delta_{x_i}\] Everything now depends on what “\(\approx\)” means. This is the point where there’s a fork in the road, and the roadmap of this post is basically about exploring two particularly enlightening branches toward getting us as close as possible to preserving the label distributions. One branch is what I’ll call the “questions you want your sample to answer” viewpoint. A dataset is valuable because you ask it questions like what’s the fraction of flights with departure delay exceeding 30 minutes? what’s the distribution of carrier codes? what is the conditional distribution of delay given month and origin airport? If we decide in advance on a large family of such “questions,” then preserving the dataset means that the full dataset and the sample should give nearly the same answers to all questions in that family. Mathematically, these “questions” are test functions \(f\) on \(\mathcal X\), and “having the same answers” means matching expectations: \[\mathbb E_{P_{S_y}}[f(X)] \approx \mathbb E_{P_{I_y}}[f(X)] \quad\text{for all }f\text{ in some function class }\mathcal F\] The challenge is to choose \(\mathcal F\) so that it is both rich (so that the guarantee we provide is actually ubiquitous and somewhat useful) and tractable (so that we can design an algorithm and prove that it works). My own approach in this post is based on choosing \(\mathcal F\) to be indicator functions of cells in a multiscale partition of the feature space. Briefly, the idea is to build a recursive partition (a tree) of the dataset into cells that correspond to coarse-to-fine “regions” in the feature space, and you ask that the sample preserves the mass of every such region. This produces a discrepancy measure \(d_{\mathcal T}(P_{S_y},P_{I_y})\) for the tree \(\mathcal T\), and then the algorithm’s task is to construct \(S_y\) so that this discrepancy is provably small. The attractive part here is that the statement “every region has almost the right mass” is something you can prove with induction, and it gives a pretty teachable correctness guarantee. The other branch is to use optimal transport (OT). Here the philosophical question is not “do these two datasets answer my pre-selected family of questions similarly?” but rather “how expensive is it to morph one of the empirical distributions into the other?” Optimal transport formalizes the notion of moving probability mass through a feature space with minimal cost. It produces a distance between distributions (like the Wasserstein distance), which is geometric, in that, if two distributions differ only by a small shift in a continuous feature, OT sees that as small, whereas histogram metrics can see it as large if your bins are not properly designed. OT is therefore a natural conceptual baseline for what it means to preserve “shape” of a distribution. In one dimension it becomes exactly quantiles, and in higher dimensions it becomes computationally harder but still interpretable. In this post I’ll use OT as a teachable interlude, partly because it’s intrinsically beautiful, and partly because it clarifies what any distribution-preserving sampling scheme is trying to approximate. To be honest, my mind immediately went to OT when I first heard this problem, and this exercise gave me the perfect excuse to teach myself the fundamentals of OT. So the plan here is as follows: first, I’ll explain why this problem is hard in a very concrete sense– hard even before you worry about fancy models– because it looks like a “simple sampling” question but is actually a constrained approximation problem over measures. Then I’ll show a construction that gives a rigorous and explicit guarantee for a meaningful notion of preservation of distributions. Finally, I’ll step back and show how OT offers a second way to view the same goal, with a complete and simple rule/theorem in one dimension and a story for tackling the general case. At the end, the hope is that you’ll walk away with two things: one “hands on” algorithm you could implement for your own project that comes with a proof of correctness, and one geometric mental model– using optimal transport– that changes how you think about what it means for a sample to be representative. Why is This a Hard Problem? At first glance, the students’ question sounds like it should have a straightforward answer, as it seems like it would be pretty commonly asked. Sampling while preserving label proportions is a built-in option in many libraries. So why does feature drift happen at all, and why can’t we just fix it by sampling more carefully? There are three distinct difficulties here, and they show up even before you think about neural networks, fancy objectives, or hyperparameter tuning. They are: (i) the object you’re trying to preserve is infinite-dimensional, (ii) exact preservation quickly becomes a combinatorial optimization problem, and (iii) high-dimensional structure forces a tradeoff between resolution and sample size. 1. Distribution Preservation is an Infinite-dimensional Constraint Fix a label \(y\in\mathcal Y\) and focus only on the class-conditional empirical feature distribution \[P_{I_y}=\frac1{N_y}\sum_{i\in I_y}\delta_{x_i}\] and its sampled counterpart \[P_{S_y}=\frac1{n_y}\sum_{i\in S_y}\delta_{x_i}\] What would it mean to “preserve the feature distribution” exactly? The strongest statement would be \[P_{S_y}=P_{I_y}\] as measures on \(\mathcal X\). But if \(n_y&amp;lt;N_y\) and the feature vectors are not massively duplicated, this is impossible, as \(P_{S_y}\) has a support size of at most \(n_y\), while \(P_{I_y}\) has support size \(N_y\). So we are forced into an approximation. Abstractly, a distribution can be thought of as an operator that assigns probabilities to all measurable sets, or equivalently a tool that assigns expectations to all bounded measurable functions. One standard way to make this precise is using an integral probability metric. Given a class of test functions \(\mathcal F\), define \[d_{\mathcal F}(P,Q)=\sup_{f\in\mathcal F}\left|\mathbb E_P[f(X)]-\mathbb E_Q[f(X)]\right|\] If \(\mathcal F\) is “too big,” then matching becomes either impossible or sample-inefficient (e.g. choosing \(\mathcal F\) as all bounded measurable functions corresponds essentially to total variation). If \(\mathcal F\) is “too small,” then the guarantee is meaningless, as you can match a few moments and still have a wildly different distribution. So the difficulty is not that we lack distances between distributions, but it’s that the notion of “preserving the distribution” necessarily requires choosing which family of distributional questions you care about. This is why the path we select in the roadmap section matters. Any solution must choose some structured class \(\mathcal F\) (or some structured metric like Wasserstein distance) that is rich enough to capture what models that we care about but simple enough that we can design algorithms and prove guarantees. 2. Even Simple Exact Matching Becomes Combinatorial (and can be NP-hard) A natural response to the previous section is, “Fine. I’ll just preserve a finite list of feature summaries like means, maybe quantiles, maybe histograms.” This is reasonable in practice, but there this is theoretically problematic. As soon as you impose exact matching constraints under a fixed sample size, you are solving a discrete feasibility problem, and those problems can be representative of classic NP-hard problems. Here is a clean reduction that captures the essence. Consider the simplest possible setting: there is a single label (so ignore stratification) there is a single numerical feature \(x_i\in\mathbb Z\) we want to choose exactly half the points Suppose we demand that the sample mean equals the full-data mean. That is, with \(n=N/2\), we want a subset \(S\subseteq{1,\dots,N}\) with \(\|S\|=n\) such that \[\frac1n\sum_{i\in S}x_i = \frac1N\sum_{i=1}^N x_i\] Now take an instance of the PARTITION problem: given integers \(a_1,\dots,a_N\) with \(N\) even, decide whether there exists a subset of exactly \(N/2\) of them whose sum is half the total. Construct data points \(x_i=a_i\). Let \[A=\sum_{i=1}^N a_i\] Then the full-data mean is \(A/N\). A subset \(S\) of size \(n=N/2\) satisfies the mean constraint iff \[\frac1n\sum_{i\in S}a_i = \frac AN \implies \sum_{i\in S}a_i = \frac nN A \implies \sum_{i\in S}a_i = \frac12 A\] So deciding whether there exists a half-sample whose mean matches the full mean is exactly PARTITION. In particular, the decision version of this “perfect mean-preserving subsample” problem is NP-complete. This is a tiny example, but it makes the key point. Once you start demanding that a subset match distributional properties exactly, you are often solving a hard combinatorial selection problem. And since realistic notions of “preserve feature distributions” typically involve matching many summaries simultaneously, exact matching is not only impossible for measure-theoretic reasons (as seen in the previous section), but also computationally intractable even when it is not impossible. The practical conclusion is that “distribution-preserving sampling” should be seen as an approximation problem with some kind of error control or minimization. 3. High-dimensional Distributions Force a Resolution vs. Sample-size Tradeoff Even if we ignore computational hardness and focus only on approximation, there is a geometric barrier that blocks the way. High-dimensional distributions cannot be preserved at fine resolution without huge sample sizes. This is essentially the curse of dimensionality, but it’s worth seeing it in a specific way. Imagine discretizing each feature into a small number of bins. Now, suppose, for simplicity, we discretize each of \(d\) numerical features into \(m\) bins (quantiles, for example). If we tried to preserve the full joint histogram over all \(d\) features, we would be working with \(m^d\) cells. Even for modest numbers like \(m=10\) and \(d=20\), this is \(10^{20}\) cells– astronomically many. Most cells are empty even in the full dataset, so the “true” empirical joint histogram is extremely sparse and unstable. A small sample cannot possibly reproduce it reliably. This is why approaches that try to preserve the full joint distribution directly are doomed except in very low dimensions or enormous sample sizes. The only viable strategy here is to choose a structured approximation to the distribution that avoids an exponential complexity. For example: you might preserve all one-dimensional marginals (feature-wise histograms) you might preserve selected pairwise interactions (a small set of correlations or conditional tables) you might preserve masses of cells in a multiscale partition (a tree), which captures coarse structure globally and finer structure locally where data supports it or you might use a geometric metric like Wasserstein distance, which implicitly balances resolution and geometry without committing to a fixed binning All of these are different ways of acknowledging the same fundamental reality in that distribution preservation is always relative to a choice of resolution (explicitly, as in partitions and histograms). The hard part is choosing a notion of similarity that is both mathematically meaningful and practically aligned with the learning problem further down the pipeline. This is exactly why the roadmap split into two branches is useful. The multiscale-partition approach makes the resolution explicit and gives clean discrepancy bounds. The optimal transport approach provides an elegant “best possible” story in one dimension, with principled approximations in higher dimensions. In the next section, I’ll start with the multiscale viewpoint, and I’ll build our approximation algorithm. The Target: Preserve Mass on a Multiscale Partition (label conditionally) We now need to make the approximation target \(P_{S_y}\approx P_{I_y}\) precise in a way that is both mathematically meaningful and algorithmically tractable. Let’s fix a finite label set \(\mathcal Y\). For each \(y\in\mathcal Y\) let \(I_y=\{i:y_i=y\}\) and \(N_y=\lvert I_y\rvert\), and choose target sample sizes \(n_y\in\mathbb Z_{\ge 0}\) with \(\sum_{y\in\mathcal Y} n_y=n\). As discussed above, we will construct the sample label conditionally, which means, for each \(y\) we select a subset \(S_y\subseteq I_y\) with \(\lvert S_y\rvert=n_y\), and then return \[S=\bigcup_{y\in\mathcal Y} S_y\] This enforces the label proportions exactly: \[P_S(Y=y)=\frac{\lvert S_y\rvert}{\lvert S\rvert}=\frac{n_y}{n}\] So here it suffices to focus on a single label \(y\) and suppress it in notation. We now have a finite population \(I\) of size \(N\) and want to select a subset \(S\subseteq I\) of size \(n\) whose empirical feature distribution is close to that of the population. Instead of trying to compare \(P_S\) and \(P_I\) on all measurable sets, which is too strong, we compare them on a structured family of sets coming from a multiscale partition of feature space. Specifically, we build a binary partition tree \(\mathcal T\) over the indices in \(I\). A partition tree \(\mathcal T\) consists of nodes \(u\), each associated with a subset \(A_u\subseteq I\), satisfying: The root node corresponds to the full class: \(A_{\mathrm{root}}=I\) Every internal node \(u\) has two children \(u_L,u_R\) such that \(A_u = A_{u_L}\sqcup A_{u_R}\) and \(A_{u_L}\cap A_{u_R}=\varnothing\). Intuitively, each node corresponds to a “cell” in feature space, that is obtained by recursively splitting on numerical thresholds or grouping categorical values. Near the root, cells are coarse. Deeper down, cells are finer. The tree is therefore a multiscale description of the feature distribution. Given such a tree, we measure how well a subset \(S\) matches the population by comparing the masses of all cells in the tree. For each node \(u\in\mathcal T\) define its population mass \(p_u = \frac{\lvert A_u\rvert}{N}\) and sample mass \(q_u = \frac{\lvert S\cap A_u\rvert}{n}\). The associated discrepancy is \[d_{\mathcal T}(S) = \max_{u\in\mathcal T} \lvert q_u-p_u\rvert\] If \(d_{\mathcal T}(S)\) is small, then for every region of feature space represented anywhere in the tree, the fraction of sampled points landing in that region is close to the fraction in the full dataset. This is a checkable notion of “distribution preservation,” and from here, we can support a rigorous proof. At this point the problem becomes algorithmic: can we construct a subset \(S\) of size \(n\) with provably small \(d_{\mathcal T}(S)\)? The answer is yes. In the next section I’ll give a multiscale balancing construction (deterministic and randomized variants) that produces this kind of \(S\) with explicit error bounds, and the proofs are short enough to hopefully make the point as clear as possible. Multiscale Balanced Rounding on a Tree Fix a label \(y\in\mathcal Y\) and suppress notation. So we have a finite population index set \(I\) of size \(N=\lvert I\rvert\) and a partition tree \(\mathcal T\) whose nodes \(u\) correspond to subsets \(A_u\subseteq I\) satisfying \[\begin{aligned} A_{\mathrm{root}} &amp;amp;= I\\ A_u &amp;amp;= A_{u_L}\sqcup A_{u_R}\ \ \text{for internal nodes }u \end{aligned}\] Our goal is to choose a subset \(S\subseteq I\) with \(\lvert S\rvert=n\) so that, for every node \(u\in\mathcal T\), the sample mass of the cell \(A_u\) is close to the population mass: \[\begin{aligned} p_u &amp;amp;= \frac{\lvert A_u\rvert}{N} \\ q_u &amp;amp;= \frac{\lvert S\cap A_u\rvert}{n} \\ d_{\mathcal T}(S) &amp;amp;= \max_{u\in\mathcal T} \lvert q_u-p_u\rvert \end{aligned}\] The key idea is to separate the problem into two steps: Allocate integer sample counts to every node of the tree in a way that respects the tree structure and tracks the population proportions Once the leaves know how many points they should contribute, sample that many points from each leaf and take the union This works because, if the leaf counts are consistent, then every internal node automatically receives the correct count. This is because internal nodes are unions of leaves. So the “distribution preservation” problem reduces to this tree rounding problem. Preliminaries: Notation for Counts For each node \(u\) define the population count \[N_u = \lvert A_u\rvert\] We will construct integers \(n_u\) (sample counts) for every node \(u\) such that \[\begin{aligned} n_{\mathrm{root}} &amp;amp;= n\\ n_u &amp;amp;= n_{u_L}+n_{u_R}\ \ \text{for every internal node }u \end{aligned}\] Once this is done, we will sample exactly \(n_\ell\) indices from each leaf \(\ell\), and define \[\begin{aligned} S &amp;amp;= \bigcup_{\ell\ \text{leaf}} S_\ell\\ S_\ell &amp;amp;\subseteq A_\ell\\ \lvert S_\ell\rvert &amp;amp;= n_\ell \end{aligned}\] The following lemma formalizes the “consistency implies exact internal counts” statement. Lemma (internal node counts are forced by leaf counts). Suppose that for every leaf \(\ell\) we have chosen a subset \(S_\ell\subseteq A_\ell\) with \(\lvert S_\ell\rvert=n_\ell\), and define \(S=\bigcup_\ell S_\ell\). Suppose also that the integers \(n_u\) satisfy the additivity constraints \(n_u=n_{u_L}+n_{u_R}\) for all internal nodes and \(n_{\mathrm{root}}=n\). Then for every node \(u\), \[\lvert S\cap A_u\rvert = n_u\] Proof. Fix a node \(u\). The set \(A_u\) is a disjoint union of the leaf cells under it: \[A_u = \bigsqcup_{\ell\in\mathrm{Leaves}(u)} A_\ell\] Since the \(A_\ell\) are disjoint and \(S_\ell\subseteq A_\ell\), we have \[S\cap A_u = \left(\bigcup_{\ell} S_\ell\right)\cap A_u = \bigcup_{\ell\in\mathrm{Leaves}(u)} S_\ell\] and this union is disjoint. Therefore \[\begin{aligned} \lvert S\cap A_u\rvert &amp;amp;= \sum_{\ell\in\mathrm{Leaves}(u)} \lvert S_\ell\rvert\\ &amp;amp;= \sum_{\ell\in\mathrm{Leaves}(u)} n_\ell \end{aligned}\] On the other hand, the additivity constraints imply that the node count \(n_u\) equals the sum of the leaf counts under \(u\) (this is proved by a simple induction down the tree: each internal node’s count is the sum of its children’s counts, so unrolling yields the sum of all descendant leaves). Hence \[n_u = \sum_{\ell\in\mathrm{Leaves}(u)} n_\ell = \lvert S\cap A_u\rvert\] This holds for every node \(u\). \(\square\) So all discrepancy guarantees can be proved purely at the level of the integer allocation rule for the counts \(n_u\). Version A: Deterministic Rounding At each internal node \(u\) we want the left child to receive approximately the population fraction \[\alpha_u = \frac{N_{u_L}}{N_u}\in[0,1]\] If the parent has sample count \(n_u\), the “ideal” non-integer count for the left child is \(n_u\alpha_u\). Deterministic rounding sets \[\begin{aligned} n_{u_L} &amp;amp;= \operatorname{Round}(n_u\alpha_u)\\ n_{u_R} &amp;amp;= n_u - n_{u_L} \end{aligned}\] Here \(\operatorname{Round}(\cdot)\) means nearest integer, where ties are broken arbitrarily. By construction, \(n_{u_L}+n_{u_R}=n_u\) exactly at every split, so additivity holds globally. At leaves we sample: \[S_\ell \sim \text{UniformWithoutReplacement}(A_\ell, n_\ell)\] (Notice here that the choice within a leaf can be randomized or deterministic. It does not affect the tree-mass discrepancy, only which specific points we keep.) A pseudocode description is seen below: Algorithm: DeterministicTreeBalance(I, T, n) Input: index set I of size N, partition tree T with node sets A_u, target sample size n Output: subset S ⊆ I with |S| = n 1. For every node u in T compute N_u = |A_u| 2. Set n_root = n 3. For each internal node u in top-down order: alpha_u = N_{u_L} / N_u n_{u_L} = Round(n_u * alpha_u) n_{u_R} = n_u - n_{u_L} 4. For each leaf ell: Choose S_ell ⊆ A_ell uniformly without replacement with |S_ell| = n_ell 5. Return S = ⋃_{leaves ell} S_ell Now we prove the discrepancy bound carefully. Theorem 1 (deterministic multiscale balance). Let \(L\) be the depth of the tree \(\mathcal T\) (the maximum root-to-node distance). The subset \(S\) that is produced by the deterministic algorithm satisfies, for every node \(u\), \[\left\lvert\frac{\lvert S\cap A_u\rvert}{n} - \frac{\lvert A_u\rvert}{N}\right\rvert \le \frac{\mathrm{depth}(u)}{2n}\] In particular, \[d_{\mathcal T}(S)\le \frac{L}{2n}\] Proof. By the previous lemma, for every node \(u\) we have \(\lvert S\cap A_u\rvert=n_u\), so it suffices to bound \[\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert\] Define the “ideal” real valued target count \[\mu_u = n\cdot\frac{N_u}{N}\] and define the allocation error \[e_u = n_u - \mu_u\] Then \[\begin{aligned} \left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert &amp;amp;= \left\lvert\frac{n_u}{n}-\frac{\mu_u}{n}\right\rvert\\ &amp;amp;= \frac{\lvert n_u-\mu_u\rvert}{n}\\ &amp;amp;= \frac{\lvert e_u\rvert}{n} \end{aligned}\] So it is enough to prove \[\lvert e_u\rvert \le \frac{\mathrm{depth}(u)}{2} \quad\text{for all nodes }u\] We prove this by induction on depth. Base case. For the root, \[\begin{aligned} n_{\mathrm{root}} &amp;amp;= n\\ \mu_{\mathrm{root}} &amp;amp;= n\cdot\frac{N_{\mathrm{root}}}{N} = n\cdot\frac{N}{N}=n \end{aligned}\] so \[e_{\mathrm{root}} = n_{\mathrm{root}}-\mu_{\mathrm{root}} = n-n = 0\] Since \(\mathrm{depth}(\mathrm{root})=0\), the base case holds. Inductive step. Fix an internal node \(u\) and assume \[\lvert e_u\rvert\le \frac{\mathrm{depth}(u)}{2}\] Let its children be \(u_L,u_R\) and define \(\alpha_u = N_{u_L}/N_u\). Deterministic rounding gives \[n_{u_L} = \operatorname{Round}(n_u\alpha_u)\] Therefore there exists a rounding error term \(\delta_u\) such that \[n_{u_L} = n_u\alpha_u + \delta_u\] where \(\lvert\delta_u\rvert\le \frac12\). Also, \[\begin{aligned} \mu_{u_L} &amp;amp;= n\cdot\frac{N_{u_L}}{N}\\ &amp;amp;= n\cdot\frac{N_u}{N}\cdot\frac{N_{u_L}}{N_u}\\ &amp;amp;= \mu_u\alpha_u \end{aligned}\] So the child error is \[\begin{aligned} e_{u_L} &amp;amp;= n_{u_L}-\mu_{u_L}\\ &amp;amp;= (n_u\alpha_u+\delta_u) - (\mu_u\alpha_u)\\ &amp;amp;= \alpha_u(n_u-\mu_u) + \delta_u\\ &amp;amp;= \alpha_u e_u + \delta_u \end{aligned}\] Next, taking absolute values and using \(0\le \alpha_u\le 1\), we get: \[\begin{aligned} \lvert e_{u_L}\rvert &amp;amp;\le \lvert\alpha_u\rvert\,\lvert e_u\rvert + \lvert\delta_u\rvert\\ &amp;amp;\le \lvert e_u\rvert + \frac12\\ &amp;amp;\le \frac{\mathrm{depth}(u)}{2} + \frac12\\ &amp;amp;= \frac{\mathrm{depth}(u)+1}{2}\\ &amp;amp;= \frac{\mathrm{depth}(u_L)}{2} \end{aligned}\] For the right child, use \(n_{u_R}=n_u-n_{u_L}\) and \(\mu_{u_R}=\mu_u-\mu_{u_L}=\mu_u(1-\alpha_u)\). Since \[n_{u_R} = n_u - (n_u\alpha_u+\delta_u) = n_u(1-\alpha_u)-\delta_u,\] we get \[\begin{aligned} e_{u_R} &amp;amp;= n_{u_R}-\mu_{u_R}\\ &amp;amp;= \bigl(n_u(1-\alpha_u)-\delta_u\bigr) - \mu_u(1-\alpha_u)\\ &amp;amp;= (1-\alpha_u)(n_u-\mu_u) - \delta_u\\ &amp;amp;= (1-\alpha_u)e_u - \delta_u \end{aligned}\] and so, \[\begin{aligned} \lvert e_{u_R}\rvert &amp;amp;\le \lvert1-\alpha_u\rvert\,\lvert e_u\rvert + \lvert\delta_u\rvert\\ &amp;amp;\le \lvert e_u\rvert + \frac12\\ &amp;amp;\le \frac{\mathrm{depth}(u)+1}{2}\\ &amp;amp;= \frac{\mathrm{depth}(u_R)}{2} \end{aligned}\] Thus the bound holds for both children, completing the induction. Therefore, for every node \(u\), \[\lvert e_u\rvert\le \frac{\mathrm{depth}(u)}{2},\] and substituting into \(\left\lvert n_u/n - N_u/N\right\rvert = \lvert e_u\rvert/n\) produces the theorem. \(\square\) This is a worst-case bound with no probability and no consideration for the “typical case.” Its conceptual limitation is also clear in that the bound grows linearly with depth. That motivates a randomized variant that removes systematic drift and produces a concentration like \(\sqrt{\mathrm{depth}}/n\) for any fixed node. Version B: Randomized Rounding The deterministic algorithm rounds to the nearest integer, which is stable but can accumulate small biases along a path in the tree. Randomized rounding replaces that with an unbiased decision at each of the splits. Fix an internal node \(u\). Let \(t_u = n_u\alpha_u\) and say \[t_u = \lfloor t_u\rfloor + \theta_u, \quad \theta_u \in [0,1)\] Define \[n_{u_L} = \begin{cases} \lfloor t_u\rfloor + 1 &amp;amp; \text{with probability }\theta_u\\ \lfloor t_u\rfloor &amp;amp; \text{with probability }1-\theta_u \end{cases}\] and \(n_{u_R} = n_u - n_{u_L}\). Everything else is the same. Algorithm: RandomizedTreeBalance(I, T, n) Input: index set I of size N, partition tree T with node sets A_u, target sample size n Output: subset S ⊆ I with |S| = n 1. Compute N_u = |A_u| for all nodes u 2. Set n_root = n 3. For each internal node u in top-down order: alpha_u = N_{u_L} / N_u t = n_u * alpha_u theta = t - floor(t) With prob theta set n_{u_L} = floor(t) + 1; otherwise n_{u_L} = floor(t) Set n_{u_R} = n_u - n_{u_L} 4. For each leaf ell: Choose S_ell ⊆ A_ell uniformly without replacement with |S_ell| = n_ell 5. Return S = ⋃_{leaves ell} S_ell We now prove the two key properties (1) unbiasedness and (2) concentration. Lemma 2 (unbiased node counts). For every node \(u\in\mathcal T\), the random count \(n_u = \lvert S\cap A_u\rvert\) satisfies \[\mathbb E[n_u] = n\cdot \frac{N_u}{N}\] Proof. As above, \(\lvert S\cap A_u\rvert=n_u\) deterministically once the allocation is fixed, so it suffices to prove the statement for the allocation random variables \(n_u\). We prove by induction on depth. For the root, \[\mathbb E[n_{\mathrm{root}}] = n = n\cdot\frac{N}{N} = n\cdot\frac{N_{\mathrm{root}}}{N}\] Assume the claim holds for a node \(u\). Conditional on \(n_u\), define \(t_u=n_u\alpha_u\). By construction, \[n_{u_L} = \begin{cases} \lfloor t_u\rfloor + 1 &amp;amp; \text{with probability }\theta_u\\ \lfloor t_u\rfloor &amp;amp; \text{with probability }1-\theta_u \end{cases}\] and \(\theta_u = t_u-\lfloor t_u\rfloor\). Therefore \[\begin{aligned} \mathbb E[n_{u_L}\mid n_u] &amp;amp;= (\lfloor t_u\rfloor + 1)\theta_u + (\lfloor t_u\rfloor)(1-\theta_u)\\ &amp;amp;= \lfloor t_u\rfloor + \theta_u\\ &amp;amp;= t_u\\ &amp;amp;= n_u\alpha_u \end{aligned}\] Taking expectations, \[\begin{aligned} \mathbb E[n_{u_L}] &amp;amp;= \mathbb E\big[\mathbb E[n_{u_L}\mid n_u]\big]\\ &amp;amp;= \mathbb E[n_u\alpha_u]\\ &amp;amp;= \alpha_u\,\mathbb E[n_u]\\ &amp;amp;= \frac{N_{u_L}}{N_u}\cdot n\frac{N_u}{N}\\ &amp;amp;= n\frac{N_{u_L}}{N} \end{aligned}\] For the right child, \(n_{u_R}=n_u-n_{u_L}\), so \[\mathbb E[n_{u_R}] = \mathbb E[n_u]-\mathbb E[n_{u_L}] = n\frac{N_u}{N} - n\frac{N_{u_L}}{N} = n\frac{N_{u_R}}{N}\] Thus the claim holds for both children, completing the induction. \(\square\) So the randomized construction is exactly correct in expectation*for every cell in the multiscale partition. Next we quantify deviations for a fixed node (the cleanest statement), and afterwards one can apply a union bound if one wants a statement holding simultaneously for all nodes. Theorem 3 (concentration for a fixed node). Let us fix a node \(u\) at depth \(d\). Then for all \(t&amp;gt;0\), \[\Pr\left(\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert\ge \frac{t}{n}\right) \le 2\exp\left(-\frac{t^2}{2d}\right)\] Equivalently, we can say for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), \[\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert \le \frac{\sqrt{2d\log(2/\delta)}}{n}\] Proof. Let \(v_0,v_1,\dots,v_d\) be the unique path from the root \(v_0\) to the node \(v_d=u\). For each step \(k=1,\dots,d\), let \(\beta_k\in[0,1]\) denote the population fraction of the chosen child along the path (i.e., if \(v_k\) is the left child of \(v_{k-1}\), then \(\beta_k = N_{v_k}/N_{v_{k-1}}\); if it is the right child, \(\beta_k = N_{v_k}/N_{v_{k-1}}\) as well). With this definition we always have \(\frac{N_{v_k}}{N_{v_{k-1}}} = \beta_k\) and thus, \(\frac{N_{v_k}}{N} = \left(\prod_{j=1}^k \beta_j\right)\). Next, define the ideal real valued target counts \[\mu_k = n\cdot\frac{N_{v_k}}{N}\] and define the errors \[e_k = n_{v_k}-\mu_k\] We will express \(e_d\) as a sum of bounded martingale differences. At step \(k\), conditional on the past (i.e. on \(n_{v_{k-1}}\)), the algorithm sets \[n_{v_k} = n_{v_{k-1}}\beta_k + \rho_k\] where the rounding noise \[\rho_k = n_{v_k} - n_{v_{k-1}}\beta_k\] satisfies \(\mathbb E[\rho_k\mid \mathcal F_{k-1}] = 0\) and \(\lvert\rho_k\rvert\le 1\). The expectation is zero because randomized rounding is unbiased. It rounds \(t=n_{v_{k-1}}\beta_k\) up with probability equal to its fractional part. The magnitude bound holds because \(\rho_k\in\{-\theta,1-\theta\}\) for some \(\theta\in[0,1)\). Also, by definition of \(\mu_k\) and the fact that \(N_{v_k}/N_{v_{k-1}}=\beta_k\), \[\begin{aligned} \mu_k &amp;amp;= n\cdot\frac{N_{v_k}}{N}\\ &amp;amp;= n\cdot\frac{N_{v_{k-1}}}{N}\cdot\frac{N_{v_k}}{N_{v_{k-1}}}\\ &amp;amp;= \mu_{k-1}\beta_k \end{aligned}\] Subtracting \(\mu_k\) from \(n_{v_k}\) produces \[\begin{aligned} e_k &amp;amp;= n_{v_k}-\mu_k\\ &amp;amp;= \bigl(n_{v_{k-1}}\beta_k+\rho_k\bigr) - \mu_{k-1}\beta_k\\ &amp;amp;= \beta_k(n_{v_{k-1}}-\mu_{k-1}) + \rho_k\\ &amp;amp;= \beta_k e_{k-1} + \rho_k \end{aligned}\] Now unroll the recursion. Since \(e_0 = n-\mu_0 = n-n = 0\), repeated substitution gives \[\begin{aligned} e_d &amp;amp;= \beta_d e_{d-1} + \rho_d\\ &amp;amp;= \beta_d(\beta_{d-1} e_{d-2}+\rho_{d-1}) + \rho_d\\ &amp;amp;= \beta_d\beta_{d-1}e_{d-2} + \beta_d\rho_{d-1} + \rho_d\\ &amp;amp;\ \ \vdots\\ &amp;amp;= \sum_{k=1}^d \rho_k \prod_{j=k+1}^d \beta_j \end{aligned}\] Define \[X_k = \rho_k \prod_{j=k+1}^d \beta_j\] Then \(e_d = \sum_{k=1}^d X_k\). Moreover, \(\prod_{j=k+1}^d \beta_j\) is measurable with respect to \(\mathcal F_{k-1}\) (it depends only on the fixed tree and the path, not on future randomness), so \[\mathbb E[X_k\mid \mathcal F_{k-1}] = \left(\prod_{j=k+1}^d \beta_j\right)\mathbb E[\rho_k\mid\mathcal F_{k-1}] = 0\] Thus \(X_k\) are martingale differences. Finally, since each \(\beta_j\in[0,1]\) and \(\lvert\rho_k\rvert\le 1\), \[\lvert X_k\rvert \le \lvert\rho_k\rvert\cdot \prod_{j=k+1}^d \lvert\beta_j\rvert \le 1\] Azuma–Hoeffding for martingales with bounded increments now gives, for all \(t&amp;gt;0\), \[\Pr\left(\lvert e_d\rvert\ge t\right) \le 2\exp\left(-\frac{t^2}{2\sum_{k=1}^d 1^2}\right) = 2\exp\left(-\frac{t^2}{2d}\right)\] Finally, observe \[\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert = \left\lvert\frac{n_{v_d}}{n}-\frac{\mu_d}{n}\right\rvert = \frac{\lvert e_d\rvert}{n}\] So \[\Pr\left(\left\lvert\frac{n_u}{n}-\frac{N_u}{N}\right\rvert\ge \frac{t}{n}\right) = \Pr\left(\lvert e_d\rvert\ge t\right) \le 2\exp\left(-\frac{t^2}{2d}\right)\] which is the first claim. The second follows by setting \(t=\sqrt{2d\log(2/\delta)}\) and rearranging. \(\square\) This should be the Aha moment! Deterministic rounding produces a worst-case guarantee of order \(\mathrm{depth}(u)/n\), while randomized rounding produces deviations of order \(\sqrt{\mathrm{depth}(u)}/n\) for a fixed node, with the tail bounds. An Interlude: Why Optimal Transport Shows Up Inevitably At this point we have a completely explicit construction where we choose a multiscale partition tree \(\mathcal T\), then build a sample whose mass in every tree cell matches the population mass up to a provable error. In other words, we chose a family of “questions” and forced the sample to answer them almost the same way as the full dataset. There is, however, a downside to any partition-based notion of similarity. It is coordinate-dependent and bin-dependent. If I shift a continuous feature by a tiny amount, the “shape” of the distribution hasn’t changed much, but a histogram can change a lot if points cross a bin boundary. The tree discrepancy \(d_{\mathcal T}\) is not “wrong,” but it is answers only a particular kind of question.”How much mass lies in each region?” But there is another question you might ask that is more geometric in nature. If I imagine the full dataset distribution as a pile of sand and the sampled dataset distribution as another pile of sand, how expensive is it to reshape one pile into the other, if moving sand by distance \(r\) costs \(r\)? That question is optimal transport. The conceptual leap is that, instead of choosing a function class \(\mathcal F\) or a partition \(\mathcal T\) up front, we choose a ground metric on feature space (a notion of distance between rows), and we measure distributional mismatch by the minimal cost of moving probability mass through this geometric landscape. I think this viewpoint is really teachable because it has (i) a clean variational definition, (ii) an exact closed form in one dimension that can be proved, and (iii) a direct algorithmic “coreset” formulation that matches our original downsampling goal. Optimal Transport and Wasserstein Distance Let \(\mu\) and \(\nu\) be probability measures on a metric space \((\mathcal X, d)\). A coupling of \(\mu\) and \(\nu\) is a joint distribution \(\pi\) on \(\mathcal X\times\mathcal X\) whose first marginal is \(\mu\) and second marginal is \(\nu\). The set of all couplings is denoted \(\Pi(\mu,\nu)\). For \(p\ge 1\), the Wasserstein-\(p\) distance is \[W_p(\mu,\nu) = \left( \inf_{\pi\in\Pi(\mu,\nu)} \int_{\mathcal X\times\mathcal X} d(x,x&apos;)^p \, d\pi(x,x&apos;) \right)^{1/p}\] For the purposes of this post, the case \(p=1\) is already very informative: \[W_1(\mu,\nu) = \inf_{\pi\in\Pi(\mu,\nu)} \int_{\mathcal X\times\mathcal X} d(x,x&apos;) \, d\pi(x,x&apos;)\] When \(\mu\) and \(\nu\) are empirical measures, this becomes a finite-dimensional linear program. Specifically, suppose \[\mu = \sum_{i=1}^N a_i\,\delta_{x_i} \quad \nu = \sum_{j=1}^m b_j\,\delta_{z_j}\] where \(a_i\ge 0\), \(b_j\ge 0\), \(\sum_i a_i=1\), \(\sum_j b_j=1\). A coupling is then a nonnegative matrix \(\Pi\in\mathbb R_{\ge 0}^{N\times m}\) whose row/column sums match \(a\) and \(b\): \[\sum_{j=1}^m \Pi_{ij} = a_i \quad \forall i, \quad \sum_{i=1}^N \Pi_{ij} = b_j \quad \forall j\] The transport cost is \[\sum_{i=1}^N\sum_{j=1}^m \Pi_{ij}\, d(x_i,z_j)\] so \[W_1(\mu,\nu) = \min_{\Pi\ge 0} \left\{ \sum_{i=1}^N\sum_{j=1}^m \Pi_{ij}\, d(x_i,z_j) \ :\ \Pi\mathbf 1=a,\ \Pi^\top \mathbf 1=b \right\}\] This is the first important connection to downsampling. OT is naturally a problem of replacing a big empirical measure by a smaller one while keeping them close in the geometric sense. OT as a Downsampling Objective and the Wasserstein Coreset Problem Return to our dataset distribution \[P_I = \frac1N\sum_{i\in I}\delta_{x_i}\] Suppose we want a summary measure supported on only \(n\) points with equal weights: \[\nu = \frac1n\sum_{k=1}^n \delta_{z_k}\] If we allow the support points \(z_k\) to be anywhere in \(\mathcal X\), one natural best-summary problem is: \[\min_{z_1,\dots,z_n\in\mathcal X} W_1\left(P_I,\ \frac1n\sum_{k=1}^n\delta_{z_k}\right)\] If we require that the summary to be a subset of the original points (which is what downsampling refers to), we add the constraint: \(z_k \in \{x_i : i\in I\}\). This becomes a Wasserstein-type of facility location problem: Choose a set of “representative points” so that transporting mass from each original point to representatives is low-cost. And if we also want to enforce label proportions, we do exactly what we did before. Solve this problem separately within each label class, which would produce \(\nu_y\), which would be supported on \(n_y\) points for each class, then union the supports. At this stage, the definition is clear but it might still feel abstract. I’m going to boil this down to one dimension so that my point becomes clear. The One-dimensional Case Where OT Becomes Quantiles Assume \(\mathcal X=\mathbb R\) with distance \(d(x,x&apos;)=\lvert x-x&apos;\rvert\), and let \(\mu\) be any probability measure on \(\mathbb R\) with CDF \(F\). Let \(\nu\) be any probability measure on \(\mathbb R\) with CDF \(G\). A foundational fact is that in one dimension, the optimal coupling is the monotone one, and the Wasserstein-1 distance has a closed form: Theorem 4 (quantile representation of \(W_1\) on \(\mathbb R\)). Let \(F^{-1}\) and \(G^{-1}\) denote the generalized inverse CDFs. Then \[W_1(\mu,\nu) = \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\] I’ll prove this because it provides us with the link between the geometric quality of transport and the notion of “sampling via quantiles,” and it is also the statement we will use to design an optimal \(n\)-point approximation. Proof of Theorem 4 Define the quantile functions \[F^{-1}(t) = \inf\{x\in\mathbb R : F(x)\ge t\}\] \[G^{-1}(t) = \inf\{x\in\mathbb R : G(x)\ge t\}\] Let \(U\sim \mathrm{Unif}[0,1]\) and define random variables \(X = F^{-1}(U)\) and \(Y = G^{-1}(U)\). Step 1: \(X\sim\mu\) and \(Y\sim\nu\). We show \(\mathbb P(X\le x)=F(x)\). Indeed, \[\begin{aligned} \mathbb P(X\le x) &amp;amp;= \mathbb P\bigl(F^{-1}(U)\le x\bigr)\\ &amp;amp;= \mathbb P\left(U \le F(x)\right) \end{aligned}\] where the last step is a standard property of generalized inverses: \(F^{-1}(u)\le x\) iff \(u\le F(x)\). Since \(U\) is uniform, \[\mathbb P(U\le F(x)) = F(x)\] Thus \(X\) has CDF \(F\), so \(X\sim\mu\). The same argument gives \(Y\sim\nu\). Therefore, the joint law of \((X,Y)\) defines a coupling \(\pi\) of \(\mu\) and \(\nu\). Step 2: the induced transport cost equals the integral. By definition of \((X,Y)\), \[\begin{aligned} \mathbb E\left[\lvert X-Y\rvert\right] &amp;amp;= \mathbb E\left[\left\lvert F^{-1}(U)-G^{-1}(U)\right\rvert\right]\\ &amp;amp;= \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt \end{aligned}\] So we have produced a coupling with cost equal to the RHS, and therefore \[W_1(\mu,\nu)\le \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\] Step 3: optimality of monotone coupling. Let \(\pi\) be any coupling of \(\mu\) and \(\nu\), and let \((X,Y)\sim\pi\). Consider the function class of 1-Lipschitz functions \(\varphi:\mathbb R\to\mathbb R\), i.e. \[\lvert\varphi(x)-\varphi(x&apos;)\rvert\le \lvert x-x&apos;\rvert\] For any such \(\varphi\), \[\begin{aligned} \mathbb E[\varphi(X)]-\mathbb E[\varphi(Y)] &amp;amp;= \mathbb E[\varphi(X)-\varphi(Y)]\\ &amp;amp;\le \mathbb E\left[\lvert\varphi(X)-\varphi(Y)\rvert\right]\\ &amp;amp;\le \mathbb E\left[\lvert X-Y\rvert\right] \end{aligned}\] Taking the supremum over all 1-Lipschitz \(\varphi\) produces \[\sup_{\mathrm{Lip}(\varphi)\le 1} \left(\mathbb E_\mu[\varphi]-\mathbb E_\nu[\varphi]\right) \le \mathbb E_\pi\left[\lvert X-Y\rvert\right]\] Since this holds for every coupling \(\pi\), \[\sup_{\mathrm{Lip}(\varphi)\le 1} \left(\mathbb E_\mu[\varphi]-\mathbb E_\nu[\varphi]\right) \le W_1(\mu,\nu)\] Now, in one dimension, one can verify that \[\sup_{\mathrm{Lip}(\varphi)\le 1} \left(\mathbb E_\mu[\varphi]-\mathbb E_\nu[\varphi]\right) = \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\] Combining with the upper bound from Step 2 gives equality, proving the theorem. \(\square\) The quantile coreset is optimal for 1D \(W_1\) Now we use Theorem 4 to solve the downsampling problem in 1D, fully. First, fix \(n\ge 1\). Consider the class of measures supported on exactly \(n\) points with equal weights: \[\nu = \frac1n\sum_{k=1}^n \delta_{z_k}\] Such a \(\nu\) has a quantile function that is a step function: if we order the support \(z_{(1)}\le \cdots\le z_{(n)}\), then \[G^{-1}(t) = z_{(k)}\qquad\text{for }t\in\left(\frac{k-1}{n},\frac{k}{n}\right]\] Plugging this into Theorem 4 provides us with \[\begin{aligned} W_1(\mu,\nu) &amp;amp;= \int_0^1 \left\lvert F^{-1}(t)-G^{-1}(t)\right\rvert\,dt\\ &amp;amp;= \sum_{k=1}^n \int_{(k-1)/n}^{k/n} \left\lvert F^{-1}(t)-z_{(k)}\right\rvert\,dt \end{aligned}\] Crucially, the RHS separates across intervals, so the minimization over \(z_{(k)}\) decouples. Lemma 5 (best constant on an interval is a median). Let \(g:[a,b]\to\mathbb R\) be integrable. Define \[\Phi(z)=\int_a^b \lvert g(t)-z\rvert\,dt\] Then any minimizer of \(\Phi(z)\) is a median of the pushforward distribution of \(g(t)\) under uniform measure on \([a,b]\). In particular, if \(g\) is monotone, a minimizer is \(g((a+b)/2)\). Proof. Fix \(z\) and consider the derivative of \(\Phi\) where it exists. For any \(z\) that is not equal to a value attained by \(g(t)\) on a set of positive measure, we can differentiate under the integral sign: \[\begin{aligned} \Phi(z) &amp;amp;= \int_a^b \lvert g(t)-z\rvert\,dt\\ &amp;amp;= \int_{g(t)\ge z} (g(t)-z)\,dt + \int_{g(t)&amp;lt;z} (z-g(t))\,dt \end{aligned}\] Differentiating with respect to \(z\) gives \[\begin{aligned} \Phi&apos;(z) &amp;amp;= \int_{g(t)\ge z} (-1)\,dt + \int_{g(t)&amp;lt;z} (1)\,dt\\ &amp;amp;= \lambda(\{t: g(t)&amp;lt;z\}) - \lambda(\{t: g(t)\ge z\}) \end{aligned}\] where \(\lambda\) denotes the Lebesgue measure on \([a,b]\). A minimizer must satisfy \(0\in \partial \Phi(z)\), equivalently that the measure of the set where \(g(t)\le z\) is at least half and the measure where \(g(t)\ge z\) is at least half. This is exactly the median condition. If \(g\) is monotone, then the median is attained at the midpoint \(t=(a+b)/2\). \(\square\) Apply this lemma with \(g(t)=F^{-1}(t)\) on each interval \(((k-1)/n,k/n]\). Since \(F^{-1}\) is monotone nondecreasing, the optimal choice on that interval is the midpoint quantile: \[z_{(k)}^\star = F^{-1}\left(\frac{(k-1)/n + k/n}{2}\right) = F^{-1}\left(\frac{2k-1}{2n}\right)\] We have proved: Theorem 6 (optimal 1D OT coreset via quantiles). Let \(\mu\) be a probability measure on \(\mathbb R\) with the CDF \(F\). Across all of the measures of the form \(\nu=\frac1n\sum_{k=1}^n\delta_{z_k}\), the minimizer of \(W_1(\mu,\nu)\) is achieved by taking \[z_k^\star = F^{-1}\left(\frac{2k-1}{2n}\right),\quad k=1,\dots,n\] Proof. Combine the quantile representation in Theorem 4, the interval decomposition above, and Lemma 5 applied on each interval. Each term is minimized by the midpoint quantile, and so the sum is minimized. \(\square\) This result is both rigorous and pedagogically pretty nice. In one dimension, the best downsampling in Wasserstein-1 is essentially equivalent to keeping evenly spaced quantiles. And it explains, why OT feels like the right notion of distribution preservation for continuous features. Shifting all points by a small amount changes the Wasserstein distance by that small amount. From the Theorem to an Algorithm (in the 1D case) If you have data points \(x_1,\dots,x_N\in\mathbb R\) and you want an \(n\)-point OT coreset with equal weights, you can implement Theorem 6 directly using order statistics. Let \(x_{(1)}\le \cdots\le x_{(N)}\) be the sorted data. The empirical CDF has quantile function \[F_N^{-1}(t)=x_{(\lceil Nt\rceil)}\] So the OT-optimal equal weight summary points are \[z_k = x_{\left(\left\lceil N\cdot \frac{2k-1}{2n}\right\rceil\right)},\quad k=1,\dots,n\] A simple implementation is: Algorithm: 1D-OT-Quantile-Coreset(x[1..N], n) 1. Sort x to obtain x_(1) &amp;lt;= ... &amp;lt;= x_(N) 2. For k = 1..n: idx = ceil( N * (2k - 1) / (2n) ) z_k = x_(idx) 3. Return {z_1, ..., z_n} This gives an exact minimizer of \(W_1(P_I,\nu)\) among all equal weight \(n\)-point measures supported on the real line, where \(P_I\) is the empirical distribution of the data. What about Higher Dimensions? In \(d&amp;gt;1\), the same objective \[\min_{\nu:\ \lvert\mathrm{supp}(\nu)\rvert=n} W_1(P_I,\nu)\] is still meaningful, but no longer has a simple quantile formula. Nonetheless, the OT viewpoint remains valuable for three reasons: It gives a coordinate free notion of distributional similarity It directly rewards summaries that preserve the geometry and clustering structure in feature space It connects to a large algorithmic toolbox of methods from linear programming, entropic regularization, and OT-inspired clustering/medoid selection A common practical restriction is to insist that \(\nu\) is supported on a subset of the observed points (true downsampling), and to take equal weights. Then we are choosing representative points that minimize the transport cost from the full empirical distribution. This is related in spirit to facility location and \(k\)-medoids, but with the framework of optimal transport For the purposes of this post, the key conceptual takeaway is that multiscale tree construction preserved distribution by matching masses on a chosen hierarchy of regions. OT preserves distribution by minimizing the geometric cost of moving mass. They are two different “languages” for the same core objective which is to replace a huge dataset by a smaller one without changing what a learner would perceive as the underlying world. Finally, to fold OT back into our original supervised setting, we can apply OT label conditionally. For each \(y\in\mathcal Y\), compute an OT based summary of the empirical conditional distribution \[P_{I_y}=\frac1{N_y}\sum_{i\in I_y}\delta_{x_i}\] using \(n_y\) points, then union the supports across \(y\). This enforces the label proportions exactly while using OT to preserve the features of each class-conditional feature distribution. In the next section, I’ll connect this OT idea back to the multiscale tree idea a bit more concretely. One can view Wasserstein distance as controlling discrepancies against 1-Lipschitz test functions, while tree discrepancy controls discrepancies against indicator functions of tree cells. That comparison is a clear way to understand what each method preserves and why they complement each other. A Brief Connection between Two Distances and Two Question Sets The multiscale tree sampler and optimal transport are best thought of as answering the same meta question with different choices of “what counts as evidence that two distributions differ.” Fix a label \(y\) and suppress it in notation. We are comparing the empirical measures \[P_I=\frac1N\sum_{i\in I}\delta_{x_i}, \quad P_S=\frac1n\sum_{i\in S}\delta_{x_i}\] The tree discrepancy tied to a partition tree \(\mathcal T\) is \[d_{\mathcal T}(S) =\max_{u\in\mathcal T}\left\lvert\frac{\lvert S\cap A_u\rvert}{n}-\frac{\lvert A_u\rvert}{N}\right\rvert\] But each node set \(A_u\) is really just an indicator query. Define \[f_u(x)=\mathbf 1\{x\in\mathcal C_u\}\] where \(\mathcal C_u\subseteq\mathcal X\) is the corresponding region of feature space. Then \[\begin{aligned} \mathbb E_{P_S}[f_u(X)] &amp;amp;=\frac1n\sum_{i\in S}\mathbf 1\{x_i\in\mathcal C_u\} =\frac{\lvert S\cap A_u\rvert}{n}\\ \mathbb E_{P_I}[f_u(X)] &amp;amp;=\frac1N\sum_{i\in I}\mathbf 1\{x_i\in\mathcal C_u\} =\frac{\lvert A_u\rvert}{N} \end{aligned}\] So, exactly, \[d_{\mathcal T}(S) = \max_{u\in\mathcal T}\left\lvert\mathbb E_{P_S}[f_u(X)]-\mathbb E_{P_I}[f_u(X)]\right\rvert\] In words, this means that tree balancing controls discrepancies against a large, multiscale family of indicator functions (cell membership queries). Optimal transport controls a different family. For \(W_1\) on a metric space \((\mathcal X,d)\), the Kantorovich–Rubinstein duality says \[W_1(P_I,P_S) = \sup_{\mathrm{Lip}(f)\le 1}\left\lvert\mathbb E_{P_I}[f(X)]-\mathbb E_{P_S}[f(X)]\right\rvert\] so, OT controls discrepancies against all 1-Lipschitz functions. One small proposition makes the guarantee for our tree-based method clear. Proposition (tree discrepancy controls piecewise-constant observables). Let \(\mathcal T\) be a partition tree with leaf set \(\mathcal L\). Suppose a function \(g:\mathcal X\to\mathbb R\) is constant on leaves, i.e. there exist constants \(c_\ell\) such that \(g(x)=c_\ell\) whenever \(x\in\mathcal C_\ell\). Then \[\left\lvert\mathbb E_{P_S}[g(X)]-\mathbb E_{P_I}[g(X)]\right\rvert \le \left(\max_{\ell\in\mathcal L}\left\lvert\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right\rvert\right) \sum_{\ell\in\mathcal L}\lvert c_\ell\rvert\] Proof. Since \(g\) is leaf-piecewise-constant, \[g(x)=\sum_{\ell\in\mathcal L} c_\ell\,\mathbf 1\{x\in\mathcal C_\ell\}\] Therefore \[\begin{aligned} \mathbb E_{P_S}[g(X)]-\mathbb E_{P_I}[g(X)] &amp;amp;=\sum_{\ell\in\mathcal L} c_\ell\left(\mathbb E_{P_S}[\mathbf 1\{X\in\mathcal C_\ell\}] - \mathbb E_{P_I}[\mathbf 1\{X\in\mathcal C_\ell\}]\right)\\ &amp;amp;=\sum_{\ell\in\mathcal L} c_\ell\left(\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right) \end{aligned}\] Taking absolute values and applying the triangle inequality, \[\begin{aligned} \left\lvert\mathbb E_{P_S}[g(X)]-\mathbb E_{P_I}[g(X)]\right\rvert &amp;amp;\le \sum_{\ell\in\mathcal L} \lvert c_\ell\rvert\left\lvert\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right\rvert\\ &amp;amp;\le \left(\max_{\ell\in\mathcal L}\left\lvert\frac{\lvert S\cap A_\ell\rvert}{n}-\frac{\lvert A_\ell\rvert}{N}\right\rvert\right)\sum_{\ell\in\mathcal L}\lvert c_\ell\rvert \end{aligned}\] \[\square\] This is the simplest statement of what the guarantee buys you. If your model or a piece of it behaves like a piecewise-constant function on your chosen multiscale partition, then the tree balancing forces its empirical averages to be stable under downsampling. OT, on the other hand, controls all averages of 1-Lipschitz functions, and so, it is naturally robust to small geometric perturbations. Conclusion One of the things I’ve come to appreciate the most about teaching is how often genuinely difficult problems can pop up disguised as mundane ones. Here, it was simply the case that a laptop couldn’t handle a dataset, which I suppose, is a pretty common problem (as complexity theory has been around for a while). But, when the problem was first posed to me in office hours, I truly did think a straightforward solution could exist somewhere in the documentation of Sci-kit Learn. This problem– and these kinds of problems in general– sound like logistical issues until you try to make them precise. And then, you realize you’ve been handed a real mathematical question that can only really be tackled using makeshift techniques and approximation tools. That shift in perspective is, to me, the main reward and most fascinating part of problem solving. It’s a reminder that there are technical problems that can show up all over the place yet remain relatively hidden. They show up wherever someone is trying to do something simple and quickly discover that the simplicity actually covers hidden structure that is much more complex. In this post I dove into one such problem. The multiscale tree construction gave a checkable notion of what it means to preserve a dataset, and it gave an provably-correct algorithm that actually produces such samples. Optimal transport offered a different lens that dependend more on the geometry of distributions. In one dimension it turned an abstract idea toward specifics, leveraging quantiles. But the real point here is broader than the downsampling problem. If you’re lucky, you’ll keep running into problems like this that are practical, slightly annoying, but amazingly elegant once you pause long enough to chalk out the problem. You don’t need to solve them perfectly on the first pass. You just need to take a stab, write down a definition that you believe in, and see the power that mathematics and modeling gives to you. Recommended Readings / Works that Served as Inspiration for this Post</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">The Economics of AI Inference Capacity</title>
      
      
      <link href="https://a-sircar1.github.io/2025/12/23/AI-Firms/" rel="alternate" type="text/html" title="The Economics of AI Inference Capacity" />
      
      <published>2025-12-23T06:10:56+00:00</published>
      <updated>2025-12-23T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2025/12/23/AI-Firms</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2025/12/23/AI-Firms/">&lt;p&gt;The world of artificial intelligence has often always been narrated through the lens of software, hardware, model development, and new products. Only lately, I’ve been noticing the breadth of articles related to the economics of all the infrastructure development and model offerings, especially in the LLM space. After all, modern AI is produced and provided using scarce resources like compute and memory running within cooled and power-constrained facilities. So, it only makes sense that the economics of AI and the sensational financing will begin to attract more attention. I’ve recently been reading a lot about productionized AI, and I’ve realized that the “supply of AI” is not really something that can be determined in immediate response to demand simply because there’s all this infrastructure that needs to be built, financed, installed, etc. &lt;sup id=&quot;fnref:reuters-dc-deals&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:reuters-dc-deals&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Then only can “allocations” in the economic sense be made.&lt;/p&gt;

&lt;p&gt;I think this matters mostly because the features of the market that are most visible to consumers, like rate limits, priority tiers, reserved throughput, pre-sales on compute resources, etc., are all key signatures of this market environment where capacity needs to be committed prior to the measurement of industry-wide demand. &lt;sup id=&quot;fnref:anthropic-rate-limits&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:anthropic-rate-limits&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:gemini-rate-limits&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gemini-rate-limits&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:openai-priority-page&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:openai-priority-page&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:openai-scale&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:openai-scale&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This concept stuck out to me, especially as I was taking my intermediate microeconomics course this semester. It’s not really a case that we studied in class, but I decided to take a crack at modeling the phenomenon to see what I would find. The hand-wavy anecdote can be boiled down into a relatively clear microeconomic dilemma: via Bertrand competition, if several firms sell an identical service and marginal costs are low, then the price competition should be intense. But, it appears that AI services price and allocate as if scarcity is the organizing principle of the market environment and not that there happens to be scarce inputs that are factors of production. &lt;sup id=&quot;fnref:bergemann-acm&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bergemann-acm&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:zhong-arxiv&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zhong-arxiv&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;See, if the latter were the case, LLMs should have a relatively simple, per-unit pricing scheme where prices would be cut until they hit the marginal cost. But in the current setting, we see firms compete by committing more and more capital upstream and rationing resources downstream. What separates AI from the general software product intuition is that, instead of cloud-like elasticity where a rise in demand increases capacity consumption and drives the marginal token cost down, the market is characterized by a race for upstream capital commitment costs and allocation that put price rules in the backseat. The fact that there are so many market pricing features indicates just that– the capacity constraint is so binding that firms cannot and will not shift away from their complicated pricing structures. And the massive timespan of AI financing only means that these pricing features are likely here to stay.&lt;/p&gt;

&lt;p&gt;In this post, I attempt to wrap these ideas into a model. In the setting I establish, I’ll show that when firms choose capacity first and then compete in prices, the price competition does not necessarily drive prices to marginal cost. In fact, in a two-stage game, the equilibrium features markups even though API-calling as a service appears to be homogenous AND Bertrand pricing is used in the second stage of the set-up.&lt;/p&gt;

&lt;p&gt;In the model, I’ll treat one good as a standardized unit of a service that is compliant with a fixed service-level agreement. In the case of AI and LLMs, this is simply a standardized unit of inference (output token or bundle of tokens) that is produced by a fixed model within a specific reliability target.&lt;/p&gt;

&lt;h2&gt;Demand and Costs&lt;/h2&gt;

&lt;p&gt;Let market demand for a general AI inference tool at price \(p\) be linear and take the form:&lt;/p&gt;

\[D(p) = A - p\]

&lt;p&gt;such that \(A &amp;gt; 0\), and so that the inverse demand is given as:&lt;/p&gt;

\[P(Q) = A - Q\]

&lt;p&gt;Let the marginal service cost of the tool be constant \(c \in [0, A)\). This can be thought of the cost of serving one more unit, conditional on a given model architecture or service-level agreement. Inference is often memory-constrained and KV-cache-bandwidth constrained. So the marginal cost depends on utilization and latency targets, and \(c\) is a stylized approximation, holding the service tier fixed. &lt;sup id=&quot;fnref:erdil-arxiv&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:erdil-arxiv&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Now, it is important to realize that markups generally result from some form of market power. So, the best place to start is the monopoly setting. With a downward-sloping demand the monopolist’s marginal revenue lies below demand, and so the profit condition that equates marginal revenue and marginal cost (\(MR = MC\)) implies price is greater than marginal cost (\(P &amp;gt; MC\)). I re-derive this below to make the point.&lt;/p&gt;

&lt;h2&gt;Monopoly Marginal Revenue&lt;/h2&gt;

&lt;p&gt;Let a single decision maker face (as monopolies are price-setters) an inverse demand \(P(q)\) with \(P&apos;(q)&amp;lt;0\). Revenue is&lt;/p&gt;

\[R(q)=P(q)\cdot q\]

&lt;p&gt;Differentiate using the product rule as follows:&lt;/p&gt;

\[R&apos;(q)=\frac{d}{dq}\big(P(q)q\big)=P&apos;(q)\,q+P(q)\]

&lt;p&gt;Since \(P&apos;(q)&amp;lt;0\) and \(q&amp;gt;0\), we have&lt;/p&gt;

\[P&apos;(q)\,q&amp;lt;0 \Longrightarrow R&apos;(q)=P&apos;(q)\,q+P(q)&amp;lt;P(q)\]

&lt;p&gt;Profit is \(\pi(q)=R(q)-cq\). The first-order condition (FOC) is then&lt;/p&gt;

\[\pi&apos;(q)=R&apos;(q)-c=0 \implies R&apos;(q)=c\]

&lt;p&gt;Combine \(R&apos;(q)=c\) with \(R&apos;(q)&amp;lt;P(q)\) to get&lt;/p&gt;

\[c=R&apos;(q)&amp;lt;P(q) \Longrightarrow P(q)&amp;gt;c\]

&lt;p&gt;So it can be seen that the monopoly markup \(P(q)-c\) simply arises from the inequality \(MR&amp;lt;P\) created by the downward-sloping demand. Then, we can build a benchmark using the linear demand defined above.&lt;/p&gt;

&lt;h3&gt;Monopoly with linear demand&lt;/h3&gt;

&lt;p&gt;If \(P(q)=A-q\), then&lt;/p&gt;

\[R(q)=(A-q)q=Aq-q^2\]

&lt;p&gt;so \(R&apos;(q)=A-2q\). Set \(R&apos;(q)=c\):&lt;/p&gt;

\[\begin{aligned}
A-2q&amp;amp;=c \\
2q&amp;amp;=A-c \\
q^M&amp;amp;=\frac{A-c}{2}
\end{aligned}\]

&lt;p&gt;Then&lt;/p&gt;

\[p^M=P(q^M)=A-q^M=A-\frac{A-c}{2}=\frac{A+c}{2}\]

&lt;p&gt;and the markup is \(p^M-c=\frac{A-c}{2}\).&lt;/p&gt;

&lt;h2&gt;Oligopoly Benchmarks&lt;/h2&gt;

&lt;p&gt;Now, we move to the oligopolistic setting. First, consider&lt;/p&gt;

&lt;h3&gt;Bertrand competition&lt;/h3&gt;

&lt;p&gt;Two firms choose prices \((p_1,p_2)\) simultaneously, and the lower-priced firm meets the demand, any ties split the demand, and each has marginal cost \(c\). Restrict to \(p_i\ge c\). Standard from theory, \((p_1,p_2)=(c,c)\) is a Nash equilibrium. This can be seen as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If both set \(c\), each earns zero profit.&lt;/li&gt;
  &lt;li&gt;Deviate upward: if firm 1 sets \(p_1&amp;gt;c\) while firm 2 stays at \(c\), firm 1 sells zero and earns zero, and there is no improvement.&lt;/li&gt;
  &lt;li&gt;Deviate downward: if firm 1 sets \(p_1&amp;lt;c\) it wins the market but earns negative returns at the margin, so profit is negative.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus no deviation strictly improves profit. Next, from theory, no Nash equilibrium has both prices above \(c\).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;If \(p_1&amp;lt;p_2\) with both \(&amp;gt;c\), firm 2 can undercut slightly (still above \(c\)) and capture the market, strictly improving profit.&lt;/li&gt;
  &lt;li&gt;If \(p_1=p_2&amp;gt;c\), either firm can undercut by a small amount \(\varepsilon\) and capture the market, strictly improving profit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So under a homogeneous good within the Betrand model, price is driven down to marginal cost.&lt;/p&gt;

&lt;h3&gt;Cournot competition&lt;/h3&gt;

&lt;p&gt;Firms compete on quantity in this set-up. Firms choose quantities \(q_1,q_2\), total \(Q=q_1+q_2\), price \(P(Q)=A-Q\). The profit is then:
\(\pi_i(q_1,q_2)=\big(A-(q_1+q_2)\big)q_i-cq_i\)&lt;/p&gt;

&lt;p&gt;Fix \(q_{-i}\). Then&lt;/p&gt;

\[\begin{aligned}
\pi_i(q_i)
&amp;amp;=\big(A-q_i-q_{-i}\big)q_i-cq_i \\
&amp;amp;=(A-q_{-i})q_i-q_i^2-cq_i \\
&amp;amp;=(A-c-q_{-i})q_i-q_i^2
\end{aligned}\]

&lt;p&gt;Differentiate:&lt;/p&gt;

\[\frac{d\pi_i}{dq_i}=(A-c-q_{-i})-2q_i\]

&lt;p&gt;The FOC is:&lt;/p&gt;

\[(A-c-q_{-i})-2q_i=0 \implies q_i=\frac{A-c-q_{-i}}{2}\]

&lt;p&gt;In symmetric equilibrium \(q_1=q_2=q\):&lt;/p&gt;

\[\begin{aligned}
q&amp;amp;=\frac{A-c-q}{2} \\
2q&amp;amp;=A-c-q \\
3q&amp;amp;=A-c \\
q^C&amp;amp;=\frac{A-c}{3}
\end{aligned}\]

&lt;p&gt;Then&lt;/p&gt;

\[\begin{aligned}
Q^C&amp;amp;=2q^C=\frac{2(A-c)}{3}\\
p^C&amp;amp;=A-Q^C=\frac{A+2c}{3}
\end{aligned}\]

&lt;p&gt;and&lt;/p&gt;

\[p^C-c=\frac{A-c}{3}\]

&lt;p&gt;And we find that \(p^C-c=q^C\). Note that the inverse demand curve in this setting is very similar to the monopoly demand curve. Fix \(q_{-i}\), and see that, firm \(i\) then faces inverse demand \(P(q_i+q_{-i})=(A-q_{-i})-q_i\) which is literally a monopoly demand curve with a shifted intercept.&lt;/p&gt;

&lt;h2&gt;The Capacity Game&lt;/h2&gt;

&lt;p&gt;Now, simply adjust the timing of events where the capacity selection comes first. Note that capacity can be thought of concretely as “tokens per second that can be generated or consumed,” and its binding constraints are generally due to GPU compute availability and memory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 1: Choosing capacity.&lt;/strong&gt; Firm \(i\in\{1,2\}\) chooses a capacity \(k_i\ge 0\) (“how many service units per period can be reliably served?”). Capacity cost is convex in the form:&lt;/p&gt;

\[I(k_i)=\frac{\gamma}{2}k_i^2\]

&lt;p&gt;with \(\gamma\ge 0\). Interpret \(\gamma\) as paramter that controls difficulty in scaling that can be attributed to power and space constraints, bottlenecks in memory, frictions in deployment, and the reality that capacity cannot be conjured instantly.&lt;sup id=&quot;fnref:reuters-memory&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:reuters-memory&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stage 2: Posting price.&lt;/strong&gt; Firms then post prices \(p_i\ge 0\). Consumers buy from the lower-priced firm first, but each firm can then serve at most its capacity. Any unserved demand spills over to the other firm. So, let \(K=k_1+k_2\) and define the market-clearing price&lt;/p&gt;

\[p^*(K)=A-K\]

&lt;p&gt;so that&lt;/p&gt;

\[D(p^*(K))=A-p^*(K)=K\]

&lt;h3&gt;Pricing subgame lemma given capacities&lt;/h3&gt;

&lt;p&gt;We need one basic argument: the pricing subgame has a pure-strategy equilibrium at the market-clearing price.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma (pricing equilibrium with binding total capacity).&lt;/strong&gt;&lt;br /&gt;
Fix \(k_1,k_2\). Suppose \(\text{(i) }p^*(K)\ge c\) and \(\text{(ii) }k_i\le p^*(K)-c\) for \(i=1, 2\). Then the stage 2 pricing subgame has a Nash equilibrium at \((p_1,p_2)=(p^*(K),p^*(K))\) in which each firm sells its full capacity \(q_i=k_i\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;br /&gt;
Assume that both of the firms set \(p^*(K)\). Then the total demand equals total capacity, so the market clears exactly, and each firm sells \(q_i=k_i\) under a tie-breaking rule.&lt;/p&gt;

&lt;p&gt;Without loss of generality, consider deviations by firm 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a) Undercut:&lt;/strong&gt; set \(p_1=p^*(K)-\varepsilon\) with \(\varepsilon&amp;gt;0\). Then firm 1 is cheapest, so it receives first priority in demand. But it cannot sell more than \(k_1\). At price \(p_1\), demand is&lt;/p&gt;

\[D(p_1)=A-p_1=A-(p^*(K)-\varepsilon)=K+\varepsilon&amp;gt;K\]

&lt;p&gt;so there is more than enough demand to fill capacity. So firm 1 still sells \(q_1=k_1\). Its operating profit is then&lt;/p&gt;

\[(p_1-c)k_1=(p^*(K)-\varepsilon-c)k_1=(p^*(K)-c)k_1-\varepsilon k_1\]

&lt;p&gt;which is strictly lower than \((p^*(K)-c)k_1\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b) Raise price:&lt;/strong&gt; set \(p_1=p^*(K)+x\) with \(x&amp;gt;0\). Then firm 2 sells first up to capacity \(k_2\), and firm 1 sells the residual demand. Total demand at \(p_1\) is \(A-p_1\), so residual demand after firm 2 serves \(k_2\) is&lt;/p&gt;

\[(A-p_1)-k_2\]

&lt;p&gt;Substitute \(p_1=p^*(K)+x\) and \(p^*(K)=A-K\):&lt;/p&gt;

\[\begin{aligned}
(A-(p^*(K)+x))-k_2
&amp;amp;=A-(A-K+x)-k_2 \\
&amp;amp;=K-x-k_2 \\
&amp;amp;=k_1-x
\end{aligned}\]

&lt;p&gt;Thus for \(0\le x\le k_1\), firm 1 sells \(q_1=k_1-x\) and earns&lt;/p&gt;

\[\pi_1(x)=(p^*(K)+x-c)(k_1-x)\]

&lt;p&gt;Differentiate to get:&lt;/p&gt;

\[\begin{aligned}
\pi_1&apos;(x)
&amp;amp;=\frac{d}{dx}\big((p^*(K)+x-c)(k_1-x)\big) \\
&amp;amp;=(k_1-x)\cdot (1) + (p^*(K)+x-c)\cdot(-1) \\
&amp;amp;=k_1-x-p^*(K)-x+c \\
&amp;amp;=k_1-(p^*(K)-c)-2x
\end{aligned}\]

&lt;p&gt;At \(x=0\), \(\pi_1&apos;(0)=k_1-(p^*(K)-c)\)&lt;/p&gt;

&lt;p&gt;So a small price increase is unprofitable whenever \(k_1\le p^*(K)-c\), which is condition (ii). As the above holds in generality, it holds for firm 2 as well.&lt;/p&gt;

&lt;p&gt;Thus neither undercutting nor raising price is profitable. \((p^*(K),p^*(K))\) is a Nash equilibrium. ∎&lt;/p&gt;

&lt;p&gt;Note that in the equilibria that will be computed next, the total capacity satisfies \(K\le A-c\) and each firm’s capacity is not too large relative to the margin, so the lemma’s conditions should hold. Intuitively, if a modest capacity is built relative to demand, there does not need to be a discount to fill the gap.&lt;/p&gt;

&lt;h3&gt;Stage 1 is a capacity choice in the Cournot style&lt;/h3&gt;

&lt;p&gt;Because the price subgame clears at \(p=A-(k_1+k_2)\) and each firm sells its capacity, stage 1 effectively becomes:&lt;/p&gt;

\[\max_{k_i\ge 0}\ \Pi_i(k_i,k_{-i})
=\big(A-(k_i+k_{-i})-c\big)k_i-\frac{\gamma}{2}k_i^2\]

&lt;p&gt;Expand it and get:&lt;/p&gt;

\[\begin{aligned}
\Pi_i
&amp;amp;=(A-c-(k_i+k_{-i}))k_i-\frac{\gamma}{2}k_i^2 \\
&amp;amp;=(A-c)k_i-k_i^2-k_{-i}k_i-\frac{\gamma}{2}k_i^2 \\
&amp;amp;=(A-c)k_i-\Big(1+\frac{\gamma}{2}\Big)k_i^2-k_{-i}k_i
\end{aligned}\]

&lt;p&gt;Differentiate with respect to \(k_i\):&lt;/p&gt;

\[\begin{aligned}
\frac{\partial \Pi_i}{\partial k_i}
&amp;amp;=(A-c)-2\Big(1+\frac{\gamma}{2}\Big)k_i-k_{-i} \\
&amp;amp;=(A-c)-(2+\gamma)k_i-k_{-i}
\end{aligned}\]

&lt;p&gt;The FOC is:&lt;/p&gt;

\[(A-c)-(2+\gamma)k_i-k_{-i}=0 \implies k_i=\frac{A-c-k_{-i}}{2+\gamma}\]

&lt;p&gt;In a symmetric equilibrium \(k_1=k_2=k\):&lt;/p&gt;

\[\begin{aligned}
k&amp;amp;=\frac{A-c-k}{2+\gamma} \\
(2+\gamma)k&amp;amp;=A-c-k \\
(3+\gamma)k&amp;amp;=A-c \\
k^{NE}&amp;amp;=\frac{A-c}{3+\gamma}
\end{aligned}\]

&lt;p&gt;So total capacity is&lt;/p&gt;

\[K^{NE}=2k^{NE}=\frac{2(A-c)}{3+\gamma}\]

&lt;p&gt;and the induced market-clearing price is&lt;/p&gt;

\[\begin{aligned}
p^{NE}
&amp;amp;=A-K^{NE} \\
&amp;amp;=A-\frac{2(A-c)}{3+\gamma} \\
&amp;amp;=\frac{A(3+\gamma)-2A+2c}{3+\gamma} \\
&amp;amp;=\frac{A(1+\gamma)+2c}{3+\gamma}
\end{aligned}\]

&lt;p&gt;The markup is then:&lt;/p&gt;

\[\begin{aligned}
p^{NE}-c
&amp;amp;=\frac{A(1+\gamma)+2c}{3+\gamma}-c \\
&amp;amp;=\frac{A(1+\gamma)+2c-c(3+\gamma)}{3+\gamma} \\
&amp;amp;=\frac{(1+\gamma)(A-c)}{3+\gamma}
\end{aligned}\]

&lt;h3&gt;The Cournot shadow&lt;/h3&gt;

&lt;p&gt;If capacity is cheap to scale (set \(\gamma=0\)), then&lt;/p&gt;

\[k^{NE}=\frac{A-c}{3},
\quad
p^{NE}=\frac{A+2c}{3}\]

&lt;p&gt;which is exactly the Cournot duopoly outcome with \(k\) playing the role of \(q\).&lt;/p&gt;

&lt;p&gt;At first glance this can seem paradoxical. Stage 2 is Bertrand pricing, so why doesn’t the Bertrand case force \(p=c\)? The reason is that the Bertrand move to “undercut-by-\(\varepsilon\) and take the whole market” only works if the lower price firm can actually serve the whole market. Once quantities are committed beforehand (here this is the capacity), undercutting does not let you expand quantity beyond your capacity. Price competition is still there, but it no longer determines the output. Instead it mainly allocates who serves first, up to their prechosen capacities.&lt;/p&gt;

&lt;p&gt;This observation is formalized in a classic paper by Kreps and Scheinkman (1983). &lt;sup id=&quot;fnref:kreps-jstor&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:kreps-jstor&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; They study a two-stage game in which firms first choose capacities (or quantities) and then compete in prices with capacity constraints. Under standard rationing assumptions, the equilibrium outcome matches with the Cournot quantity outcome in that the strategic action “moves upstream” into the capacity choice, and the second-stage price competition ends up clearing the market at a price that is consistent with total committed capacity.&lt;/p&gt;

&lt;h2&gt;Welfare&lt;/h2&gt;

&lt;p&gt;Under welfare theorems, competitive equilibrium is found to be efficient under price-taking. Our equilibrium is not price-taking, so the theorems should not apply. Then how should efficiency be benchmarked? I choose to compare equilibrium total capacity to the surplus-maximizing total capacity.&lt;/p&gt;

&lt;h3&gt;Planner’s problem (case with two firms)&lt;/h3&gt;

&lt;p&gt;Total gross benefit from serving quantity \(Q\) is the area under inverse demand:&lt;/p&gt;

\[\int_0^Q P(q)\,dq=\int_0^Q (A-q)\,dq=AQ-\frac{Q^2}{2}\]

&lt;p&gt;Serving \(Q\) costs \(cQ\). First, recognize that capacity costs can be minimized by splitting \(k_1=k_2=K/2\)&lt;sup class=&quot;proof-inline&quot;&gt;&lt;a href=&quot;#&quot; data-proof=&quot;pf-k-split&quot;&gt;See proof&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;details id=&quot;pf-k-split&quot; class=&quot;proof-box&quot;&gt;
  &lt;summary&gt;Proof of $$k_1 = k_2 = K/2$$&lt;/summary&gt;
  &lt;div&gt;
    Fix total capacity \(K = k_1 + k_2\) with \(k_1,k_2 \ge 0\).
    Capacity cost is \(I(k)=\tfrac{\gamma}{2}k^2\) with \(\gamma\ge 0\).
    Impose the constraint by writing \(k_2 = K - k_1\). Define
    \[
    C(k_1)=I(k_1)+I(K-k_1)=\tfrac{\gamma}{2}\big(k_1^2+(K-k_1)^2\big)
    \]
    Expand:
    \[
    C(k_1)=\tfrac{\gamma}{2}\big(2k_1^2-2Kk_1+K^2\big)
    \]
    differentiate:
    \[
    C&apos;(k_1)=\gamma(2k_1-K)
    \]
    set \(C&apos;(k_1)=0\Rightarrow k_1=K/2\), and \(C&apos;&apos;(k_1)=2\gamma\ge 0\),
    so \(k_1=K/2\) is a global minimizer. Thus \(k_2=K/2\). Plugging in,
    \[
    I(K/2)+I(K/2)=\gamma\frac{K^2}{4}
    \]
  &lt;/div&gt;
&lt;/details&gt;

\[\frac{\gamma}{2}\Big(\frac{K}{2}\Big)^2+\frac{\gamma}{2}\Big(\frac{K}{2}\Big)^2
=\gamma\frac{K^2}{4}\]

&lt;p&gt;The planner never builds unused capacity in this toy model, so set \(Q=K\).&lt;/p&gt;

\[\begin{aligned}
W(K)
&amp;amp;=\Big(AK-\frac{K^2}{2}\Big)-cK-\gamma\frac{K^2}{4} \\
&amp;amp;=(A-c)K-\Big(\frac12+\frac{\gamma}{4}\Big)K^2
\end{aligned}\]

&lt;p&gt;Differentiate:&lt;/p&gt;

\[\begin{aligned}
W&apos;(K)
&amp;amp;=(A-c)-2\Big(\frac12+\frac{\gamma}{4}\Big)K \\
&amp;amp;=(A-c)-\Big(1+\frac{\gamma}{2}\Big)K
\end{aligned}\]

&lt;p&gt;Set \(W&apos;(K)=0\):&lt;/p&gt;

\[\begin{aligned}
(A-c)-\Big(1+\frac{\gamma}{2}\Big)K&amp;amp;=0 \\
K^*&amp;amp;=\frac{A-c}{1+\gamma/2}=\frac{2(A-c)}{2+\gamma}
\end{aligned}\]

&lt;p&gt;Market equilibrium total capacity determined earlier is&lt;/p&gt;

\[K^{NE}=\frac{2(A-c)}{3+\gamma}\]

&lt;p&gt;Take the ratio:&lt;/p&gt;

\[\begin{aligned}
\frac{K^{NE}}{K^*}
&amp;amp;=\frac{\frac{2(A-c)}{3+\gamma}}{\frac{2(A-c)}{2+\gamma}} \\
&amp;amp;=\frac{2+\gamma}{3+\gamma} \\
&amp;amp;&amp;lt;1
\end{aligned}\]

&lt;p&gt;So we get the following statement describing the:&lt;/p&gt;

\[\frac{K^{NE}}{K^*}=\frac{2+\gamma}{3+\gamma}&amp;lt;1\]

&lt;p&gt;So, it is seen that strategic capacity choice underprovides capacity relative to the surplus-maximizing benchmark by the factor shown above.&lt;/p&gt;

&lt;h3&gt;A strong welfare identity&lt;/h3&gt;

&lt;p&gt;Because the welfare function is concave and quadratic, the welfare loss can be computed in closed form in addition to the capacity loss. First, write&lt;/p&gt;

\[W(K)=(A-c)K-aK^2\]

&lt;p&gt;with \(a=\frac12+\frac{\gamma}{4}=\frac{2+\gamma}{4}\). The maximizer is \(K^*=\frac{A-c}{2a}\) and the maximum value is&lt;/p&gt;

\[W^*=\frac{(A-c)^2}{4a}=\frac{(A-c)^2}{2+\gamma}\]

&lt;p&gt;Now plug \(K^{NE}=\frac{2(A-c)}{3+\gamma}\) into \(W(K)\):&lt;/p&gt;

\[\begin{aligned}
W^{NE}
&amp;amp;=(A-c)\frac{2(A-c)}{3+\gamma}
-a\Big(\frac{2(A-c)}{3+\gamma}\Big)^2 \\
&amp;amp;=\frac{2(A-c)^2}{3+\gamma}-\frac{2+\gamma}{4}\cdot \frac{4(A-c)^2}{(3+\gamma)^2} \\
&amp;amp;=\frac{2(A-c)^2}{3+\gamma}-\frac{(2+\gamma)(A-c)^2}{(3+\gamma)^2} \\
&amp;amp;=\frac{(A-c)^2}{(3+\gamma)^2}\Big(2(3+\gamma)-(2+\gamma)\Big) \\
&amp;amp;=\frac{(A-c)^2}{(3+\gamma)^2}(4+\gamma)
\end{aligned}\]

&lt;p&gt;So&lt;/p&gt;

\[\frac{W^{NE}}{W^*} =
\frac{\frac{(A-c)^2(4+\gamma)}{(3+\gamma)^2}}{\frac{(A-c)^2}{2+\gamma}} =
\frac{(2+\gamma)(4+\gamma)}{(3+\gamma)^2} = 1-\frac{1}{(3+\gamma)^2}\]

&lt;p&gt;Thus:&lt;/p&gt;

\[\begin{aligned}
\frac{W^{NE}}{W^*}&amp;amp;=1-\frac{1}{(3+\gamma)^2} \\
W^*-W^{NE}&amp;amp;=\frac{(A-c)^2}{(2+\gamma)(3+\gamma)^2}
\end{aligned}\]

&lt;p&gt;This is slightly counterintuitive– the fractional welfare loss decreases with the factor \(1/(3+\gamma)^2\), even though the prices can remain above marginal cost.&lt;/p&gt;

&lt;h2&gt;Peak-load Demand&lt;/h2&gt;

&lt;p&gt;Everything so far has assumed that there is a fixed demand intercept \(A\). In reality, AI demand is not constant. It has peaks. The easiest way to incorporate that is to let the demand intercept be random &lt;strong&gt;after&lt;/strong&gt; the capacity is chosen. So, let the period demand curve be&lt;/p&gt;

\[\tilde D(p)=\tilde A-p\]

&lt;p&gt;so inverse demand is&lt;/p&gt;

\[\tilde P(Q)=\tilde A-Q\]

&lt;p&gt;Capacities \(k_1,k_2\) are selected first, with total capacity \(K=k_1+k_2\). Then \(\tilde A\) is realized and the same capacity-constrained pricing game is played.&lt;/p&gt;

&lt;p&gt;The point here is not to simply add noise to the set-up. The goal is to isolate the economic impacts of the value of capacity in peak demand periods.&lt;/p&gt;

&lt;h3&gt;Spot price with capacity&lt;/h3&gt;

&lt;p&gt;We can fix the total capacity \(K\) and a market realization \(\tilde A\). The market-clearing price that equates demand to total capacity is \(\tilde A-K\). But if demand is low enough, then the capacity is slack, and under vanilla Bertrand competition, this would push the spot usage price down to the marginal cost.&lt;/p&gt;

&lt;p&gt;There is a minor technical subtlety here. When total capacity is slack, a single firm may still have local market power if its rival cannot serve the whole residual demand by itself (we’ve noted something similar above). In production AI markets, that kind of power is often self-regulated by substitution and contestability in that users have flexibility in model/provider selection. &lt;sup id=&quot;fnref:fradkin-demand-arxiv&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fradkin-demand-arxiv&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:openrouter-state&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:openrouter-state&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; I capture this in a simple way– when capacity is slack, the spot usage price is capped at marginal cost \(c\).&lt;/p&gt;

&lt;p&gt;With that in mind, the spot price in the stage 2 equilibrium can be written in one line.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition (spot price with capacity).&lt;/strong&gt;&lt;br /&gt;
In the stage 2 equilibrium,&lt;/p&gt;

\[\tilde p(\tilde A,K)=\max\{c,\ \tilde A-K\}\]

&lt;p&gt;and therefore the scarcity premium is&lt;/p&gt;

\[\tilde p(\tilde A,K)-c=(\tilde A-c-K)^+\]

&lt;p&gt;where \((x)^+=\max\{x,0\}\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; If \(\tilde A-K&amp;lt;c\), then at price \(p=c\) demand is \(\tilde A-c&amp;lt;K\), so the capacity is slack and the spot price is \(c\). If \(\tilde A-K\ge c\), then the market-clearing price \(\tilde A-K\) is at least the marginal cost. In that case, the same argument as in the deterministic pricing lemma can be applied– undercutting cannot increase the quantity beyond capacity, and so it only reduces margin. And raising price loses residual demand. So, the outcome price is \(\tilde A-K\). Combining the two cases produces the formula above. ∎&lt;/p&gt;

&lt;p&gt;Now define \(X=\tilde A-c\). Then the scarcity premium is&lt;/p&gt;

\[\tilde p-c=(X-K)^+\]

&lt;p&gt;This expression is the unit that I use to think about congestion. It is basically the premium that is realized only when demand is above what the system can serve at the service level.&lt;/p&gt;

&lt;p&gt;Finally, define the &lt;strong&gt;expected scarcity rent&lt;/strong&gt; at total capacity \(K\) by&lt;/p&gt;

\[\Lambda(K)=\mathbb E\big[(X-K)^+\big]\]

&lt;p&gt;This is the value that priority tiers and guaranteed throughput try to capture– the expected congestion rent per unit.&lt;/p&gt;

&lt;h3&gt;Two identities for \(\Lambda(K)\)&lt;/h3&gt;

&lt;p&gt;Assume \(X\) has CDF \(F\) and finite mean.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma (tail-integral formula).&lt;/strong&gt;&lt;/p&gt;

\[\Lambda(K)=\int_K^\infty (1-F(t))\,dt\]

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; Start from&lt;/p&gt;

\[\Lambda(K)=\int_{-\infty}^{\infty} (x-K)^+\,dF(x)=\int_K^\infty (x-K)\,dF(x)\]

&lt;p&gt;Write \(x-K=\int_K^x 1\,ds\) and swap the integrals:&lt;/p&gt;

\[\begin{aligned}
\Lambda(K)
&amp;amp;=\int_K^\infty\left(\int_K^x 1\,ds\right)dF(x) \\
&amp;amp;=\int_K^\infty\left(\int_s^\infty dF(x)\right)ds \\
&amp;amp;=\int_K^\infty (1-F(s))\,ds
\end{aligned}\]

&lt;p&gt;Rename \(s\) to \(t\). ∎&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary (marginal value equals congestion probability).&lt;/strong&gt;&lt;br /&gt;
If \(F\) is continuous at \(K\), then&lt;/p&gt;

\[\Lambda&apos;(K)=-(1-F(K))=-\Pr(X&amp;gt;K)\]

&lt;h2&gt;Efficient Capacity under Peak-load Demand&lt;/h2&gt;

&lt;p&gt;I now redo the welfare calculation from above in this new stochastic environment. The setting remains the same in that inverse demand is linear in each period, marginal service cost is constant, and the capacity cost is convex. As before, a planner chooses total capacity \(K\) and splits it symmetrically across the two identical technologies. Total capacity cost is then minimized by the split, producing \(\gamma\frac{K^2}{4}\). In a given state \(\tilde A\), serving quantity \(Q\) generates the total benefit&lt;/p&gt;

\[\int_0^Q (\tilde A-q)\,dq=\tilde A Q-\frac{Q^2}{2}\]

&lt;p&gt;Net of service cost \(cQ\), the net benefit per state is:&lt;/p&gt;

\[(\tilde A-c)Q-\frac{Q^2}{2}=XQ-\frac{Q^2}{2}\]

&lt;p&gt;Given the capacity \(K\), the planner will not serve more than \(K\). Also, in each state, the unconstrained efficient quantity solves \(\tilde A-Q=c\), so \(Q=X\). Thus the served quantity is \(Q(K,X)=\min\{K,X\}\). Expected welfare is therefore&lt;/p&gt;

\[W(K)=\mathbb E\!\left[X\min\{K,X\}-\frac{1}{2}\min\{K,X\}^2\right]-\gamma\frac{K^2}{4}\]

&lt;p&gt;Rather than expanding this piecewise, it is easier to calculate the marginal condition directly. Increase capacity from \(K\) to \(K+dK\). In states where \(X\le K\), capacity is slack and the extra unit is unused, so the marginal welfare gain would be zero. In states where \(X&amp;gt;K\), the capacity is binding at \(Q=K\), and the marginal welfare gain from an extra unit is the marginal net benefit at \(Q=K\):&lt;/p&gt;

\[\frac{\partial}{\partial Q}\left(XQ-\frac{Q^2}{2}\right)\Bigg|_{Q=K} = X-K\]

&lt;p&gt;So the expected marginal benefit of capacity is exactly \(\mathbb E[(X-K)^+]=\Lambda(K)\). The marginal cost of capacity is:&lt;/p&gt;

\[\frac{d}{dK}\left(\gamma\frac{K^2}{4}\right)=\gamma\frac{K}{2}\]

&lt;p&gt;Therefore the efficient capacity satisfies the FOC:&lt;/p&gt;

\[\Lambda(K^*)=\gamma\frac{K^*}{2}\]

&lt;p&gt;This houses the deterministic result from before. If \(X=A-c\) is constant, then \(\Lambda(K)=(A-c-K)^+\) and&lt;/p&gt;

\[A-c-K^*=\gamma\frac{K^*}{2}
\implies
K^*=\frac{2(A-c)}{2+\gamma}\]

&lt;p&gt;So under uncertainty, the efficient rule is still simple. Build capacity until expected congestion rent is equal to the marginal cost of capacity.&lt;/p&gt;

&lt;h2&gt;Oligopoly Capacity under Peak-load Demand&lt;/h2&gt;

&lt;p&gt;Now repeat the capacity game for two firms under the same uncertainty.&lt;/p&gt;

&lt;p&gt;In the stage 2 equilibrium, the scarcity premium in state \(X\) is \((X-K)^+\). When that premium is positive, the system is capacity constrained and each firm sells approximately its full capacity. In that case, firm \(i\)’s operating profit in state \(X\) is \((X-K)^+\,k_i\). Taking expectations and subtracting capacity cost gives the stage 1 expected payoff&lt;/p&gt;

\[\Pi_i(k_i,k_{-i})=k_i\,\Lambda(K)-\frac{\gamma}{2}k_i^2\]

&lt;p&gt;and \(K=k_i+k_{-i}\). Differentiate with respect to \(k_i\), using \(\partial K/\partial k_i=1\):&lt;/p&gt;

\[\frac{\partial \Pi_i}{\partial k_i} = \Lambda(K)+k_i\Lambda&apos;(K)-\gamma k_i\]

&lt;p&gt;A best response satisfies \(\Lambda(K)+k_i\Lambda&apos;(K)=\gamma k_i\). Apply the corollary from before, \(\Lambda&apos;(K)=-\Pr(X&amp;gt;K)\):&lt;/p&gt;

\[\Lambda(K)-k_i\,\Pr(X&amp;gt;K)=\gamma k_i \implies \Lambda(K)=\big(\gamma+\Pr(X&amp;gt;K)\big)\,k_i\]

&lt;p&gt;In a symmetric equilibrium \(k_1=k_2=k\), so \(K=2k\) and&lt;/p&gt;

\[\Lambda(2k)=\big(\gamma+\Pr(X&amp;gt;2k)\big)\,k\]

&lt;p&gt;This is basically the stochastic analog of the Cournot shadow from before. It isolates a term that does not appear in the planner’s condition:&lt;/p&gt;

\[\Lambda(K^*)=\gamma\frac{K^*}{2}
\quad\text{vs.}\quad
\Lambda(K^{NE})=\gamma\frac{K^{NE}}{2}+\frac{K^{NE}}{2}\Pr(X&amp;gt;K^{NE})\]

&lt;p&gt;So the entire gap between private and efficient capacity comes from the additional term \(\frac{K}{2}\Pr(X&amp;gt;K)\). This is the strategic scarcity incentive. Each firm knows that adding capacity reduces the scarcity rent that it can earn in all peak states, and the strength of that effect is proportional to how often the peaks occur.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem (strategic underprovision)&lt;/strong&gt;&lt;br /&gt;
Assume that \(\Lambda(K)\) is strictly decreasing and continuous. Then the equilibrium total capacity \(K^{NE}\) is strictly below the efficient capacity \(K^*\).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt; The planner’s condition is \(\Lambda(K^*)=\gamma\frac{K^*}{2}\). The equilibrium condition is&lt;/p&gt;

\[\Lambda(K^{NE})=\gamma\frac{K^{NE}}{2}+\frac{K^{NE}}{2}\Pr(X&amp;gt;K^{NE})\]

&lt;p&gt;Whenever the capacity binds with positive probability, \(\Pr(X&amp;gt;K^{NE})&amp;gt;0\), so&lt;/p&gt;

\[\Lambda(K^{NE})&amp;gt;\gamma\frac{K^{NE}}{2}\]

&lt;p&gt;Since \(\Lambda(K)\) is strictly decreasing while \(\gamma K/2\) is strictly increasing, the intersection of these curves is \(K^*\). The inequality therefore implies that \(K^{NE}&amp;lt;K^*\). ∎&lt;/p&gt;

&lt;p&gt;This can be seen as a stronger inefficiency claim than the deterministic ratio that we found above. Here, the gap is tied to the congestion probability \(\Pr(X&amp;gt;K)\), which can be easily measured from past occurrences.&lt;/p&gt;

&lt;h2&gt;A Closed-form Example&lt;/h2&gt;

&lt;p&gt;Consider a simple but realistic example. Take a simple distribution for \(X\) that captures “how spiky the demand is.” Assume \(X\sim\mathrm{Unif}[0,M]\) with \(M&amp;gt;0\). Then for \(0\le K\le M\),&lt;/p&gt;

\[\begin{aligned}
\Pr(X&amp;gt;K)&amp;amp;=\frac{M-K}{M} \\
\Lambda(K)&amp;amp;=\mathbb E[(X-K)^+]=\frac{(M-K)^2}{2M}
\end{aligned}\]

&lt;h3&gt;Efficient capacity&lt;/h3&gt;

&lt;p&gt;The planner’s condition \(\Lambda(K^*)=\gamma K^*/2\) becomes&lt;/p&gt;

\[\frac{(M-K^*)^2}{2M}=\gamma\frac{K^*}{2}
\implies
(M-K^*)^2=\gamma M K^*\]

&lt;p&gt;The solution in \([0,M]\) is then&lt;/p&gt;

\[K^*=\frac{M}{2}\Big(2+\gamma-\sqrt{\gamma^2+4\gamma}\Big)\]

&lt;h3&gt;Symmetric oligopoly capacity&lt;/h3&gt;

&lt;p&gt;The equilibrium condition is&lt;/p&gt;

\[\Lambda(K^{NE})=\gamma\frac{K^{NE}}{2}+\frac{K^{NE}}{2}\Pr(X&amp;gt;K^{NE})\]

&lt;p&gt;Substitute the closed forms:&lt;/p&gt;

\[\frac{(M-K)^2}{2M}
=
\gamma\frac{K}{2}
+\frac{K}{2}\cdot\frac{M-K}{M}\]

&lt;p&gt;Multiply by \(2M\) and rearrange to get:&lt;/p&gt;

\[M^2-(\gamma+3)MK+2K^2=0\]

&lt;p&gt;The equilibrium total capacity is therefore&lt;/p&gt;

\[K^{NE}
=\frac{M}{4}\Big((\gamma+3)-\sqrt{(\gamma+3)^2-8}\Big)\]

&lt;p&gt;The comparison \(K^{NE}&amp;lt;K^*\) holds here as well, and you can see that the economic reason is there in the math. The oligopoly condition contains an extra peak-probability term, which pushes the capacity downward.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:reuters-dc-deals&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Reuters, &lt;em&gt;“AI boom drives data-center dealmaking to record high, says report”&lt;/em&gt;. Reuters (Dec 19, 2025) &lt;a href=&quot;https://www.reuters.com/business/ai-boom-drives-data-center-dealmaking-record-high-says-report-2025-12-19/&quot;&gt;https://www.reuters.com/business/ai-boom-drives-data-center-dealmaking-record-high-says-report-2025-12-19/&lt;/a&gt;. &lt;a href=&quot;#fnref:reuters-dc-deals&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:anthropic-rate-limits&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Anthropic (Claude Docs), &lt;em&gt;“Rate limits”&lt;/em&gt;.  &lt;a href=&quot;https://docs.anthropic.com/en/api/rate-limits&quot;&gt;https://docs.anthropic.com/en/api/rate-limits&lt;/a&gt;. &lt;a href=&quot;#fnref:anthropic-rate-limits&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gemini-rate-limits&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Google AI for Developers, &lt;em&gt;“Rate limits, Gemini API”&lt;/em&gt;. &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/rate-limits&quot;&gt;https://ai.google.dev/gemini-api/docs/rate-limits&lt;/a&gt;. &lt;a href=&quot;#fnref:gemini-rate-limits&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:openai-priority-page&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;OpenAI, &lt;em&gt;“Priority Processing for API Customers”&lt;/em&gt;. &lt;a href=&quot;https://openai.com/api-priority-processing/&quot;&gt;https://openai.com/api-priority-processing/&lt;/a&gt;. &lt;a href=&quot;#fnref:openai-priority-page&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:openai-scale&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;OpenAI, &lt;em&gt;“Scale Tier for API Customers”&lt;/em&gt;. &lt;a href=&quot;https://openai.com/api-scale-tier/&quot;&gt;https://openai.com/api-scale-tier/&lt;/a&gt;. &lt;a href=&quot;#fnref:openai-scale&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bergemann-acm&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Bergemann, Dirk; Bonatti, Alessandro; Smolin, Alex. &lt;em&gt;“The Economics of Large Language Models: Token Allocation, Fine-Tuning, and Optimal Pricing”&lt;/em&gt;. EC ‘25: Proceedings of the 26th ACM Conference on Economics and Computation (Jul 2, 2025). &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3736252.3742625&quot;&gt;https://dl.acm.org/doi/10.1145/3736252.3742625&lt;/a&gt;. &lt;a href=&quot;#fnref:bergemann-acm&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zhong-arxiv&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Zhong, Weijie. &lt;em&gt;“Token is All You Price”&lt;/em&gt;. arXiv prepring (Oct 10, 2025). &lt;a href=&quot;https://arxiv.org/abs/2510.09859&quot;&gt;https://arxiv.org/abs/2510.09859&lt;/a&gt;. &lt;a href=&quot;#fnref:zhong-arxiv&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:erdil-arxiv&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Erdil, Ege. &lt;em&gt;“Inference economics of language models”&lt;/em&gt;. arXiv preprint (Jun 5, 2025). &lt;a href=&quot;https://arxiv.org/abs/2506.04645&quot;&gt;https://arxiv.org/abs/2506.04645&lt;/a&gt;. &lt;a href=&quot;#fnref:erdil-arxiv&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:reuters-memory&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jin, Hyunjoo; Potkin, Fanny; Lee, Wen-Yee; Bridge, Anton; Cherney, Max. &lt;em&gt;“The AI frenzy is driving a new global supply chain crisis”&lt;/em&gt; Reuters (Dec 3, 2025). &lt;a href=&quot;https://www.reuters.com/world/china/ai-frenzy-is-driving-new-global-supply-chain-crisis-2025-12-03/&quot;&gt;https://www.reuters.com/world/china/ai-frenzy-is-driving-new-global-supply-chain-crisis-2025-12-03/&lt;/a&gt;. &lt;a href=&quot;#fnref:reuters-memory&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:kreps-jstor&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kreps, David; Scheinkman, Jose. &lt;em&gt;“Quantity Precommitment and Bertrand Competition Yield Cournot Outcomes”&lt;/em&gt;. The Bell Journal of Economics (Autumn 1983). &lt;a href=&quot;https://www.jstor.org/stable/3003636&quot;&gt;https://www.jstor.org/stable/3003636&lt;/a&gt;. &lt;a href=&quot;#fnref:kreps-jstor&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fradkin-demand-arxiv&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Fradkin, Andrey. &lt;em&gt;“Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming”&lt;/em&gt;. arXiv preprint (Apr 21, 2025). &lt;a href=&quot;https://arxiv.org/abs/2504.15440&quot;&gt;https://arxiv.org/abs/2504.15440&lt;/a&gt;. &lt;a href=&quot;#fnref:fradkin-demand-arxiv&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:openrouter-state&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Aubakirova, Malika; Attalah, Alex; Clark, Chris; Summerville, Justin; Anjney, Midha. &lt;em&gt;“State of AI”&lt;/em&gt;. OpenRouter (Dec 4, 2025). &lt;a href=&quot;https://openrouter.ai/state-of-ai&quot;&gt;https://openrouter.ai/state-of-ai&lt;/a&gt;. &lt;a href=&quot;#fnref:openrouter-state&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="economics" />
      
        <category term="artificial-intelligence" />
      

      

      
        <summary type="html">The world of artificial intelligence has often always been narrated through the lens of software, hardware, model development, and new products. Only lately, I’ve been noticing the breadth of articles related to the economics of all the infrastructure development and model offerings, especially in the LLM space. After all, modern AI is produced and provided using scarce resources like compute and memory running within cooled and power-constrained facilities. So, it only makes sense that the economics of AI and the sensational financing will begin to attract more attention. I’ve recently been reading a lot about productionized AI, and I’ve realized that the “supply of AI” is not really something that can be determined in immediate response to demand simply because there’s all this infrastructure that needs to be built, financed, installed, etc. 1 Then only can “allocations” in the economic sense be made. I think this matters mostly because the features of the market that are most visible to consumers, like rate limits, priority tiers, reserved throughput, pre-sales on compute resources, etc., are all key signatures of this market environment where capacity needs to be committed prior to the measurement of industry-wide demand. 2 3 4 5 This concept stuck out to me, especially as I was taking my intermediate microeconomics course this semester. It’s not really a case that we studied in class, but I decided to take a crack at modeling the phenomenon to see what I would find. The hand-wavy anecdote can be boiled down into a relatively clear microeconomic dilemma: via Bertrand competition, if several firms sell an identical service and marginal costs are low, then the price competition should be intense. But, it appears that AI services price and allocate as if scarcity is the organizing principle of the market environment and not that there happens to be scarce inputs that are factors of production. 6 7 See, if the latter were the case, LLMs should have a relatively simple, per-unit pricing scheme where prices would be cut until they hit the marginal cost. But in the current setting, we see firms compete by committing more and more capital upstream and rationing resources downstream. What separates AI from the general software product intuition is that, instead of cloud-like elasticity where a rise in demand increases capacity consumption and drives the marginal token cost down, the market is characterized by a race for upstream capital commitment costs and allocation that put price rules in the backseat. The fact that there are so many market pricing features indicates just that– the capacity constraint is so binding that firms cannot and will not shift away from their complicated pricing structures. And the massive timespan of AI financing only means that these pricing features are likely here to stay. In this post, I attempt to wrap these ideas into a model. In the setting I establish, I’ll show that when firms choose capacity first and then compete in prices, the price competition does not necessarily drive prices to marginal cost. In fact, in a two-stage game, the equilibrium features markups even though API-calling as a service appears to be homogenous AND Bertrand pricing is used in the second stage of the set-up. In the model, I’ll treat one good as a standardized unit of a service that is compliant with a fixed service-level agreement. In the case of AI and LLMs, this is simply a standardized unit of inference (output token or bundle of tokens) that is produced by a fixed model within a specific reliability target. Demand and Costs Let market demand for a general AI inference tool at price \(p\) be linear and take the form: \[D(p) = A - p\] such that \(A &amp;gt; 0\), and so that the inverse demand is given as: \[P(Q) = A - Q\] Let the marginal service cost of the tool be constant \(c \in [0, A)\). This can be thought of the cost of serving one more unit, conditional on a given model architecture or service-level agreement. Inference is often memory-constrained and KV-cache-bandwidth constrained. So the marginal cost depends on utilization and latency targets, and \(c\) is a stylized approximation, holding the service tier fixed. 8 Now, it is important to realize that markups generally result from some form of market power. So, the best place to start is the monopoly setting. With a downward-sloping demand the monopolist’s marginal revenue lies below demand, and so the profit condition that equates marginal revenue and marginal cost (\(MR = MC\)) implies price is greater than marginal cost (\(P &amp;gt; MC\)). I re-derive this below to make the point. Monopoly Marginal Revenue Let a single decision maker face (as monopolies are price-setters) an inverse demand \(P(q)\) with \(P&apos;(q)&amp;lt;0\). Revenue is \[R(q)=P(q)\cdot q\] Differentiate using the product rule as follows: \[R&apos;(q)=\frac{d}{dq}\big(P(q)q\big)=P&apos;(q)\,q+P(q)\] Since \(P&apos;(q)&amp;lt;0\) and \(q&amp;gt;0\), we have \[P&apos;(q)\,q&amp;lt;0 \Longrightarrow R&apos;(q)=P&apos;(q)\,q+P(q)&amp;lt;P(q)\] Profit is \(\pi(q)=R(q)-cq\). The first-order condition (FOC) is then \[\pi&apos;(q)=R&apos;(q)-c=0 \implies R&apos;(q)=c\] Combine \(R&apos;(q)=c\) with \(R&apos;(q)&amp;lt;P(q)\) to get \[c=R&apos;(q)&amp;lt;P(q) \Longrightarrow P(q)&amp;gt;c\] So it can be seen that the monopoly markup \(P(q)-c\) simply arises from the inequality \(MR&amp;lt;P\) created by the downward-sloping demand. Then, we can build a benchmark using the linear demand defined above. Monopoly with linear demand If \(P(q)=A-q\), then \[R(q)=(A-q)q=Aq-q^2\] so \(R&apos;(q)=A-2q\). Set \(R&apos;(q)=c\): \[\begin{aligned} A-2q&amp;amp;=c \\ 2q&amp;amp;=A-c \\ q^M&amp;amp;=\frac{A-c}{2} \end{aligned}\] Then \[p^M=P(q^M)=A-q^M=A-\frac{A-c}{2}=\frac{A+c}{2}\] and the markup is \(p^M-c=\frac{A-c}{2}\). Oligopoly Benchmarks Now, we move to the oligopolistic setting. First, consider Bertrand competition Two firms choose prices \((p_1,p_2)\) simultaneously, and the lower-priced firm meets the demand, any ties split the demand, and each has marginal cost \(c\). Restrict to \(p_i\ge c\). Standard from theory, \((p_1,p_2)=(c,c)\) is a Nash equilibrium. This can be seen as follows: If both set \(c\), each earns zero profit. Deviate upward: if firm 1 sets \(p_1&amp;gt;c\) while firm 2 stays at \(c\), firm 1 sells zero and earns zero, and there is no improvement. Deviate downward: if firm 1 sets \(p_1&amp;lt;c\) it wins the market but earns negative returns at the margin, so profit is negative. Thus no deviation strictly improves profit. Next, from theory, no Nash equilibrium has both prices above \(c\). If \(p_1&amp;lt;p_2\) with both \(&amp;gt;c\), firm 2 can undercut slightly (still above \(c\)) and capture the market, strictly improving profit. If \(p_1=p_2&amp;gt;c\), either firm can undercut by a small amount \(\varepsilon\) and capture the market, strictly improving profit. So under a homogeneous good within the Betrand model, price is driven down to marginal cost. Cournot competition Firms compete on quantity in this set-up. Firms choose quantities \(q_1,q_2\), total \(Q=q_1+q_2\), price \(P(Q)=A-Q\). The profit is then: \(\pi_i(q_1,q_2)=\big(A-(q_1+q_2)\big)q_i-cq_i\) Fix \(q_{-i}\). Then \[\begin{aligned} \pi_i(q_i) &amp;amp;=\big(A-q_i-q_{-i}\big)q_i-cq_i \\ &amp;amp;=(A-q_{-i})q_i-q_i^2-cq_i \\ &amp;amp;=(A-c-q_{-i})q_i-q_i^2 \end{aligned}\] Differentiate: \[\frac{d\pi_i}{dq_i}=(A-c-q_{-i})-2q_i\] The FOC is: \[(A-c-q_{-i})-2q_i=0 \implies q_i=\frac{A-c-q_{-i}}{2}\] In symmetric equilibrium \(q_1=q_2=q\): \[\begin{aligned} q&amp;amp;=\frac{A-c-q}{2} \\ 2q&amp;amp;=A-c-q \\ 3q&amp;amp;=A-c \\ q^C&amp;amp;=\frac{A-c}{3} \end{aligned}\] Then \[\begin{aligned} Q^C&amp;amp;=2q^C=\frac{2(A-c)}{3}\\ p^C&amp;amp;=A-Q^C=\frac{A+2c}{3} \end{aligned}\] and \[p^C-c=\frac{A-c}{3}\] And we find that \(p^C-c=q^C\). Note that the inverse demand curve in this setting is very similar to the monopoly demand curve. Fix \(q_{-i}\), and see that, firm \(i\) then faces inverse demand \(P(q_i+q_{-i})=(A-q_{-i})-q_i\) which is literally a monopoly demand curve with a shifted intercept. The Capacity Game Now, simply adjust the timing of events where the capacity selection comes first. Note that capacity can be thought of concretely as “tokens per second that can be generated or consumed,” and its binding constraints are generally due to GPU compute availability and memory. Stage 1: Choosing capacity. Firm \(i\in\{1,2\}\) chooses a capacity \(k_i\ge 0\) (“how many service units per period can be reliably served?”). Capacity cost is convex in the form: \[I(k_i)=\frac{\gamma}{2}k_i^2\] with \(\gamma\ge 0\). Interpret \(\gamma\) as paramter that controls difficulty in scaling that can be attributed to power and space constraints, bottlenecks in memory, frictions in deployment, and the reality that capacity cannot be conjured instantly.9 Stage 2: Posting price. Firms then post prices \(p_i\ge 0\). Consumers buy from the lower-priced firm first, but each firm can then serve at most its capacity. Any unserved demand spills over to the other firm. So, let \(K=k_1+k_2\) and define the market-clearing price \[p^*(K)=A-K\] so that \[D(p^*(K))=A-p^*(K)=K\] Pricing subgame lemma given capacities We need one basic argument: the pricing subgame has a pure-strategy equilibrium at the market-clearing price. Lemma (pricing equilibrium with binding total capacity). Fix \(k_1,k_2\). Suppose \(\text{(i) }p^*(K)\ge c\) and \(\text{(ii) }k_i\le p^*(K)-c\) for \(i=1, 2\). Then the stage 2 pricing subgame has a Nash equilibrium at \((p_1,p_2)=(p^*(K),p^*(K))\) in which each firm sells its full capacity \(q_i=k_i\). Proof. Assume that both of the firms set \(p^*(K)\). Then the total demand equals total capacity, so the market clears exactly, and each firm sells \(q_i=k_i\) under a tie-breaking rule. Without loss of generality, consider deviations by firm 1. (a) Undercut: set \(p_1=p^*(K)-\varepsilon\) with \(\varepsilon&amp;gt;0\). Then firm 1 is cheapest, so it receives first priority in demand. But it cannot sell more than \(k_1\). At price \(p_1\), demand is \[D(p_1)=A-p_1=A-(p^*(K)-\varepsilon)=K+\varepsilon&amp;gt;K\] so there is more than enough demand to fill capacity. So firm 1 still sells \(q_1=k_1\). Its operating profit is then \[(p_1-c)k_1=(p^*(K)-\varepsilon-c)k_1=(p^*(K)-c)k_1-\varepsilon k_1\] which is strictly lower than \((p^*(K)-c)k_1\). (b) Raise price: set \(p_1=p^*(K)+x\) with \(x&amp;gt;0\). Then firm 2 sells first up to capacity \(k_2\), and firm 1 sells the residual demand. Total demand at \(p_1\) is \(A-p_1\), so residual demand after firm 2 serves \(k_2\) is \[(A-p_1)-k_2\] Substitute \(p_1=p^*(K)+x\) and \(p^*(K)=A-K\): \[\begin{aligned} (A-(p^*(K)+x))-k_2 &amp;amp;=A-(A-K+x)-k_2 \\ &amp;amp;=K-x-k_2 \\ &amp;amp;=k_1-x \end{aligned}\] Thus for \(0\le x\le k_1\), firm 1 sells \(q_1=k_1-x\) and earns \[\pi_1(x)=(p^*(K)+x-c)(k_1-x)\] Differentiate to get: \[\begin{aligned} \pi_1&apos;(x) &amp;amp;=\frac{d}{dx}\big((p^*(K)+x-c)(k_1-x)\big) \\ &amp;amp;=(k_1-x)\cdot (1) + (p^*(K)+x-c)\cdot(-1) \\ &amp;amp;=k_1-x-p^*(K)-x+c \\ &amp;amp;=k_1-(p^*(K)-c)-2x \end{aligned}\] At \(x=0\), \(\pi_1&apos;(0)=k_1-(p^*(K)-c)\) So a small price increase is unprofitable whenever \(k_1\le p^*(K)-c\), which is condition (ii). As the above holds in generality, it holds for firm 2 as well. Thus neither undercutting nor raising price is profitable. \((p^*(K),p^*(K))\) is a Nash equilibrium. ∎ Note that in the equilibria that will be computed next, the total capacity satisfies \(K\le A-c\) and each firm’s capacity is not too large relative to the margin, so the lemma’s conditions should hold. Intuitively, if a modest capacity is built relative to demand, there does not need to be a discount to fill the gap. Stage 1 is a capacity choice in the Cournot style Because the price subgame clears at \(p=A-(k_1+k_2)\) and each firm sells its capacity, stage 1 effectively becomes: \[\max_{k_i\ge 0}\ \Pi_i(k_i,k_{-i}) =\big(A-(k_i+k_{-i})-c\big)k_i-\frac{\gamma}{2}k_i^2\] Expand it and get: \[\begin{aligned} \Pi_i &amp;amp;=(A-c-(k_i+k_{-i}))k_i-\frac{\gamma}{2}k_i^2 \\ &amp;amp;=(A-c)k_i-k_i^2-k_{-i}k_i-\frac{\gamma}{2}k_i^2 \\ &amp;amp;=(A-c)k_i-\Big(1+\frac{\gamma}{2}\Big)k_i^2-k_{-i}k_i \end{aligned}\] Differentiate with respect to \(k_i\): \[\begin{aligned} \frac{\partial \Pi_i}{\partial k_i} &amp;amp;=(A-c)-2\Big(1+\frac{\gamma}{2}\Big)k_i-k_{-i} \\ &amp;amp;=(A-c)-(2+\gamma)k_i-k_{-i} \end{aligned}\] The FOC is: \[(A-c)-(2+\gamma)k_i-k_{-i}=0 \implies k_i=\frac{A-c-k_{-i}}{2+\gamma}\] In a symmetric equilibrium \(k_1=k_2=k\): \[\begin{aligned} k&amp;amp;=\frac{A-c-k}{2+\gamma} \\ (2+\gamma)k&amp;amp;=A-c-k \\ (3+\gamma)k&amp;amp;=A-c \\ k^{NE}&amp;amp;=\frac{A-c}{3+\gamma} \end{aligned}\] So total capacity is \[K^{NE}=2k^{NE}=\frac{2(A-c)}{3+\gamma}\] and the induced market-clearing price is \[\begin{aligned} p^{NE} &amp;amp;=A-K^{NE} \\ &amp;amp;=A-\frac{2(A-c)}{3+\gamma} \\ &amp;amp;=\frac{A(3+\gamma)-2A+2c}{3+\gamma} \\ &amp;amp;=\frac{A(1+\gamma)+2c}{3+\gamma} \end{aligned}\] The markup is then: \[\begin{aligned} p^{NE}-c &amp;amp;=\frac{A(1+\gamma)+2c}{3+\gamma}-c \\ &amp;amp;=\frac{A(1+\gamma)+2c-c(3+\gamma)}{3+\gamma} \\ &amp;amp;=\frac{(1+\gamma)(A-c)}{3+\gamma} \end{aligned}\] The Cournot shadow If capacity is cheap to scale (set \(\gamma=0\)), then \[k^{NE}=\frac{A-c}{3}, \quad p^{NE}=\frac{A+2c}{3}\] which is exactly the Cournot duopoly outcome with \(k\) playing the role of \(q\). At first glance this can seem paradoxical. Stage 2 is Bertrand pricing, so why doesn’t the Bertrand case force \(p=c\)? The reason is that the Bertrand move to “undercut-by-\(\varepsilon\) and take the whole market” only works if the lower price firm can actually serve the whole market. Once quantities are committed beforehand (here this is the capacity), undercutting does not let you expand quantity beyond your capacity. Price competition is still there, but it no longer determines the output. Instead it mainly allocates who serves first, up to their prechosen capacities. This observation is formalized in a classic paper by Kreps and Scheinkman (1983). 10 They study a two-stage game in which firms first choose capacities (or quantities) and then compete in prices with capacity constraints. Under standard rationing assumptions, the equilibrium outcome matches with the Cournot quantity outcome in that the strategic action “moves upstream” into the capacity choice, and the second-stage price competition ends up clearing the market at a price that is consistent with total committed capacity. Welfare Under welfare theorems, competitive equilibrium is found to be efficient under price-taking. Our equilibrium is not price-taking, so the theorems should not apply. Then how should efficiency be benchmarked? I choose to compare equilibrium total capacity to the surplus-maximizing total capacity. Planner’s problem (case with two firms) Total gross benefit from serving quantity \(Q\) is the area under inverse demand: \[\int_0^Q P(q)\,dq=\int_0^Q (A-q)\,dq=AQ-\frac{Q^2}{2}\] Serving \(Q\) costs \(cQ\). First, recognize that capacity costs can be minimized by splitting \(k_1=k_2=K/2\)See proof. Proof of $$k_1 = k_2 = K/2$$ Fix total capacity \(K = k_1 + k_2\) with \(k_1,k_2 \ge 0\). Capacity cost is \(I(k)=\tfrac{\gamma}{2}k^2\) with \(\gamma\ge 0\). Impose the constraint by writing \(k_2 = K - k_1\). Define \[ C(k_1)=I(k_1)+I(K-k_1)=\tfrac{\gamma}{2}\big(k_1^2+(K-k_1)^2\big) \] Expand: \[ C(k_1)=\tfrac{\gamma}{2}\big(2k_1^2-2Kk_1+K^2\big) \] differentiate: \[ C&apos;(k_1)=\gamma(2k_1-K) \] set \(C&apos;(k_1)=0\Rightarrow k_1=K/2\), and \(C&apos;&apos;(k_1)=2\gamma\ge 0\), so \(k_1=K/2\) is a global minimizer. Thus \(k_2=K/2\). Plugging in, \[ I(K/2)+I(K/2)=\gamma\frac{K^2}{4} \] \[\frac{\gamma}{2}\Big(\frac{K}{2}\Big)^2+\frac{\gamma}{2}\Big(\frac{K}{2}\Big)^2 =\gamma\frac{K^2}{4}\] The planner never builds unused capacity in this toy model, so set \(Q=K\). \[\begin{aligned} W(K) &amp;amp;=\Big(AK-\frac{K^2}{2}\Big)-cK-\gamma\frac{K^2}{4} \\ &amp;amp;=(A-c)K-\Big(\frac12+\frac{\gamma}{4}\Big)K^2 \end{aligned}\] Differentiate: \[\begin{aligned} W&apos;(K) &amp;amp;=(A-c)-2\Big(\frac12+\frac{\gamma}{4}\Big)K \\ &amp;amp;=(A-c)-\Big(1+\frac{\gamma}{2}\Big)K \end{aligned}\] Set \(W&apos;(K)=0\): \[\begin{aligned} (A-c)-\Big(1+\frac{\gamma}{2}\Big)K&amp;amp;=0 \\ K^*&amp;amp;=\frac{A-c}{1+\gamma/2}=\frac{2(A-c)}{2+\gamma} \end{aligned}\] Market equilibrium total capacity determined earlier is \[K^{NE}=\frac{2(A-c)}{3+\gamma}\] Take the ratio: \[\begin{aligned} \frac{K^{NE}}{K^*} &amp;amp;=\frac{\frac{2(A-c)}{3+\gamma}}{\frac{2(A-c)}{2+\gamma}} \\ &amp;amp;=\frac{2+\gamma}{3+\gamma} \\ &amp;amp;&amp;lt;1 \end{aligned}\] So we get the following statement describing the: \[\frac{K^{NE}}{K^*}=\frac{2+\gamma}{3+\gamma}&amp;lt;1\] So, it is seen that strategic capacity choice underprovides capacity relative to the surplus-maximizing benchmark by the factor shown above. A strong welfare identity Because the welfare function is concave and quadratic, the welfare loss can be computed in closed form in addition to the capacity loss. First, write \[W(K)=(A-c)K-aK^2\] with \(a=\frac12+\frac{\gamma}{4}=\frac{2+\gamma}{4}\). The maximizer is \(K^*=\frac{A-c}{2a}\) and the maximum value is \[W^*=\frac{(A-c)^2}{4a}=\frac{(A-c)^2}{2+\gamma}\] Now plug \(K^{NE}=\frac{2(A-c)}{3+\gamma}\) into \(W(K)\): \[\begin{aligned} W^{NE} &amp;amp;=(A-c)\frac{2(A-c)}{3+\gamma} -a\Big(\frac{2(A-c)}{3+\gamma}\Big)^2 \\ &amp;amp;=\frac{2(A-c)^2}{3+\gamma}-\frac{2+\gamma}{4}\cdot \frac{4(A-c)^2}{(3+\gamma)^2} \\ &amp;amp;=\frac{2(A-c)^2}{3+\gamma}-\frac{(2+\gamma)(A-c)^2}{(3+\gamma)^2} \\ &amp;amp;=\frac{(A-c)^2}{(3+\gamma)^2}\Big(2(3+\gamma)-(2+\gamma)\Big) \\ &amp;amp;=\frac{(A-c)^2}{(3+\gamma)^2}(4+\gamma) \end{aligned}\] So \[\frac{W^{NE}}{W^*} = \frac{\frac{(A-c)^2(4+\gamma)}{(3+\gamma)^2}}{\frac{(A-c)^2}{2+\gamma}} = \frac{(2+\gamma)(4+\gamma)}{(3+\gamma)^2} = 1-\frac{1}{(3+\gamma)^2}\] Thus: \[\begin{aligned} \frac{W^{NE}}{W^*}&amp;amp;=1-\frac{1}{(3+\gamma)^2} \\ W^*-W^{NE}&amp;amp;=\frac{(A-c)^2}{(2+\gamma)(3+\gamma)^2} \end{aligned}\] This is slightly counterintuitive– the fractional welfare loss decreases with the factor \(1/(3+\gamma)^2\), even though the prices can remain above marginal cost. Peak-load Demand Everything so far has assumed that there is a fixed demand intercept \(A\). In reality, AI demand is not constant. It has peaks. The easiest way to incorporate that is to let the demand intercept be random after the capacity is chosen. So, let the period demand curve be \[\tilde D(p)=\tilde A-p\] so inverse demand is \[\tilde P(Q)=\tilde A-Q\] Capacities \(k_1,k_2\) are selected first, with total capacity \(K=k_1+k_2\). Then \(\tilde A\) is realized and the same capacity-constrained pricing game is played. The point here is not to simply add noise to the set-up. The goal is to isolate the economic impacts of the value of capacity in peak demand periods. Spot price with capacity We can fix the total capacity \(K\) and a market realization \(\tilde A\). The market-clearing price that equates demand to total capacity is \(\tilde A-K\). But if demand is low enough, then the capacity is slack, and under vanilla Bertrand competition, this would push the spot usage price down to the marginal cost. There is a minor technical subtlety here. When total capacity is slack, a single firm may still have local market power if its rival cannot serve the whole residual demand by itself (we’ve noted something similar above). In production AI markets, that kind of power is often self-regulated by substitution and contestability in that users have flexibility in model/provider selection. 11 12 I capture this in a simple way– when capacity is slack, the spot usage price is capped at marginal cost \(c\). With that in mind, the spot price in the stage 2 equilibrium can be written in one line. Proposition (spot price with capacity). In the stage 2 equilibrium, \[\tilde p(\tilde A,K)=\max\{c,\ \tilde A-K\}\] and therefore the scarcity premium is \[\tilde p(\tilde A,K)-c=(\tilde A-c-K)^+\] where \((x)^+=\max\{x,0\}\). Proof. If \(\tilde A-K&amp;lt;c\), then at price \(p=c\) demand is \(\tilde A-c&amp;lt;K\), so the capacity is slack and the spot price is \(c\). If \(\tilde A-K\ge c\), then the market-clearing price \(\tilde A-K\) is at least the marginal cost. In that case, the same argument as in the deterministic pricing lemma can be applied– undercutting cannot increase the quantity beyond capacity, and so it only reduces margin. And raising price loses residual demand. So, the outcome price is \(\tilde A-K\). Combining the two cases produces the formula above. ∎ Now define \(X=\tilde A-c\). Then the scarcity premium is \[\tilde p-c=(X-K)^+\] This expression is the unit that I use to think about congestion. It is basically the premium that is realized only when demand is above what the system can serve at the service level. Finally, define the expected scarcity rent at total capacity \(K\) by \[\Lambda(K)=\mathbb E\big[(X-K)^+\big]\] This is the value that priority tiers and guaranteed throughput try to capture– the expected congestion rent per unit. Two identities for \(\Lambda(K)\) Assume \(X\) has CDF \(F\) and finite mean. Lemma (tail-integral formula). \[\Lambda(K)=\int_K^\infty (1-F(t))\,dt\] Proof. Start from \[\Lambda(K)=\int_{-\infty}^{\infty} (x-K)^+\,dF(x)=\int_K^\infty (x-K)\,dF(x)\] Write \(x-K=\int_K^x 1\,ds\) and swap the integrals: \[\begin{aligned} \Lambda(K) &amp;amp;=\int_K^\infty\left(\int_K^x 1\,ds\right)dF(x) \\ &amp;amp;=\int_K^\infty\left(\int_s^\infty dF(x)\right)ds \\ &amp;amp;=\int_K^\infty (1-F(s))\,ds \end{aligned}\] Rename \(s\) to \(t\). ∎ Corollary (marginal value equals congestion probability). If \(F\) is continuous at \(K\), then \[\Lambda&apos;(K)=-(1-F(K))=-\Pr(X&amp;gt;K)\] Efficient Capacity under Peak-load Demand I now redo the welfare calculation from above in this new stochastic environment. The setting remains the same in that inverse demand is linear in each period, marginal service cost is constant, and the capacity cost is convex. As before, a planner chooses total capacity \(K\) and splits it symmetrically across the two identical technologies. Total capacity cost is then minimized by the split, producing \(\gamma\frac{K^2}{4}\). In a given state \(\tilde A\), serving quantity \(Q\) generates the total benefit \[\int_0^Q (\tilde A-q)\,dq=\tilde A Q-\frac{Q^2}{2}\] Net of service cost \(cQ\), the net benefit per state is: \[(\tilde A-c)Q-\frac{Q^2}{2}=XQ-\frac{Q^2}{2}\] Given the capacity \(K\), the planner will not serve more than \(K\). Also, in each state, the unconstrained efficient quantity solves \(\tilde A-Q=c\), so \(Q=X\). Thus the served quantity is \(Q(K,X)=\min\{K,X\}\). Expected welfare is therefore \[W(K)=\mathbb E\!\left[X\min\{K,X\}-\frac{1}{2}\min\{K,X\}^2\right]-\gamma\frac{K^2}{4}\] Rather than expanding this piecewise, it is easier to calculate the marginal condition directly. Increase capacity from \(K\) to \(K+dK\). In states where \(X\le K\), capacity is slack and the extra unit is unused, so the marginal welfare gain would be zero. In states where \(X&amp;gt;K\), the capacity is binding at \(Q=K\), and the marginal welfare gain from an extra unit is the marginal net benefit at \(Q=K\): \[\frac{\partial}{\partial Q}\left(XQ-\frac{Q^2}{2}\right)\Bigg|_{Q=K} = X-K\] So the expected marginal benefit of capacity is exactly \(\mathbb E[(X-K)^+]=\Lambda(K)\). The marginal cost of capacity is: \[\frac{d}{dK}\left(\gamma\frac{K^2}{4}\right)=\gamma\frac{K}{2}\] Therefore the efficient capacity satisfies the FOC: \[\Lambda(K^*)=\gamma\frac{K^*}{2}\] This houses the deterministic result from before. If \(X=A-c\) is constant, then \(\Lambda(K)=(A-c-K)^+\) and \[A-c-K^*=\gamma\frac{K^*}{2} \implies K^*=\frac{2(A-c)}{2+\gamma}\] So under uncertainty, the efficient rule is still simple. Build capacity until expected congestion rent is equal to the marginal cost of capacity. Oligopoly Capacity under Peak-load Demand Now repeat the capacity game for two firms under the same uncertainty. In the stage 2 equilibrium, the scarcity premium in state \(X\) is \((X-K)^+\). When that premium is positive, the system is capacity constrained and each firm sells approximately its full capacity. In that case, firm \(i\)’s operating profit in state \(X\) is \((X-K)^+\,k_i\). Taking expectations and subtracting capacity cost gives the stage 1 expected payoff \[\Pi_i(k_i,k_{-i})=k_i\,\Lambda(K)-\frac{\gamma}{2}k_i^2\] and \(K=k_i+k_{-i}\). Differentiate with respect to \(k_i\), using \(\partial K/\partial k_i=1\): \[\frac{\partial \Pi_i}{\partial k_i} = \Lambda(K)+k_i\Lambda&apos;(K)-\gamma k_i\] A best response satisfies \(\Lambda(K)+k_i\Lambda&apos;(K)=\gamma k_i\). Apply the corollary from before, \(\Lambda&apos;(K)=-\Pr(X&amp;gt;K)\): \[\Lambda(K)-k_i\,\Pr(X&amp;gt;K)=\gamma k_i \implies \Lambda(K)=\big(\gamma+\Pr(X&amp;gt;K)\big)\,k_i\] In a symmetric equilibrium \(k_1=k_2=k\), so \(K=2k\) and \[\Lambda(2k)=\big(\gamma+\Pr(X&amp;gt;2k)\big)\,k\] This is basically the stochastic analog of the Cournot shadow from before. It isolates a term that does not appear in the planner’s condition: \[\Lambda(K^*)=\gamma\frac{K^*}{2} \quad\text{vs.}\quad \Lambda(K^{NE})=\gamma\frac{K^{NE}}{2}+\frac{K^{NE}}{2}\Pr(X&amp;gt;K^{NE})\] So the entire gap between private and efficient capacity comes from the additional term \(\frac{K}{2}\Pr(X&amp;gt;K)\). This is the strategic scarcity incentive. Each firm knows that adding capacity reduces the scarcity rent that it can earn in all peak states, and the strength of that effect is proportional to how often the peaks occur. Theorem (strategic underprovision) Assume that \(\Lambda(K)\) is strictly decreasing and continuous. Then the equilibrium total capacity \(K^{NE}\) is strictly below the efficient capacity \(K^*\). Proof. The planner’s condition is \(\Lambda(K^*)=\gamma\frac{K^*}{2}\). The equilibrium condition is \[\Lambda(K^{NE})=\gamma\frac{K^{NE}}{2}+\frac{K^{NE}}{2}\Pr(X&amp;gt;K^{NE})\] Whenever the capacity binds with positive probability, \(\Pr(X&amp;gt;K^{NE})&amp;gt;0\), so \[\Lambda(K^{NE})&amp;gt;\gamma\frac{K^{NE}}{2}\] Since \(\Lambda(K)\) is strictly decreasing while \(\gamma K/2\) is strictly increasing, the intersection of these curves is \(K^*\). The inequality therefore implies that \(K^{NE}&amp;lt;K^*\). ∎ This can be seen as a stronger inefficiency claim than the deterministic ratio that we found above. Here, the gap is tied to the congestion probability \(\Pr(X&amp;gt;K)\), which can be easily measured from past occurrences. A Closed-form Example Consider a simple but realistic example. Take a simple distribution for \(X\) that captures “how spiky the demand is.” Assume \(X\sim\mathrm{Unif}[0,M]\) with \(M&amp;gt;0\). Then for \(0\le K\le M\), \[\begin{aligned} \Pr(X&amp;gt;K)&amp;amp;=\frac{M-K}{M} \\ \Lambda(K)&amp;amp;=\mathbb E[(X-K)^+]=\frac{(M-K)^2}{2M} \end{aligned}\] Efficient capacity The planner’s condition \(\Lambda(K^*)=\gamma K^*/2\) becomes \[\frac{(M-K^*)^2}{2M}=\gamma\frac{K^*}{2} \implies (M-K^*)^2=\gamma M K^*\] The solution in \([0,M]\) is then \[K^*=\frac{M}{2}\Big(2+\gamma-\sqrt{\gamma^2+4\gamma}\Big)\] Symmetric oligopoly capacity The equilibrium condition is \[\Lambda(K^{NE})=\gamma\frac{K^{NE}}{2}+\frac{K^{NE}}{2}\Pr(X&amp;gt;K^{NE})\] Substitute the closed forms: \[\frac{(M-K)^2}{2M} = \gamma\frac{K}{2} +\frac{K}{2}\cdot\frac{M-K}{M}\] Multiply by \(2M\) and rearrange to get: \[M^2-(\gamma+3)MK+2K^2=0\] The equilibrium total capacity is therefore \[K^{NE} =\frac{M}{4}\Big((\gamma+3)-\sqrt{(\gamma+3)^2-8}\Big)\] The comparison \(K^{NE}&amp;lt;K^*\) holds here as well, and you can see that the economic reason is there in the math. The oligopoly condition contains an extra peak-probability term, which pushes the capacity downward. References Reuters, “AI boom drives data-center dealmaking to record high, says report”. Reuters (Dec 19, 2025) https://www.reuters.com/business/ai-boom-drives-data-center-dealmaking-record-high-says-report-2025-12-19/. &amp;#8617; Anthropic (Claude Docs), “Rate limits”. https://docs.anthropic.com/en/api/rate-limits. &amp;#8617; Google AI for Developers, “Rate limits, Gemini API”. https://ai.google.dev/gemini-api/docs/rate-limits. &amp;#8617; OpenAI, “Priority Processing for API Customers”. https://openai.com/api-priority-processing/. &amp;#8617; OpenAI, “Scale Tier for API Customers”. https://openai.com/api-scale-tier/. &amp;#8617; Bergemann, Dirk; Bonatti, Alessandro; Smolin, Alex. “The Economics of Large Language Models: Token Allocation, Fine-Tuning, and Optimal Pricing”. EC ‘25: Proceedings of the 26th ACM Conference on Economics and Computation (Jul 2, 2025). https://dl.acm.org/doi/10.1145/3736252.3742625. &amp;#8617; Zhong, Weijie. “Token is All You Price”. arXiv prepring (Oct 10, 2025). https://arxiv.org/abs/2510.09859. &amp;#8617; Erdil, Ege. “Inference economics of language models”. arXiv preprint (Jun 5, 2025). https://arxiv.org/abs/2506.04645. &amp;#8617; Jin, Hyunjoo; Potkin, Fanny; Lee, Wen-Yee; Bridge, Anton; Cherney, Max. “The AI frenzy is driving a new global supply chain crisis” Reuters (Dec 3, 2025). https://www.reuters.com/world/china/ai-frenzy-is-driving-new-global-supply-chain-crisis-2025-12-03/. &amp;#8617; Kreps, David; Scheinkman, Jose. “Quantity Precommitment and Bertrand Competition Yield Cournot Outcomes”. The Bell Journal of Economics (Autumn 1983). https://www.jstor.org/stable/3003636. &amp;#8617; Fradkin, Andrey. “Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming”. arXiv preprint (Apr 21, 2025). https://arxiv.org/abs/2504.15440. &amp;#8617; Aubakirova, Malika; Attalah, Alex; Clark, Chris; Summerville, Justin; Anjney, Midha. “State of AI”. OpenRouter (Dec 4, 2025). https://openrouter.ai/state-of-ai. &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Catastrophe Bond Pricing and Risk</title>
      
      
      <link href="https://a-sircar1.github.io/2025/07/14/25-Cat-Bonds/" rel="alternate" type="text/html" title="Catastrophe Bond Pricing and Risk" />
      
      <published>2025-07-14T06:10:56+00:00</published>
      <updated>2025-07-14T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2025/07/14/25-Cat-Bonds</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2025/07/14/25-Cat-Bonds/">&lt;p&gt;This summer, I’ve gotten the chance to dive deep into the economics and finance of insurance. While I’ve mainly been digging deep into the wave of insurer collapses in Louisiana and the state’s unprecedented and subsequent access of the debt markets, I stumbled upon the catastrophe bond market.&lt;/p&gt;

&lt;p&gt;In credit markets, spreads are often representative of probabilities of default, default intensity, and assumptions on the recovery of the debt. For catastrophe bonds (or cat bonds), the idea remains, but the same abstraction has such a different flavor that I decided to think through it a little more carefully. I detail my findings in this post.&lt;/p&gt;

&lt;p&gt;For cat bonds, the quoted spread is really the market’s price for a defined portion of the losses associated with a particular catastrophe. You take the bond’s discount margin, compare it to the model’s expected loss for that tranche, and then ask the question: is the market paying enough for the risk in this layer? In other words, do you get paid a lot per unit of modeled loss, or is the compensation low? This post is all about making the translation between how a hurricane loss distribution becomes a tranche, and how that tranche can become a spread that can actually be traded.&lt;/p&gt;

&lt;details open=&quot;&quot; class=&quot;toc-wrap&quot;&gt;
  &lt;summary&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#recap&quot;&gt;A Recap on Cat Bonds&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#el-to-dm&quot;&gt;Expected Loss to Discount Margin&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#multiple&quot;&gt;The “Multiple” and the Implied Disaster Curve&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#structuring&quot;&gt;Structuring a Deal&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#example&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#going-forward&quot;&gt;2025 and Going Forward&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;Notes and References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;h2 id=&quot;recap&quot;&gt;A Recap on Cat Bonds&lt;/h2&gt;

&lt;p&gt;A cat bond is basically a principal-at-risk, floating rate note that is issued out of an SPV. Investor principal is placed in a collateral account (typically Treasuries/cash accounts), the coupons float off that collateral return, and then the principal can be written down if certain kinds of conditions are met. The key quoting convention here is the &lt;strong&gt;discount margin&lt;/strong&gt; (DM) which is the spread over the collateral yield. &lt;sup id=&quot;fnref:chicagofed&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:chicagofed&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;figure class=&quot;img-center&quot;&gt;
  &lt;img src=&quot;/assets/img/chicago_fed_catbond_structure.png&quot; alt=&quot;Cat bond structure diagram (Chicago Fed)&quot; /&gt;
  &lt;figcaption&gt;Catastrophe Bond Structure (Federal Reserve Bank of Chicago)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Please refer to the Chicago Fed’s primer article (linked in the references) for a much more detailed background and sketch. With the basics out of the way, we can move forward.&lt;/p&gt;

&lt;h2 id=&quot;el-to-dm&quot;&gt;Expected Loss to Discount Margin&lt;/h2&gt;

&lt;p&gt;Other than the reported coupon, there are two values that matter the most when it comes to the evaluation of cat bonds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expected loss (EL)&lt;/strong&gt;: This is the catastrophe risk modeler’s estimate of the average annual principal loss (or actuarial loss cost) fraction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discount margin (DM)&lt;/strong&gt;: This is what the bond pays you over the collateral for taking that risk.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the sake of intuition, if collateral earns \(r_t\), then the coupon is roughly:&lt;/p&gt;

\[C_t \approx r_t + \mathrm{DM}\]

&lt;p&gt;So DM is the “cat spread.” The market then roughly decomposes DM into the following:&lt;/p&gt;

\[\mathrm{DM} \approx \mathrm{EL} + \text{risk premium} + \text{frictions (liquidity, model risk, etc.)}\]

&lt;p&gt;For a single standardized valuation statistic, consider the &lt;em&gt;multiple&lt;/em&gt;:&lt;/p&gt;

\[M \equiv \frac{\mathrm{DM}}{\mathrm{EL}}\]

&lt;p&gt;It’s the cat bond analog of “spread per unit of default risk,” and it’s a convenient shorthand because cat bond cashflows are event-driven and the modeling inputs can be noisy.&lt;/p&gt;

&lt;h2 id=&quot;multiple&quot;&gt;The “Multiple” and the Implied Disaster Curve&lt;/h2&gt;

&lt;p&gt;This is where cat bonds offer an interesting mental model– an equivalence can be drawn between catastrophe insurance coverage and the priced tranche of a particular loss distribution. That is, &lt;em&gt;the term sheet can be treated as a tranche quote on a tail distrbution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;An easy way to see this is to start with a toy model that lets us do some back-of-the-envelope math: we consider maturity \(T\), a trigger condition that arises with risk-neutral intensity \(q\), and a principal loss fraction \(L\in[0,1]\) if it triggers.&lt;/p&gt;

&lt;p&gt;Then the annualized expected principal loss is approximately \(qL\), and the par spread here becomes the following:&lt;/p&gt;

\[\mathrm{DM}_{\text{par}} \approx qL\]

&lt;p&gt;i.e. we end up with &lt;strong&gt;spread ≈ hazard × loss-given-trigger&lt;/strong&gt; (the same mental model you use for a CDS premium when the “recovery” is fixed). So in practice, you can do two translations here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;From DM to an implied “market EL”&lt;/strong&gt;&lt;br /&gt;
If the bond has binary losses \((L \approx 1)\), then \(\mathrm{DM}\) is already a rough proxy for \(q\). If the losses are partial, you can scale by \(L\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;From modeled EL to a clearing DM through the multiple&lt;/strong&gt;&lt;br /&gt;
Rearranging it gives us \(\mathrm{DM} = M \cdot \mathrm{EL}\). The job is then to decide whether \(M\) is generous given the layer, the triggering condition, and the current market.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the perspective of hedge funds, this makes cat bonds really interesting. In this case, you’re not just “buying catastrophe risk,” but in fact, you’re essentially betting on a point on an implied tail curve– the bets in the market are on the shape of the curve.&lt;/p&gt;

&lt;h2 id=&quot;structuring&quot;&gt;Structuring a Deal&lt;/h2&gt;

&lt;p&gt;This is the structuring problem that sponsors and banks consider and “solve”:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Choose attachment and limit so the tranche has the expected loss investors will buy at an acceptable multiple.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let \(X\) be the sponsor loss (or the index loss) over one year, in dollars. A cat bond tranche can be thought of as a layer that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;attaches at \(A\): the bond starts taking losses only once \(X &amp;gt; A\)&lt;/li&gt;
  &lt;li&gt;exhausts at \(E\): once the losses hit \(E\), the bond pays out its maximum; the investors can’t lose more than the tranche size&lt;/li&gt;
  &lt;li&gt;limit \(K = E-A\): the maximum dollar amount that the tranche can pay out&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Its payout is:&lt;/p&gt;

\[\text{payout}(X)=\min\{\max(X-A,0),K\}\]

&lt;p&gt;A very useful identity is then (boiling down the math from Excel):&lt;/p&gt;

\[\mathbb{E}[\text{payout}] = \int_A^E \mathbb{P}(X&amp;gt;u)\,du\]

&lt;p&gt;So the annual expected loss fraction is:&lt;/p&gt;

\[\mathrm{EL} = \frac{\mathbb{E}[\text{payout}]}{K}\]

&lt;p&gt;It’s simple but that’s the connection from catastrophe modeling to the bond structuring:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The modeler gives a probability of excess curve that maps \(u\) to \(\mathbb{P}(X&amp;gt;u)\)&lt;/li&gt;
  &lt;li&gt;Integrate it over \([A,E]\) to get expected payout&lt;/li&gt;
  &lt;li&gt;Divide by \(K\) to get \(EL\)&lt;/li&gt;
  &lt;li&gt;Multiply by a clearing multiple \(M\) to get the spread that you print&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Discretization&lt;/h3&gt;

&lt;p&gt;If the model provides points \((u_i, p_i)\) on the exceedance probability curve rather than the actual functional form, then approximate the integral with trapezoids:&lt;/p&gt;

\[\int_A^E \mathbb{P}(X&amp;gt;u)\,du \approx \sum_i \frac{p_i+p_{i+1}}{2}\cdot (u_{i+1}-u_i)\]

&lt;p&gt;over grid points spanning \([A,E]\).&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;An Example&lt;/h3&gt;

&lt;p&gt;Suppose the risk modeler produces the following annual exceedance curve for a hurricane loss index \(X\) (loss thresholds in $ millions):&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Loss threshold \(u\) (USD mm)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;800&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;900&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;1000&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;1100&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;1200&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;1300&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;\(\mathbb{P}(X&amp;gt;u)\)&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6.0%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.8%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.8%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.0%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.4%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.9%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You’re deciding between two candidate tranches, each with $200mm limit (so \(K=200\)).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Candidate 1:&lt;/strong&gt; \(A=1000\), \(E=1200\) (\(K=200\))&lt;/p&gt;

&lt;p&gt;Approximate the expected payout using trapezoids with step size \(100\):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\([1000,1100]\): avg prob \(=\frac{3.8\%+3.0\%}{2}=3.4\%\) ⇒ contribution \(=0.034\times 100=3.4\) (mm)&lt;/li&gt;
  &lt;li&gt;\([1100,1200]\): avg prob \(=\frac{3.0\%+2.4\%}{2}=2.7\%\) ⇒ contribution \(=0.027\times 100=2.7\) (mm)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So \(\mathbb{E}[\text{payout}] \approx 3.4+2.7=6.1\) (mm), and:&lt;/p&gt;

\[\mathrm{EL}_1 = \frac{6.1}{200} = 3.05\% \text{ per year}\]

&lt;p&gt;If the market is clearing this style of risk at, say, &lt;strong&gt;\(M=1.8\)&lt;/strong&gt;, then the required discount margin is:&lt;/p&gt;

\[\mathrm{DM}_1 = M\cdot \mathrm{EL}_1 = 1.8\times 3.05\% = 5.49\% \approx 549\text{ bps}\]

&lt;p&gt;&lt;strong&gt;Candidate 2:&lt;/strong&gt; Raise the layer by $100mm: \(A=1100\), \(E=1300\) (still \(K=200\))&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\([1100,1200]\): contribution \(=2.7\) (mm) (same as above)&lt;/li&gt;
  &lt;li&gt;\([1200,1300]\): avg prob \(=\frac{2.4\%+1.9\%}{2}=2.15\%\) ⇒ contribution \(=0.0215\times 100=2.15\) (mm)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So \(\mathbb{E}[\text{payout}]\approx 4.85\) (mm), and:&lt;/p&gt;

\[\mathrm{EL}_2 = \frac{4.85}{200} = 2.425\%\]

\[\mathrm{DM}_2 = 1.8\times 2.425\% = 4.365\% \approx 437\text{ bps}\]

&lt;p&gt;&lt;em&gt;So, given the same disaster, same maturity, and same $200mm size, if the tranche moves up by $100mm, the fair \(DM\) decreases by about 112 bps.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;That’s why this abstraction and the translation matters– for a &lt;strong&gt;sponsor&lt;/strong&gt;, choosing \((A,E)\) is literally choosing a point on the disaster/risk probability curve that will print at a given spread. For an &lt;strong&gt;investor&lt;/strong&gt;, doing this quickly is how you decide whether a new bond issuance is compensatory or not relative to its modeled tail risk.&lt;/p&gt;

&lt;h2 id=&quot;going-forward&quot;&gt;2025 and Going Forward&lt;/h2&gt;

&lt;p&gt;If you only look at the coupon, 2025 cat bonds can look enticing. But Plenum’s market data (via Artemis) says something more subtle. As of &lt;strong&gt;June 27, 2025&lt;/strong&gt;, Plenum data show the cat bond market’s risk spread (discount margin) at &lt;strong&gt;6.73%&lt;/strong&gt; and the collateral yield at &lt;strong&gt;4.3%&lt;/strong&gt;– implying an overall yield of roughly &lt;strong&gt;11.03%&lt;/strong&gt;. &lt;sup id=&quot;fnref:plenum-jun2025&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:plenum-jun2025&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; A few weeks earlier, as of &lt;strong&gt;May 30, 2025&lt;/strong&gt;, the overall cat bond market yield was around &lt;strong&gt;10.93%&lt;/strong&gt;. &lt;sup id=&quot;fnref:plenum-may2025&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:plenum-may2025&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This means that, even when the headline yield is ~11%, a large chunk of it is just cash rates (the collateral yield), not cat bond compensation. The number to focus on for catastrophe pricing is still the &lt;strong&gt;DM&lt;/strong&gt; and how it stacks up against modeled expected loss.&lt;/p&gt;

&lt;p&gt;And the other 2025 development that matters for structure is distribution. The &lt;strong&gt;Brookmont Catastrophic Bond ETF (ILS)&lt;/strong&gt; launched &lt;strong&gt;April 1, 2025&lt;/strong&gt;, with a stated &lt;strong&gt;1.58%&lt;/strong&gt; expense ratio, framed in the FT as the first cat bond ETF opening broader access to the asset class. &lt;sup id=&quot;fnref:ils-etf&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ils-etf&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:ft&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ft&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;None of this news really says where the spreads should be. But based on all this, the takeaway is that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Don’t focus on on “8–10% yields.” Focus on &lt;strong&gt;DM&lt;/strong&gt;, &lt;strong&gt;EL&lt;/strong&gt;, and the &lt;strong&gt;multiple&lt;/strong&gt;, and then ask whether you’re being paid for (i) pure catastrophe risk, or (ii) a rate regime that can vanish faster than hurricane season or any disaster-prone environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;Notes and References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:chicagofed&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Polacek, Andy. &lt;em&gt;“Catastrophe Bonds: A Primer and Retrospective,”&lt;/em&gt; Chicago Fed Letter No. 405 (2018). The diagram in the screenshot above is taken from the website. &lt;a href=&quot;https://www.chicagofed.org/publications/chicago-fed-letter/2018/405&quot;&gt;https://www.chicagofed.org/publications/chicago-fed-letter/2018/405&lt;/a&gt;. &lt;a href=&quot;#fnref:chicagofed&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:plenum-jun2025&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Evans, Steve. &lt;em&gt;“Cat bond yields rise above 11%, discount margin to decrease through wind season: Plenum,”&lt;/em&gt; Artemis (July 7, 2025). &lt;a href=&quot;https://www.artemis.bm/news/cat-bond-yields-rise-above-11-discount-margin-to-decrease-through-wind-season-plenum/#:~:text=The%20overall%20yield%20of%20the,bond%20fund%20manager%20Plenum%20Investments.&quot;&gt;https://www.artemis.bm/news/cat-bond-yields-rise-above-11-discount-margin-to-decrease-through-wind-season-plenum/#:~:text=The%20overall%20yield%20of%20the,bond%20fund%20manager%20Plenum%20Investments.&lt;/a&gt;. &lt;a href=&quot;#fnref:plenum-jun2025&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:plenum-may2025&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Evans, Steve. &lt;em&gt;“Cat bond yields up slightly to 10.93% at May 30th. Seasonal spread widening nears end: Plenum,”&lt;/em&gt; Artemis (June 3, 2025). &lt;a href=&quot;https://www.artemis.bm/news/cat-bond-yields-up-slightly-to-10-93-at-may-30th-seasonal-spread-widening-nears-end-plenum/&quot;&gt;https://www.artemis.bm/news/cat-bond-yields-up-slightly-to-10-93-at-may-30th-seasonal-spread-widening-nears-end-plenum/&lt;/a&gt;. &lt;a href=&quot;#fnref:plenum-may2025&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ils-etf&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Brookmont, &lt;em&gt;Brookmont Catastrophic Bond ETF (ILS) Fact Sheet&lt;/em&gt;. &lt;a href=&quot;https://47598668.fs1.hubspotusercontent-na1.net/hubfs/47598668/ILS%20ETF%20Fund%20Documents/ILS%20ETF%20Fact%20Sheet.pdf&quot;&gt;https://47598668.fs1.hubspotusercontent-na1.net/hubfs/47598668/ILS%20ETF%20Fund%20Documents/ILS%20ETF%20Fact%20Sheet.pdf&lt;/a&gt;. &lt;a href=&quot;#fnref:ils-etf&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ft&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Financial Times, &lt;em&gt;“First cat bond ETF opens new frontier in low-correlation returns”&lt;/em&gt; (Apr 1, 2025). &lt;a href=&quot;https://www.ft.com/content/6465b1de-d476-4920-94ce-849d93116233&quot;&gt;https://www.ft.com/content/6465b1de-d476-4920-94ce-849d93116233&lt;/a&gt;. &lt;a href=&quot;#fnref:ft&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="finance" />
      

      

      
        <summary type="html">This summer, I’ve gotten the chance to dive deep into the economics and finance of insurance. While I’ve mainly been digging deep into the wave of insurer collapses in Louisiana and the state’s unprecedented and subsequent access of the debt markets, I stumbled upon the catastrophe bond market. In credit markets, spreads are often representative of probabilities of default, default intensity, and assumptions on the recovery of the debt. For catastrophe bonds (or cat bonds), the idea remains, but the same abstraction has such a different flavor that I decided to think through it a little more carefully. I detail my findings in this post. For cat bonds, the quoted spread is really the market’s price for a defined portion of the losses associated with a particular catastrophe. You take the bond’s discount margin, compare it to the model’s expected loss for that tranche, and then ask the question: is the market paying enough for the risk in this layer? In other words, do you get paid a lot per unit of modeled loss, or is the compensation low? This post is all about making the translation between how a hurricane loss distribution becomes a tranche, and how that tranche can become a spread that can actually be traded. Table of Contents A Recap on Cat Bonds Expected Loss to Discount Margin The “Multiple” and the Implied Disaster Curve Structuring a Deal Example 2025 and Going Forward Notes and References A Recap on Cat Bonds A cat bond is basically a principal-at-risk, floating rate note that is issued out of an SPV. Investor principal is placed in a collateral account (typically Treasuries/cash accounts), the coupons float off that collateral return, and then the principal can be written down if certain kinds of conditions are met. The key quoting convention here is the discount margin (DM) which is the spread over the collateral yield. 1 Catastrophe Bond Structure (Federal Reserve Bank of Chicago) Please refer to the Chicago Fed’s primer article (linked in the references) for a much more detailed background and sketch. With the basics out of the way, we can move forward. Expected Loss to Discount Margin Other than the reported coupon, there are two values that matter the most when it comes to the evaluation of cat bonds: Expected loss (EL): This is the catastrophe risk modeler’s estimate of the average annual principal loss (or actuarial loss cost) fraction. Discount margin (DM): This is what the bond pays you over the collateral for taking that risk. For the sake of intuition, if collateral earns \(r_t\), then the coupon is roughly: \[C_t \approx r_t + \mathrm{DM}\] So DM is the “cat spread.” The market then roughly decomposes DM into the following: \[\mathrm{DM} \approx \mathrm{EL} + \text{risk premium} + \text{frictions (liquidity, model risk, etc.)}\] For a single standardized valuation statistic, consider the multiple: \[M \equiv \frac{\mathrm{DM}}{\mathrm{EL}}\] It’s the cat bond analog of “spread per unit of default risk,” and it’s a convenient shorthand because cat bond cashflows are event-driven and the modeling inputs can be noisy. The “Multiple” and the Implied Disaster Curve This is where cat bonds offer an interesting mental model– an equivalence can be drawn between catastrophe insurance coverage and the priced tranche of a particular loss distribution. That is, the term sheet can be treated as a tranche quote on a tail distrbution. An easy way to see this is to start with a toy model that lets us do some back-of-the-envelope math: we consider maturity \(T\), a trigger condition that arises with risk-neutral intensity \(q\), and a principal loss fraction \(L\in[0,1]\) if it triggers. Then the annualized expected principal loss is approximately \(qL\), and the par spread here becomes the following: \[\mathrm{DM}_{\text{par}} \approx qL\] i.e. we end up with spread ≈ hazard × loss-given-trigger (the same mental model you use for a CDS premium when the “recovery” is fixed). So in practice, you can do two translations here: From DM to an implied “market EL” If the bond has binary losses \((L \approx 1)\), then \(\mathrm{DM}\) is already a rough proxy for \(q\). If the losses are partial, you can scale by \(L\). From modeled EL to a clearing DM through the multiple Rearranging it gives us \(\mathrm{DM} = M \cdot \mathrm{EL}\). The job is then to decide whether \(M\) is generous given the layer, the triggering condition, and the current market. From the perspective of hedge funds, this makes cat bonds really interesting. In this case, you’re not just “buying catastrophe risk,” but in fact, you’re essentially betting on a point on an implied tail curve– the bets in the market are on the shape of the curve. Structuring a Deal This is the structuring problem that sponsors and banks consider and “solve”: Choose attachment and limit so the tranche has the expected loss investors will buy at an acceptable multiple. Let \(X\) be the sponsor loss (or the index loss) over one year, in dollars. A cat bond tranche can be thought of as a layer that: attaches at \(A\): the bond starts taking losses only once \(X &amp;gt; A\) exhausts at \(E\): once the losses hit \(E\), the bond pays out its maximum; the investors can’t lose more than the tranche size limit \(K = E-A\): the maximum dollar amount that the tranche can pay out Its payout is: \[\text{payout}(X)=\min\{\max(X-A,0),K\}\] A very useful identity is then (boiling down the math from Excel): \[\mathbb{E}[\text{payout}] = \int_A^E \mathbb{P}(X&amp;gt;u)\,du\] So the annual expected loss fraction is: \[\mathrm{EL} = \frac{\mathbb{E}[\text{payout}]}{K}\] It’s simple but that’s the connection from catastrophe modeling to the bond structuring: The modeler gives a probability of excess curve that maps \(u\) to \(\mathbb{P}(X&amp;gt;u)\) Integrate it over \([A,E]\) to get expected payout Divide by \(K\) to get \(EL\) Multiply by a clearing multiple \(M\) to get the spread that you print Discretization If the model provides points \((u_i, p_i)\) on the exceedance probability curve rather than the actual functional form, then approximate the integral with trapezoids: \[\int_A^E \mathbb{P}(X&amp;gt;u)\,du \approx \sum_i \frac{p_i+p_{i+1}}{2}\cdot (u_{i+1}-u_i)\] over grid points spanning \([A,E]\). An Example Suppose the risk modeler produces the following annual exceedance curve for a hurricane loss index \(X\) (loss thresholds in $ millions): Loss threshold \(u\) (USD mm) 800 900 1000 1100 1200 1300 \(\mathbb{P}(X&amp;gt;u)\) 6.0% 4.8% 3.8% 3.0% 2.4% 1.9% You’re deciding between two candidate tranches, each with $200mm limit (so \(K=200\)). Candidate 1: \(A=1000\), \(E=1200\) (\(K=200\)) Approximate the expected payout using trapezoids with step size \(100\): \([1000,1100]\): avg prob \(=\frac{3.8\%+3.0\%}{2}=3.4\%\) ⇒ contribution \(=0.034\times 100=3.4\) (mm) \([1100,1200]\): avg prob \(=\frac{3.0\%+2.4\%}{2}=2.7\%\) ⇒ contribution \(=0.027\times 100=2.7\) (mm) So \(\mathbb{E}[\text{payout}] \approx 3.4+2.7=6.1\) (mm), and: \[\mathrm{EL}_1 = \frac{6.1}{200} = 3.05\% \text{ per year}\] If the market is clearing this style of risk at, say, \(M=1.8\), then the required discount margin is: \[\mathrm{DM}_1 = M\cdot \mathrm{EL}_1 = 1.8\times 3.05\% = 5.49\% \approx 549\text{ bps}\] Candidate 2: Raise the layer by $100mm: \(A=1100\), \(E=1300\) (still \(K=200\)) \([1100,1200]\): contribution \(=2.7\) (mm) (same as above) \([1200,1300]\): avg prob \(=\frac{2.4\%+1.9\%}{2}=2.15\%\) ⇒ contribution \(=0.0215\times 100=2.15\) (mm) So \(\mathbb{E}[\text{payout}]\approx 4.85\) (mm), and: \[\mathrm{EL}_2 = \frac{4.85}{200} = 2.425\%\] \[\mathrm{DM}_2 = 1.8\times 2.425\% = 4.365\% \approx 437\text{ bps}\] So, given the same disaster, same maturity, and same $200mm size, if the tranche moves up by $100mm, the fair \(DM\) decreases by about 112 bps. That’s why this abstraction and the translation matters– for a sponsor, choosing \((A,E)\) is literally choosing a point on the disaster/risk probability curve that will print at a given spread. For an investor, doing this quickly is how you decide whether a new bond issuance is compensatory or not relative to its modeled tail risk. 2025 and Going Forward If you only look at the coupon, 2025 cat bonds can look enticing. But Plenum’s market data (via Artemis) says something more subtle. As of June 27, 2025, Plenum data show the cat bond market’s risk spread (discount margin) at 6.73% and the collateral yield at 4.3%– implying an overall yield of roughly 11.03%. 2 A few weeks earlier, as of May 30, 2025, the overall cat bond market yield was around 10.93%. 3 This means that, even when the headline yield is ~11%, a large chunk of it is just cash rates (the collateral yield), not cat bond compensation. The number to focus on for catastrophe pricing is still the DM and how it stacks up against modeled expected loss. And the other 2025 development that matters for structure is distribution. The Brookmont Catastrophic Bond ETF (ILS) launched April 1, 2025, with a stated 1.58% expense ratio, framed in the FT as the first cat bond ETF opening broader access to the asset class. 45 None of this news really says where the spreads should be. But based on all this, the takeaway is that: Don’t focus on on “8–10% yields.” Focus on DM, EL, and the multiple, and then ask whether you’re being paid for (i) pure catastrophe risk, or (ii) a rate regime that can vanish faster than hurricane season or any disaster-prone environment. Notes and References Polacek, Andy. “Catastrophe Bonds: A Primer and Retrospective,” Chicago Fed Letter No. 405 (2018). The diagram in the screenshot above is taken from the website. https://www.chicagofed.org/publications/chicago-fed-letter/2018/405. &amp;#8617; Evans, Steve. “Cat bond yields rise above 11%, discount margin to decrease through wind season: Plenum,” Artemis (July 7, 2025). https://www.artemis.bm/news/cat-bond-yields-rise-above-11-discount-margin-to-decrease-through-wind-season-plenum/#:~:text=The%20overall%20yield%20of%20the,bond%20fund%20manager%20Plenum%20Investments.. &amp;#8617; Evans, Steve. “Cat bond yields up slightly to 10.93% at May 30th. Seasonal spread widening nears end: Plenum,” Artemis (June 3, 2025). https://www.artemis.bm/news/cat-bond-yields-up-slightly-to-10-93-at-may-30th-seasonal-spread-widening-nears-end-plenum/. &amp;#8617; Brookmont, Brookmont Catastrophic Bond ETF (ILS) Fact Sheet. https://47598668.fs1.hubspotusercontent-na1.net/hubfs/47598668/ILS%20ETF%20Fund%20Documents/ILS%20ETF%20Fact%20Sheet.pdf. &amp;#8617; Financial Times, “First cat bond ETF opens new frontier in low-correlation returns” (Apr 1, 2025). https://www.ft.com/content/6465b1de-d476-4920-94ce-849d93116233. &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Regimes of Return Smoothing</title>
      
      
      <link href="https://a-sircar1.github.io/2024/12/30/24-IHMS-SETARMA/" rel="alternate" type="text/html" title="Regimes of Return Smoothing" />
      
      <published>2024-12-30T06:10:56+00:00</published>
      <updated>2024-12-30T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2024/12/30/24-IHMS-SETARMA</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2024/12/30/24-IHMS-SETARMA/">&lt;p&gt;I recently finished up a paper on determining the regimes of return smoothing for an independent study. In my research, I took a look at how reported returns in illiquid asset classes– like real estate and hedge funds– move through different “smoothing regimes” over time. I introduce an Infinite Hidden Markov-switching Self-exciting ARMA (IHMS-SETARMA) model that lets the data uncover both the number and structure of these regimes, with regime-specific lag orders and autocorrelation patterns. The core idea is that reported returns reflect not just true economic performance, but evolving reporting and valuation practices that may react to market conditions. Using industry indices, I document how the intensity and shape of smoothing shift around major market events, and propose summary measures– like the sum of MA coefficients and a Herfindahl-style concentration index– to compare regimes. My hope is that this framework becomes a starting point for a more systematic, regime-based view of return smoothing and risk management in illiquid markets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Return smoothing is a common feature observed in the returns of illiquid and alternative asset classes, including real estate and private equity. Although the definitive reasons for serial correlation observed in returns is widely debated, empirical studies have discovered time-varying autocorrelation in observed returns. In this paper, we argue that this dynamic autocorrelation or level of smoothing can be attributed to changes in return reporting structure or valuation technique, largely developed as responses to market conditions. To this extent, we develop a statistical framework called the Infinite Hidden Markov-switching Self-exciting ARMA (IHMS-SETARMA) model and estimation approach toward studying regime recovery and analysis in financial time-series data. Then, we adapt the standard model of return smoothing for analysis using our model. We report several preliminary statistics on the regimes and their features observed for the FTSE Nareit U.S. Real Estate Index and the Credit Suisse Hedge Fund Index as broad indicators of their respective industries/asset classes.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="finance" />
      
        <category term="statistics" />
      

      

      
        <summary type="html">I recently finished up a paper on determining the regimes of return smoothing for an independent study. In my research, I took a look at how reported returns in illiquid asset classes– like real estate and hedge funds– move through different “smoothing regimes” over time. I introduce an Infinite Hidden Markov-switching Self-exciting ARMA (IHMS-SETARMA) model that lets the data uncover both the number and structure of these regimes, with regime-specific lag orders and autocorrelation patterns. The core idea is that reported returns reflect not just true economic performance, but evolving reporting and valuation practices that may react to market conditions. Using industry indices, I document how the intensity and shape of smoothing shift around major market events, and propose summary measures– like the sum of MA coefficients and a Herfindahl-style concentration index– to compare regimes. My hope is that this framework becomes a starting point for a more systematic, regime-based view of return smoothing and risk management in illiquid markets. Abstract Return smoothing is a common feature observed in the returns of illiquid and alternative asset classes, including real estate and private equity. Although the definitive reasons for serial correlation observed in returns is widely debated, empirical studies have discovered time-varying autocorrelation in observed returns. In this paper, we argue that this dynamic autocorrelation or level of smoothing can be attributed to changes in return reporting structure or valuation technique, largely developed as responses to market conditions. To this extent, we develop a statistical framework called the Infinite Hidden Markov-switching Self-exciting ARMA (IHMS-SETARMA) model and estimation approach toward studying regime recovery and analysis in financial time-series data. Then, we adapt the standard model of return smoothing for analysis using our model. We report several preliminary statistics on the regimes and their features observed for the FTSE Nareit U.S. Real Estate Index and the Credit Suisse Hedge Fund Index as broad indicators of their respective industries/asset classes.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">The Sample Complexity of RL-based Optimizers for Financial Applications (Part II): EM FX Carry Trade Control Problem</title>
      
      
      <link href="https://a-sircar1.github.io/2024/07/25/24-RL-optimizers-part2/" rel="alternate" type="text/html" title="The Sample Complexity of RL-based Optimizers for Financial Applications (Part II): EM FX Carry Trade Control Problem" />
      
      <published>2024-07-25T06:10:56+00:00</published>
      <updated>2024-07-25T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2024/07/25/24-RL-optimizers-part2</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2024/07/25/24-RL-optimizers-part2/">&lt;p&gt;&lt;em&gt;If you haven’t taken a look at Part I yet, please take a look at it &lt;a href=&quot;/2024/06/06/24-RL-optimizers/&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As soon as I finished fleshing out the single period mean-variance model from &lt;a href=&quot;/2024/06/06/24-RL-optimizers/&quot;&gt;Part I&lt;/a&gt;, I started thinking about more tangible problems that a trading desk may face, specifically (1) where the portfolios need to be dynamically rebalanced, as spreads, liquidity, and funding conditions change over time, and (2) when the risk constraints or funding constraints change and market stressors change, i.e., risk is not only captured via a fixed covariance matrix across the sample.&lt;/p&gt;

&lt;p&gt;Recently, the hedge funds and investors have flocked toward the Turkish Lira due to the higher interest rate environment in Turkey (as high as 50% at the time of writing this piece!), largely a result of a change in course toward conventional economic approaches in the nation and a relative curtailing of inflation.&lt;sup id=&quot;fnref:ft-try&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ft-try&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Investors are taking advantage of the classic carry trade, in which money is borrowed for a trade in currency with lower interest rates to maximize gain on a currency in a high interest rate setting while betting that any shifts in the exchange rate do not upset the strategy. As FX in emerging markets is clearly quite a risky asset, and market regimes shift all the time, I decided it might be interesting to model a trading desk’s dynamic optimization problem to reflect this setting.&lt;/p&gt;

&lt;h3&gt;Picking Up from Where We Left Off&lt;/h3&gt;

&lt;p&gt;In short, my analysis in Part I treated each of the optimizers as an arm in a single period mean-variance bandit. For a fixed pair \((\mu,\Sigma)\), the problem was to choose, with as few episodes as possible, among a finite family of algorithms that output portfolios onto a simplex. The meta-distribution over cases \(p = (\mu,\Sigma)\) captured cross-sectional and temporal variation in the opportunity sets, and the main object of interest was the gap \(\mu_{k^{\star}} - \mu_{\hat{k}}\) between the best optimizer and the one that was empirically selected.&lt;/p&gt;

&lt;p&gt;But now, we’re shifting from single period mean-variance to a dynamic stochastic program.&lt;/p&gt;

&lt;p&gt;Given that sequential adjustments are now made to the portfolio in response to sequential occurrences, a finite-state Markov decision process (MDP) can be appropriate for the model. Instead of drawing independent problem cases \(p_i \sim \mathcal{P}\) as in Part I, I now observe a single evolving state process \(s_t\) whose components summarize carry premia, volatility, any crash indicators, and also any slack on the balance sheet. In this setting, a trading strategy is then a policy \(\pi\) that maps each state \(s_t\) into a feasible control \(a_t\), for example a vector of currency positions subject to leverage and margin constraints. The objective here is to maximize a discounted risk-adjusted P&amp;amp;L functional as follows:&lt;/p&gt;

\[V^{\pi}(s_0)
=
\mathbb{E}^{\pi}\!\left[
\sum_{t=0}^{\infty} \gamma^t r(s_t,a_t)
\right]\]

&lt;p&gt;where \(\gamma \in (0,1)\) captures the persistence of carry and crash regimes, and the reward \(r(s_t,a_t)\) aggregates the carry, mark-to-market movements, transaction costs, and risk penalties in a bounded way.&lt;/p&gt;

&lt;p&gt;From the perspective of Part I, this is a shift from comparing a finite list of static optimizers on i.i.d. portfolio problems to comparing policies in a stochastic environment. Similar underlying concentration inequalities still govern how many observations are needed to distinguish near-optimal policies from sub-optimal ones. The difference is that the relevant complexity parameters are now the number of market regimes and control actions, and the effective horizon \(1 / (1 - \gamma)\) that is implied by the persistence of emerging market (EM) FX or credit risk factors instead of the simple cardinality \(K\) of a candidate optimizer family.&lt;/p&gt;

&lt;p&gt;With this in mind, I place the sample-complexity results of AJKS to a concrete dynamic problem that is controlling an EM FX carry portfolio under leverage and crash risk. The structure of the problem is inspired by the empirical literature on carry trades and currency crashes, which documents regime-dependent downside risk,&lt;sup id=&quot;fnref:brunnermeier2009&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:brunnermeier2009&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and by work on optimal carry trade portfolios that already treat carry investing as a state-dependent dynamic allocation problem.&lt;sup id=&quot;fnref:laborda2014&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:laborda2014&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:chen2022&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:chen2022&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; The aim of this note is to treat the same economic environment as an unknown MDP and ask how many simulated or historical episodes would be needed for an RL agent to learn a policy whose risk–adjusted value is provably close to that of the best feasible EM FX carry control policy.&lt;/p&gt;

&lt;h3&gt;Model&lt;/h3&gt;

&lt;p&gt;A trading desk that runs an EM FX carry book faces a dynamic stochastic program. The desk chooses a leveraged long-short position in a set of EM currencies against a funding currency, under balance-sheet, margin, and drawdown constraints. The control problem is cast as an MDP, and minmax-optimal sample complexity bounds for discounted control and finite-horizon exploration are used from results from AJKS (I will refer to the RL theory book as AJKS from here on out). &lt;sup id=&quot;fnref:ajks-ch2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:ajks-ch7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; The bounds are then rewritten in terms of economically meaningful quantities like regime persistence, the number of states needed to track carry and crash risk, and the discretization of position size and leverage. The set-up can now be formalized. Consider a single funding currency, indexed by \(f\), and \(d\) EM currencies, indexed by \(j = 1,\dots,d\). At discrete times \(t = 0,1,2,\dots\), the desk rebalances over a fixed interval \(\Delta t\) (like one week, for example). For each EM currency \(j\) at time \(t\), the following objects are observed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The short risk-free rate of the funding currency \(r_t^f\)&lt;/li&gt;
  &lt;li&gt;The short local collateral rate \(r_{t}^j\)&lt;/li&gt;
  &lt;li&gt;The log spot exchange rate \(S_t^j = \log \text{FX}_t^j\), that is quoted as units of funding currency per unit of EM currency&lt;/li&gt;
  &lt;li&gt;A volatility proxy \(\sigma_t^j\) (for example, implied volatility)&lt;/li&gt;
  &lt;li&gt;A crash-regime variable \(Z_t^j \in \{0,1\}\), where \(Z_t^j = 1\) indicates a stressed or regime that is prone to crashes for currency \(j\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Define the instantaneous carry spread of currency \(j\) over the funding currency as&lt;/p&gt;

\[c_t^j = r_{t}^j - r_t^f\]

&lt;p&gt;At time \(t\) the desk chooses a position vector \(u_t = (u_t^1,\dots,u_t^d)\) in units of notional per unit of capital, where \(u_t^j &amp;gt; 0\) means being long EM currency \(j\) funded in \(f\), and \(u_t^j &amp;lt; 0\) means short EM currency \(j\) and long the funding currency. There is a gross leverage constraint:&lt;/p&gt;

\[\sum_{j=1}^d \|u_t^j\| \leq L_{\max}\]

&lt;p&gt;and a position cap for each of the currencies \(\|u_t^j\| \leq U_{\max}^j\).&lt;/p&gt;

&lt;p&gt;Over one period, the currency-level log return in the funding currency is&lt;/p&gt;

\[R_{t+1}^j = S_{t+1}^j - S_t^j + c_t^j \Delta t - \kappa_t^j(u_t^j)\]

&lt;p&gt;where \(\kappa_t^j(u_t^j)\) captures trading and funding costs (for example, bid-ask spread, charges on the balance sheet, and basis). At the portfolio level, the unadjusted P&amp;amp;L per unit of capital over one period is&lt;/p&gt;

\[\Pi_{t+1} = \sum_{j=1}^d u_t^j R_{t+1}^j\]

&lt;p&gt;To turn this into a risk-adjusted reward that is compatible with discounted control, fix a risk-aversion parameter \(\lambda &amp;gt; 0\) and define the conditional variance like so:&lt;/p&gt;

\[v_t(u_t) = \operatorname{Var}\!\left(\Pi_{t+1} \mid \mathcal{F}_t\right)\]

&lt;p&gt;where \(\mathcal{F}_t\) is the information at time \(t\). Define the raw risk-adjusted payoff&lt;/p&gt;

\[X_{t+1} = \Pi_{t+1} - \frac{\lambda}{2} v_t(u_t)\]

&lt;p&gt;Because currency returns and carry spreads are bounded in any realistic risk environment, I say there exists a constant \(B &amp;gt; 0\) such that with high probability \(\|X_{t+1}\| \leq B\). For the math to hold, I clip and rescale to obtain the bounded reward&lt;/p&gt;

\[r_{t+1} = \frac{1}{2} + \frac{1}{2B} \left( \max\{-B, \min\{X_{t+1}, B\}\} \right)\]

&lt;p&gt;so that \(r_{t+1} \in [0,1]\) and all policies are compared on a normalized scale. This rescaling does not change the ordering of the policies by risk-adjusted performance.&lt;/p&gt;

&lt;p&gt;The desk then faces a stochastic program of maximizing the discounted value&lt;/p&gt;

\[V^{\pi}(s_0) = \mathbb{E}^{\pi}\!\left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_0 \right]\]

&lt;p&gt;over policies \(\pi\) mapping states to actions, where \(\gamma \in (0,1)\) is a discount factor.&lt;/p&gt;

&lt;h3&gt;State, action, and discount in financial terms&lt;/h3&gt;

&lt;p&gt;Define the state at time \(t\) as a summary:&lt;/p&gt;

\[s_t = \big(C_t, \Sigma_t, Z_t, M_t, K_t\big)\]

&lt;p&gt;where there is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(C_t\) stores the discretized carry spreads \(c_t^j\)&lt;/li&gt;
  &lt;li&gt;\(\Sigma_t\) stores discretized volatility proxies \(\sigma_t^j\)&lt;/li&gt;
  &lt;li&gt;\(Z_t\) is the vector of crash regimes \(Z_t^j\)&lt;/li&gt;
  &lt;li&gt;\(M_t\) stores remaining time to any hard horizon (for example, investor capital lock-up)&lt;/li&gt;
  &lt;li&gt;\(K_t\) stores risk and margin information like remaining capacity under VaR limits or drawdown constraints, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This state takes values in a finite set \(\mathcal{S}\) once each of the components is discretized into a finite grid. For example, each \(c_t^j\) might be mapped into one of \(G_c\) quantile buckets, each \(\sigma_t^j\) into \(G_{\sigma}\) buckets, and each crash regime \(Z_t^j\) into the two values \(0\) or \(1\). If aggregation is done across the currencies into factor summaries (for example, a global EM carry factor, a volatility factor, and a crash indicator), the resulting state cardinality can be written as so:&lt;/p&gt;

\[\| \mathcal{S} \| = S_{\text{carry}} \cdot S_{\text{vol}} \cdot S_{\text{crash}} \cdot S_{\text{horizon}} \cdot S_{\text{margin}}\]

&lt;p&gt;where each \(S_{\cdot}\) is a design choice of the desk. The action at time \(t\) is the position vector \(u_t\). For the purposes of sample-complexity analysis, discretize \(u_t^j\) onto a finite grid, for instance&lt;/p&gt;

\[u_t^j \in \{-U_{\max}^j, -U_{\max}^j + \Delta u^j, \dots, U_{\max}^j\}\]

&lt;p&gt;and only allow grids to be consistent with the leverage and margin constraints. The action space \(\mathcal{A}\) is then finite with cardinality \(\| \mathcal{A} \| = A_{\text{pos}}\), which depends on the position grid and the number of currencies, as well as the shape of the leverage constraint.&lt;/p&gt;

&lt;p&gt;The discount factor \(\gamma\) has a financial interpretation. Suppose that the EM carry factor has an approximate exponential autocorrelation with half-life \(H_{\text{half}}\) that is measured in rebalancing periods. That is, if the lag-k autocorrelation of a relevant factor is approximately \(\rho_k \approx \exp(-k / \tau)\) for some persistence scale \(\tau\), then the half-life is defined as&lt;/p&gt;

\[\frac{1}{2} = \exp\!\left(-\frac{H_{\text{half}}}{\tau}\right)\]

&lt;p&gt;and solving for \(H_{\text{half}}\) produces&lt;/p&gt;

\[H_{\text{half}} = \tau \log 2\]

&lt;p&gt;Setting one step to length \(\Delta t\) and choosing a discount factor&lt;/p&gt;

\[\gamma = \exp\!\left(-\frac{\Delta t}{\tau}\right)\]

&lt;p&gt;gives a planning horizon on the order of&lt;/p&gt;

\[\frac{1}{1 - \gamma} \approx \frac{\tau}{\Delta t}\]

&lt;p&gt;So the factor \(1 - \gamma\) that governs the sample complexity can be written in terms of observed carry persistence.&lt;/p&gt;

&lt;p&gt;With these definitions, EM FX carry control is a discounted MDP with finite state space \(\mathcal{S}\), finite action space \(\mathcal{A}\), bounded reward in \([0,1]\), and discount factor \(\gamma \in (0,1)\). This is exactly the setting of the discounted sample-complexity results in AJKS Chapter 2. &lt;sup id=&quot;fnref:ajks-ch2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3&gt;Generative-model Sample Complexity in terms of EM FX&lt;/h3&gt;

&lt;p&gt;Now, step back and assume the desk has access to a calibrated generative model of EM FX and carry dynamics. This can be any kind of risk simulator. Given any state-action pair \((s,a)\), the simulator returns \((s&apos;, r) \sim G(\cdot \mid s,a)\) where \(s&apos;\) is the next state and \(r \in [0,1]\) is the normalized reward. As in AJKS, define an empirical MDP \(\hat{\mathcal{M}}\) that has a transition kernel \(\hat{P}\) that is built from \(N\) independent samples for each \((s,a)\). AJKS show that there exist absolute constants such that, for any \(\varepsilon \in (0,1)\) and \(\delta \in (0,1)\), if the number of simulator calls per state-action pair \(N\) satisfies&lt;/p&gt;

\[N \ge c_0 \cdot \frac{1}{(1 - \gamma)^3} \cdot \frac{\log\!\big(c_1 \| \mathcal{S} \| \| \mathcal{A} \| / \delta\big)}{\varepsilon^2}\]

&lt;p&gt;then, with probability at least \(1 - \delta\), the optimal value function \(Q^{\star}\) and the optimal empirical value function \(\hat{Q}^{\star}\) satisfy&lt;/p&gt;

\[\| Q^{\star} - \hat{Q}^{\star} \|_{\infty} \leq \varepsilon\]

&lt;p&gt;and the policy \(\hat{\pi}\) that is optimal in \(\hat{\mathcal{M}}\) satisfies&lt;/p&gt;

\[\| Q^{\star} - Q^{\hat{\pi}} \|_{\infty} \leq \varepsilon\]

&lt;p&gt;given that \(N\) is not too small.&lt;sup id=&quot;fnref:ajks-ch2:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; The total number of simulator calls is then&lt;/p&gt;

\[N_{\text{total}} = \| \mathcal{S} \| \| \mathcal{A} \| N\]

&lt;p&gt;Substituting the inequality for \(N\) into the expression for \(N_{\text{total}}\), I obtain the following sample-complexity bound that is specific to EM. Let \(S = \| \mathcal{S} \|, A = \| \mathcal{A} \|\). Then it should suffice to take&lt;/p&gt;

\[N_{\text{total}} \ge c \cdot \frac{S A}{(1 - \gamma)^3} \cdot \frac{\log\!\big(c S A / \delta\big)}{\varepsilon^2}\]

&lt;p&gt;for a constant \(c\), to make sure that the policy learned by model-based planning on the simulator is \(\varepsilon\)-optimal with probability at least \(1 - \delta\).&lt;sup id=&quot;fnref:ajks-ch2:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The financial interpretation of each factor is as follows. First, the factor \(S A\) is the effective size of the EM FX control problem. Increasing the granularity of the carry buckets, volatility regimes, crash flags, or margin states increases \(S\). Increasing the granularity of position and leverage grids increases \(A\). These are design choices that would be made by the trading desk and are not fixed constraints.&lt;/p&gt;

&lt;p&gt;Second, the factor \((1 - \gamma)^{-3}\) measures the difficulty that is added by persistence in carry and crash regimes. Using the relation&lt;/p&gt;

\[\gamma = \exp\!\left(-\frac{\Delta t}{\tau}\right)\]

&lt;p&gt;expand \(1 - \gamma\) using the first-order approximation of the exponential,&lt;/p&gt;

\[1 - \gamma = 1 - \exp\!\left(-\frac{\Delta t}{\tau}\right) \approx \frac{\Delta t}{\tau}\]

&lt;p&gt;when \(\Delta t / \tau\) is small. Then&lt;/p&gt;

\[\frac{1}{(1 - \gamma)^3} \approx \left( \frac{\tau}{\Delta t} \right)^3\]

&lt;p&gt;Thus the sample requirement grows as the cube of the ratio between the carry persistence scale \(\tau\) and the rebalancing interval \(\Delta t\). For weekly rebalancing and assuming an eight-week half-life (for example), this ratio is of order \(8\), and its cube is of order \(512\). Slow-moving regimes make learning an optimal EM carry control policy inherently more sample intensive.&lt;/p&gt;

&lt;p&gt;Third, the logarithmic factor \(\log(c S A / \delta)\) is relatively mild, but it captures the reliability parameter \(\delta\). Demanding that the learned policy is close to optimal with probability \(1 - \delta = 0.99\) instead of \(0.95\) increases the log factor but does not change the polynomial dependence on \(S\), \(A\), \(\varepsilon\), or \(1 - \gamma\).&lt;/p&gt;

&lt;p&gt;This theorem is formally identical to the abstract discounted bound in AJKS, but its economics becomes interpretable once \(\gamma\), \(S\), and \(A\) are written in terms of state aggregation, position grids, and half-lives of factors.&lt;/p&gt;

&lt;h3&gt;Episodic Exploration without a Generative Model&lt;/h3&gt;

&lt;p&gt;In many situations the desk cannot query a simulator arbitrarily and only learns from executed positions. In that case it is natural to work with a finite-horizon formulation that is episodic. I consider a horizon of \(H\) rebalancing steps, after which the book is then forced to unwind. Episodes are indexed by \(k = 0,1,\dots,K - 1\), and each episode consists of states and actions&lt;/p&gt;

\[(s_{k,0}, a_{k,0}, \dots, s_{k,H-1}, a_{k,H-1})\]

&lt;p&gt;Assume that each of the episodes starts from the same benchmark state \(s_{0}\) (which could represent an unlevered start). For each episode define the regret of the policy \(\pi_k\) used in that episode as&lt;/p&gt;

\[\text{Regret}_k = V^{\star}_0(s_0) - V^{\pi_k}_0(s_0)\]

&lt;p&gt;where \(V^{\star}_0(s_0)\) is the optimal value starting at \(s_0\), and \(V^{\pi_k}_0(s_0)\) is the value of policy \(\pi_k\).&lt;sup id=&quot;fnref:ajks-ch7:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; The total expected regret after \(K\) episodes is&lt;/p&gt;

\[\text{Regret}(K) = \mathbb{E}\!\left[ \sum_{k=0}^{K-1} \text{Regret}_k \right]\]

&lt;p&gt;In the tabular finite-horizon setting, AJKS analyze the UCB-VI algorithm and show that the regret is bounded by a term of order&lt;/p&gt;

\[\text{Regret}(K) \leq C_1 H^2 S \sqrt{A K} \log\!\big(C_2 S A H K\big)\]

&lt;p&gt;for absolute constants \(C_1\) and \(C_2\).&lt;sup id=&quot;fnref:ajks-ch7:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; Now, in terms of the problem I look at, it can be seen that the average regret per episode satisfies&lt;/p&gt;

\[\frac{\text{Regret}(K)}{K} \leq C_1 H^2 S \sqrt{\frac{A}{K}} \log\!\big(C_2 S A H K\big)\]

&lt;p&gt;The key dependence here is that the shortfall in risk-adjusted value per episode decays on the order of&lt;/p&gt;

\[H^2 S \sqrt{\frac{A}{K}}\]

&lt;p&gt;up to logarithmic factors. To invert the bound, fix a target average regret per episode \(\rho &amp;gt; 0\). I want&lt;/p&gt;

\[\frac{\text{Regret}(K)}{K} \leq \rho\]

&lt;p&gt;Ignoring the logarithmic factors to see the main scaling, impose&lt;/p&gt;

\[C_1 H^2 S \sqrt{\frac{A}{K}} \leq \rho\]

&lt;p&gt;Solve the inequality for \(K\) as follows.&lt;/p&gt;

\[\begin{aligned}
&amp;amp;C_1 H^2 S \sqrt{\frac{A}{K}} \leq \rho \\
&amp;amp;\implies \sqrt{\frac{A}{K}} \leq \frac{\rho}{C_1 H^2 S} \\
&amp;amp;\implies \frac{A}{K} \leq \frac{\rho^2}{C_1^2 H^4 S^2}  \\
&amp;amp;\implies K \geq A \cdot \frac{C_1^2 H^4 S^2}{\rho^2} = \left( \frac{C_1 H^2 S \sqrt{A}}{\rho} \right)^2
\end{aligned}\]

&lt;p&gt;Thus, to drive the average regret per episode below \(\rho\), the number of episodes must scale to at least the order of \(H^4 S^2 A / \rho^2\). Because \(H\) is proportional to the horizon of the control problem and \(S\) and \(A\) are determined by the state and action discretizations, this gives an explicit trade-off here– decreasing state and action granularity can reduce learning time by large factors.&lt;/p&gt;

&lt;h3&gt;Optimizers as Policies for the Trading Desk&lt;/h3&gt;

&lt;p&gt;Li and Malik propose viewing an optimization algorithm itself as a policy in an MDP, where the “state” consists of past instances, gradients, and objective values, and the “action” is the next update step.&lt;sup id=&quot;fnref:limalik&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:limalik&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; In this context, I adopt the same perspective from a trading desk’s perspective. An EM carry optimization algorithm is a policy mapping the history of spreads, volatilities, crash indicators, and portfolio P&amp;amp;L to the next position vector.&lt;/p&gt;

&lt;p&gt;This has two consequences.&lt;/p&gt;

&lt;p&gt;First, a learned optimizer that is trained on a distribution of EM FX environments is effectively a meta-policy that solves many related MDPs. The sample-complexity theory here that is based on AJKS gives the number of simulated or historical episodes that is required to guarantee that this learned optimizer is near-optimal across that distribution of environments, once the optimizer class is viewed as a parameterized policy class.&lt;sup id=&quot;fnref:ajks-ch2:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ajks-ch2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Second, because the optimizer’s internal state can include risk metrics (such as realized variance), its behavior in stressed regimes can be shaped through the reward definition. For example, aggressively penalizing severe drawdowns in the reward (before clipping) may change the shape of the optimal policy in crash or crash-prone regimes without altering the official sample-complexity rate determined. The bounds guarantee that, with sufficient data, a learned optimizer that internalizes these kinds of penalties will behave appropriately across both normal and crash regimes.&lt;/p&gt;

&lt;h3&gt;Notes and References&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ft-try&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Financial Times, “Traders pour billions of dollars into Turkish lira trade” (July 20, 2024). Available at &lt;a href=&quot;https://www.ft.com/content/d93d22ff-0c46-4982-874e-0a0b156d140c&quot;&gt;https://www.ft.com/content/d93d22ff-0c46-4982-874e-0a0b156d140c&lt;/a&gt;. &lt;a href=&quot;#fnref:ft-try&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:brunnermeier2009&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Brunnermeier, Markus K.; Nagel, Stefan; Pedersen, Lasse H. (2009). “Carry Trades and Currency Crashes.” &lt;em&gt;NBER Macroeconomics Annual&lt;/em&gt; 23, 313–347. &lt;a href=&quot;#fnref:brunnermeier2009&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:laborda2014&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Laborda, Juan; Laborda, Ricardo; Olmo, José (2014). “Optimal currency carry trade strategies.” &lt;em&gt;International Review of Economics &amp;amp; Finance&lt;/em&gt; 33, 52–66. &lt;a href=&quot;#fnref:laborda2014&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:chen2022&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen, Ching-Neng (2022). “Optimal carry trade portfolio choice under regime shifts.” &lt;em&gt;Review of Quantitative Finance and Accounting&lt;/em&gt; 59(2), 541–573. &lt;a href=&quot;#fnref:chen2022&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ajks-ch2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Agarwal, Alekh; Jiang, Nan; Kakade, Sham; Sun, Wen. &lt;em&gt;Reinforcement Learning and Stochastic Optimization&lt;/em&gt; Chapter 2: “Sample Complexity with a Generative Model.” Available at &lt;a href=&quot;https://rltheorybook.github.io/&quot;&gt;https://rltheorybook.github.io/&lt;/a&gt;. &lt;a href=&quot;#fnref:ajks-ch2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:ajks-ch2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:ajks-ch2:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:ajks-ch2:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:ajks-ch2:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ajks-ch7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Agarwal, Alekh; Jiang, Nan; Kakade, Sham; Sun, Wen. &lt;em&gt;Reinforcement Learning and Stochastic Optimization&lt;/em&gt; Chapter 7: “Strategic Exploration in Tabular MDPs.” Available at &lt;a href=&quot;https://rltheorybook.github.io/&quot;&gt;https://rltheorybook.github.io/&lt;/a&gt;. &lt;a href=&quot;#fnref:ajks-ch7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:ajks-ch7:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:ajks-ch7:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:limalik&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Li, Ke; Malik, Jitendra. “Learning to Optimize.” arXiv preprint (2016). Available at &lt;a href=&quot;https://arxiv.org/abs/1606.01885&quot;&gt;https://arxiv.org/abs/1606.01885&lt;/a&gt;. &lt;a href=&quot;#fnref:limalik&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="finance" />
      
        <category term="optimization" />
      
        <category term="machine-learning" />
      

      

      
        <summary type="html">If you haven’t taken a look at Part I yet, please take a look at it here. As soon as I finished fleshing out the single period mean-variance model from Part I, I started thinking about more tangible problems that a trading desk may face, specifically (1) where the portfolios need to be dynamically rebalanced, as spreads, liquidity, and funding conditions change over time, and (2) when the risk constraints or funding constraints change and market stressors change, i.e., risk is not only captured via a fixed covariance matrix across the sample. Recently, the hedge funds and investors have flocked toward the Turkish Lira due to the higher interest rate environment in Turkey (as high as 50% at the time of writing this piece!), largely a result of a change in course toward conventional economic approaches in the nation and a relative curtailing of inflation.1 Investors are taking advantage of the classic carry trade, in which money is borrowed for a trade in currency with lower interest rates to maximize gain on a currency in a high interest rate setting while betting that any shifts in the exchange rate do not upset the strategy. As FX in emerging markets is clearly quite a risky asset, and market regimes shift all the time, I decided it might be interesting to model a trading desk’s dynamic optimization problem to reflect this setting. Picking Up from Where We Left Off In short, my analysis in Part I treated each of the optimizers as an arm in a single period mean-variance bandit. For a fixed pair \((\mu,\Sigma)\), the problem was to choose, with as few episodes as possible, among a finite family of algorithms that output portfolios onto a simplex. The meta-distribution over cases \(p = (\mu,\Sigma)\) captured cross-sectional and temporal variation in the opportunity sets, and the main object of interest was the gap \(\mu_{k^{\star}} - \mu_{\hat{k}}\) between the best optimizer and the one that was empirically selected. But now, we’re shifting from single period mean-variance to a dynamic stochastic program. Given that sequential adjustments are now made to the portfolio in response to sequential occurrences, a finite-state Markov decision process (MDP) can be appropriate for the model. Instead of drawing independent problem cases \(p_i \sim \mathcal{P}\) as in Part I, I now observe a single evolving state process \(s_t\) whose components summarize carry premia, volatility, any crash indicators, and also any slack on the balance sheet. In this setting, a trading strategy is then a policy \(\pi\) that maps each state \(s_t\) into a feasible control \(a_t\), for example a vector of currency positions subject to leverage and margin constraints. The objective here is to maximize a discounted risk-adjusted P&amp;amp;L functional as follows: \[V^{\pi}(s_0) = \mathbb{E}^{\pi}\!\left[ \sum_{t=0}^{\infty} \gamma^t r(s_t,a_t) \right]\] where \(\gamma \in (0,1)\) captures the persistence of carry and crash regimes, and the reward \(r(s_t,a_t)\) aggregates the carry, mark-to-market movements, transaction costs, and risk penalties in a bounded way. From the perspective of Part I, this is a shift from comparing a finite list of static optimizers on i.i.d. portfolio problems to comparing policies in a stochastic environment. Similar underlying concentration inequalities still govern how many observations are needed to distinguish near-optimal policies from sub-optimal ones. The difference is that the relevant complexity parameters are now the number of market regimes and control actions, and the effective horizon \(1 / (1 - \gamma)\) that is implied by the persistence of emerging market (EM) FX or credit risk factors instead of the simple cardinality \(K\) of a candidate optimizer family. With this in mind, I place the sample-complexity results of AJKS to a concrete dynamic problem that is controlling an EM FX carry portfolio under leverage and crash risk. The structure of the problem is inspired by the empirical literature on carry trades and currency crashes, which documents regime-dependent downside risk,2 and by work on optimal carry trade portfolios that already treat carry investing as a state-dependent dynamic allocation problem.3 4 The aim of this note is to treat the same economic environment as an unknown MDP and ask how many simulated or historical episodes would be needed for an RL agent to learn a policy whose risk–adjusted value is provably close to that of the best feasible EM FX carry control policy. Model A trading desk that runs an EM FX carry book faces a dynamic stochastic program. The desk chooses a leveraged long-short position in a set of EM currencies against a funding currency, under balance-sheet, margin, and drawdown constraints. The control problem is cast as an MDP, and minmax-optimal sample complexity bounds for discounted control and finite-horizon exploration are used from results from AJKS (I will refer to the RL theory book as AJKS from here on out). 5 6 The bounds are then rewritten in terms of economically meaningful quantities like regime persistence, the number of states needed to track carry and crash risk, and the discretization of position size and leverage. The set-up can now be formalized. Consider a single funding currency, indexed by \(f\), and \(d\) EM currencies, indexed by \(j = 1,\dots,d\). At discrete times \(t = 0,1,2,\dots\), the desk rebalances over a fixed interval \(\Delta t\) (like one week, for example). For each EM currency \(j\) at time \(t\), the following objects are observed: The short risk-free rate of the funding currency \(r_t^f\) The short local collateral rate \(r_{t}^j\) The log spot exchange rate \(S_t^j = \log \text{FX}_t^j\), that is quoted as units of funding currency per unit of EM currency A volatility proxy \(\sigma_t^j\) (for example, implied volatility) A crash-regime variable \(Z_t^j \in \{0,1\}\), where \(Z_t^j = 1\) indicates a stressed or regime that is prone to crashes for currency \(j\). Define the instantaneous carry spread of currency \(j\) over the funding currency as \[c_t^j = r_{t}^j - r_t^f\] At time \(t\) the desk chooses a position vector \(u_t = (u_t^1,\dots,u_t^d)\) in units of notional per unit of capital, where \(u_t^j &amp;gt; 0\) means being long EM currency \(j\) funded in \(f\), and \(u_t^j &amp;lt; 0\) means short EM currency \(j\) and long the funding currency. There is a gross leverage constraint: \[\sum_{j=1}^d \|u_t^j\| \leq L_{\max}\] and a position cap for each of the currencies \(\|u_t^j\| \leq U_{\max}^j\). Over one period, the currency-level log return in the funding currency is \[R_{t+1}^j = S_{t+1}^j - S_t^j + c_t^j \Delta t - \kappa_t^j(u_t^j)\] where \(\kappa_t^j(u_t^j)\) captures trading and funding costs (for example, bid-ask spread, charges on the balance sheet, and basis). At the portfolio level, the unadjusted P&amp;amp;L per unit of capital over one period is \[\Pi_{t+1} = \sum_{j=1}^d u_t^j R_{t+1}^j\] To turn this into a risk-adjusted reward that is compatible with discounted control, fix a risk-aversion parameter \(\lambda &amp;gt; 0\) and define the conditional variance like so: \[v_t(u_t) = \operatorname{Var}\!\left(\Pi_{t+1} \mid \mathcal{F}_t\right)\] where \(\mathcal{F}_t\) is the information at time \(t\). Define the raw risk-adjusted payoff \[X_{t+1} = \Pi_{t+1} - \frac{\lambda}{2} v_t(u_t)\] Because currency returns and carry spreads are bounded in any realistic risk environment, I say there exists a constant \(B &amp;gt; 0\) such that with high probability \(\|X_{t+1}\| \leq B\). For the math to hold, I clip and rescale to obtain the bounded reward \[r_{t+1} = \frac{1}{2} + \frac{1}{2B} \left( \max\{-B, \min\{X_{t+1}, B\}\} \right)\] so that \(r_{t+1} \in [0,1]\) and all policies are compared on a normalized scale. This rescaling does not change the ordering of the policies by risk-adjusted performance. The desk then faces a stochastic program of maximizing the discounted value \[V^{\pi}(s_0) = \mathbb{E}^{\pi}\!\left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_0 \right]\] over policies \(\pi\) mapping states to actions, where \(\gamma \in (0,1)\) is a discount factor. State, action, and discount in financial terms Define the state at time \(t\) as a summary: \[s_t = \big(C_t, \Sigma_t, Z_t, M_t, K_t\big)\] where there is: \(C_t\) stores the discretized carry spreads \(c_t^j\) \(\Sigma_t\) stores discretized volatility proxies \(\sigma_t^j\) \(Z_t\) is the vector of crash regimes \(Z_t^j\) \(M_t\) stores remaining time to any hard horizon (for example, investor capital lock-up) \(K_t\) stores risk and margin information like remaining capacity under VaR limits or drawdown constraints, etc. This state takes values in a finite set \(\mathcal{S}\) once each of the components is discretized into a finite grid. For example, each \(c_t^j\) might be mapped into one of \(G_c\) quantile buckets, each \(\sigma_t^j\) into \(G_{\sigma}\) buckets, and each crash regime \(Z_t^j\) into the two values \(0\) or \(1\). If aggregation is done across the currencies into factor summaries (for example, a global EM carry factor, a volatility factor, and a crash indicator), the resulting state cardinality can be written as so: \[\| \mathcal{S} \| = S_{\text{carry}} \cdot S_{\text{vol}} \cdot S_{\text{crash}} \cdot S_{\text{horizon}} \cdot S_{\text{margin}}\] where each \(S_{\cdot}\) is a design choice of the desk. The action at time \(t\) is the position vector \(u_t\). For the purposes of sample-complexity analysis, discretize \(u_t^j\) onto a finite grid, for instance \[u_t^j \in \{-U_{\max}^j, -U_{\max}^j + \Delta u^j, \dots, U_{\max}^j\}\] and only allow grids to be consistent with the leverage and margin constraints. The action space \(\mathcal{A}\) is then finite with cardinality \(\| \mathcal{A} \| = A_{\text{pos}}\), which depends on the position grid and the number of currencies, as well as the shape of the leverage constraint. The discount factor \(\gamma\) has a financial interpretation. Suppose that the EM carry factor has an approximate exponential autocorrelation with half-life \(H_{\text{half}}\) that is measured in rebalancing periods. That is, if the lag-k autocorrelation of a relevant factor is approximately \(\rho_k \approx \exp(-k / \tau)\) for some persistence scale \(\tau\), then the half-life is defined as \[\frac{1}{2} = \exp\!\left(-\frac{H_{\text{half}}}{\tau}\right)\] and solving for \(H_{\text{half}}\) produces \[H_{\text{half}} = \tau \log 2\] Setting one step to length \(\Delta t\) and choosing a discount factor \[\gamma = \exp\!\left(-\frac{\Delta t}{\tau}\right)\] gives a planning horizon on the order of \[\frac{1}{1 - \gamma} \approx \frac{\tau}{\Delta t}\] So the factor \(1 - \gamma\) that governs the sample complexity can be written in terms of observed carry persistence. With these definitions, EM FX carry control is a discounted MDP with finite state space \(\mathcal{S}\), finite action space \(\mathcal{A}\), bounded reward in \([0,1]\), and discount factor \(\gamma \in (0,1)\). This is exactly the setting of the discounted sample-complexity results in AJKS Chapter 2. 5 Generative-model Sample Complexity in terms of EM FX Now, step back and assume the desk has access to a calibrated generative model of EM FX and carry dynamics. This can be any kind of risk simulator. Given any state-action pair \((s,a)\), the simulator returns \((s&apos;, r) \sim G(\cdot \mid s,a)\) where \(s&apos;\) is the next state and \(r \in [0,1]\) is the normalized reward. As in AJKS, define an empirical MDP \(\hat{\mathcal{M}}\) that has a transition kernel \(\hat{P}\) that is built from \(N\) independent samples for each \((s,a)\). AJKS show that there exist absolute constants such that, for any \(\varepsilon \in (0,1)\) and \(\delta \in (0,1)\), if the number of simulator calls per state-action pair \(N\) satisfies \[N \ge c_0 \cdot \frac{1}{(1 - \gamma)^3} \cdot \frac{\log\!\big(c_1 \| \mathcal{S} \| \| \mathcal{A} \| / \delta\big)}{\varepsilon^2}\] then, with probability at least \(1 - \delta\), the optimal value function \(Q^{\star}\) and the optimal empirical value function \(\hat{Q}^{\star}\) satisfy \[\| Q^{\star} - \hat{Q}^{\star} \|_{\infty} \leq \varepsilon\] and the policy \(\hat{\pi}\) that is optimal in \(\hat{\mathcal{M}}\) satisfies \[\| Q^{\star} - Q^{\hat{\pi}} \|_{\infty} \leq \varepsilon\] given that \(N\) is not too small.5 The total number of simulator calls is then \[N_{\text{total}} = \| \mathcal{S} \| \| \mathcal{A} \| N\] Substituting the inequality for \(N\) into the expression for \(N_{\text{total}}\), I obtain the following sample-complexity bound that is specific to EM. Let \(S = \| \mathcal{S} \|, A = \| \mathcal{A} \|\). Then it should suffice to take \[N_{\text{total}} \ge c \cdot \frac{S A}{(1 - \gamma)^3} \cdot \frac{\log\!\big(c S A / \delta\big)}{\varepsilon^2}\] for a constant \(c\), to make sure that the policy learned by model-based planning on the simulator is \(\varepsilon\)-optimal with probability at least \(1 - \delta\).5 The financial interpretation of each factor is as follows. First, the factor \(S A\) is the effective size of the EM FX control problem. Increasing the granularity of the carry buckets, volatility regimes, crash flags, or margin states increases \(S\). Increasing the granularity of position and leverage grids increases \(A\). These are design choices that would be made by the trading desk and are not fixed constraints. Second, the factor \((1 - \gamma)^{-3}\) measures the difficulty that is added by persistence in carry and crash regimes. Using the relation \[\gamma = \exp\!\left(-\frac{\Delta t}{\tau}\right)\] expand \(1 - \gamma\) using the first-order approximation of the exponential, \[1 - \gamma = 1 - \exp\!\left(-\frac{\Delta t}{\tau}\right) \approx \frac{\Delta t}{\tau}\] when \(\Delta t / \tau\) is small. Then \[\frac{1}{(1 - \gamma)^3} \approx \left( \frac{\tau}{\Delta t} \right)^3\] Thus the sample requirement grows as the cube of the ratio between the carry persistence scale \(\tau\) and the rebalancing interval \(\Delta t\). For weekly rebalancing and assuming an eight-week half-life (for example), this ratio is of order \(8\), and its cube is of order \(512\). Slow-moving regimes make learning an optimal EM carry control policy inherently more sample intensive. Third, the logarithmic factor \(\log(c S A / \delta)\) is relatively mild, but it captures the reliability parameter \(\delta\). Demanding that the learned policy is close to optimal with probability \(1 - \delta = 0.99\) instead of \(0.95\) increases the log factor but does not change the polynomial dependence on \(S\), \(A\), \(\varepsilon\), or \(1 - \gamma\). This theorem is formally identical to the abstract discounted bound in AJKS, but its economics becomes interpretable once \(\gamma\), \(S\), and \(A\) are written in terms of state aggregation, position grids, and half-lives of factors. Episodic Exploration without a Generative Model In many situations the desk cannot query a simulator arbitrarily and only learns from executed positions. In that case it is natural to work with a finite-horizon formulation that is episodic. I consider a horizon of \(H\) rebalancing steps, after which the book is then forced to unwind. Episodes are indexed by \(k = 0,1,\dots,K - 1\), and each episode consists of states and actions \[(s_{k,0}, a_{k,0}, \dots, s_{k,H-1}, a_{k,H-1})\] Assume that each of the episodes starts from the same benchmark state \(s_{0}\) (which could represent an unlevered start). For each episode define the regret of the policy \(\pi_k\) used in that episode as \[\text{Regret}_k = V^{\star}_0(s_0) - V^{\pi_k}_0(s_0)\] where \(V^{\star}_0(s_0)\) is the optimal value starting at \(s_0\), and \(V^{\pi_k}_0(s_0)\) is the value of policy \(\pi_k\).6 The total expected regret after \(K\) episodes is \[\text{Regret}(K) = \mathbb{E}\!\left[ \sum_{k=0}^{K-1} \text{Regret}_k \right]\] In the tabular finite-horizon setting, AJKS analyze the UCB-VI algorithm and show that the regret is bounded by a term of order \[\text{Regret}(K) \leq C_1 H^2 S \sqrt{A K} \log\!\big(C_2 S A H K\big)\] for absolute constants \(C_1\) and \(C_2\).6 Now, in terms of the problem I look at, it can be seen that the average regret per episode satisfies \[\frac{\text{Regret}(K)}{K} \leq C_1 H^2 S \sqrt{\frac{A}{K}} \log\!\big(C_2 S A H K\big)\] The key dependence here is that the shortfall in risk-adjusted value per episode decays on the order of \[H^2 S \sqrt{\frac{A}{K}}\] up to logarithmic factors. To invert the bound, fix a target average regret per episode \(\rho &amp;gt; 0\). I want \[\frac{\text{Regret}(K)}{K} \leq \rho\] Ignoring the logarithmic factors to see the main scaling, impose \[C_1 H^2 S \sqrt{\frac{A}{K}} \leq \rho\] Solve the inequality for \(K\) as follows. \[\begin{aligned} &amp;amp;C_1 H^2 S \sqrt{\frac{A}{K}} \leq \rho \\ &amp;amp;\implies \sqrt{\frac{A}{K}} \leq \frac{\rho}{C_1 H^2 S} \\ &amp;amp;\implies \frac{A}{K} \leq \frac{\rho^2}{C_1^2 H^4 S^2} \\ &amp;amp;\implies K \geq A \cdot \frac{C_1^2 H^4 S^2}{\rho^2} = \left( \frac{C_1 H^2 S \sqrt{A}}{\rho} \right)^2 \end{aligned}\] Thus, to drive the average regret per episode below \(\rho\), the number of episodes must scale to at least the order of \(H^4 S^2 A / \rho^2\). Because \(H\) is proportional to the horizon of the control problem and \(S\) and \(A\) are determined by the state and action discretizations, this gives an explicit trade-off here– decreasing state and action granularity can reduce learning time by large factors. Optimizers as Policies for the Trading Desk Li and Malik propose viewing an optimization algorithm itself as a policy in an MDP, where the “state” consists of past instances, gradients, and objective values, and the “action” is the next update step.7 In this context, I adopt the same perspective from a trading desk’s perspective. An EM carry optimization algorithm is a policy mapping the history of spreads, volatilities, crash indicators, and portfolio P&amp;amp;L to the next position vector. This has two consequences. First, a learned optimizer that is trained on a distribution of EM FX environments is effectively a meta-policy that solves many related MDPs. The sample-complexity theory here that is based on AJKS gives the number of simulated or historical episodes that is required to guarantee that this learned optimizer is near-optimal across that distribution of environments, once the optimizer class is viewed as a parameterized policy class.5 Second, because the optimizer’s internal state can include risk metrics (such as realized variance), its behavior in stressed regimes can be shaped through the reward definition. For example, aggressively penalizing severe drawdowns in the reward (before clipping) may change the shape of the optimal policy in crash or crash-prone regimes without altering the official sample-complexity rate determined. The bounds guarantee that, with sufficient data, a learned optimizer that internalizes these kinds of penalties will behave appropriately across both normal and crash regimes. Notes and References Financial Times, “Traders pour billions of dollars into Turkish lira trade” (July 20, 2024). Available at https://www.ft.com/content/d93d22ff-0c46-4982-874e-0a0b156d140c. &amp;#8617; Brunnermeier, Markus K.; Nagel, Stefan; Pedersen, Lasse H. (2009). “Carry Trades and Currency Crashes.” NBER Macroeconomics Annual 23, 313–347. &amp;#8617; Laborda, Juan; Laborda, Ricardo; Olmo, José (2014). “Optimal currency carry trade strategies.” International Review of Economics &amp;amp; Finance 33, 52–66. &amp;#8617; Chen, Ching-Neng (2022). “Optimal carry trade portfolio choice under regime shifts.” Review of Quantitative Finance and Accounting 59(2), 541–573. &amp;#8617; Agarwal, Alekh; Jiang, Nan; Kakade, Sham; Sun, Wen. Reinforcement Learning and Stochastic Optimization Chapter 2: “Sample Complexity with a Generative Model.” Available at https://rltheorybook.github.io/. &amp;#8617; &amp;#8617;2 &amp;#8617;3 &amp;#8617;4 &amp;#8617;5 Agarwal, Alekh; Jiang, Nan; Kakade, Sham; Sun, Wen. Reinforcement Learning and Stochastic Optimization Chapter 7: “Strategic Exploration in Tabular MDPs.” Available at https://rltheorybook.github.io/. &amp;#8617; &amp;#8617;2 &amp;#8617;3 Li, Ke; Malik, Jitendra. “Learning to Optimize.” arXiv preprint (2016). Available at https://arxiv.org/abs/1606.01885. &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">The Sample Complexity of RL-based Optimizers for Financial Applications (Part I)</title>
      
      
      <link href="https://a-sircar1.github.io/2024/06/06/24-RL-optimizers/" rel="alternate" type="text/html" title="The Sample Complexity of RL-based Optimizers for Financial Applications (Part I)" />
      
      <published>2024-06-06T06:10:56+00:00</published>
      <updated>2024-06-06T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2024/06/06/24-RL-optimizers</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2024/06/06/24-RL-optimizers/">&lt;p&gt;Earlier this year in the spring, I got the chance to take part in Penn’s Directed Reading Program (DRP) in the Math department. I wanted to explore machine learning for time-series data and sequential learning, and eventually settled on reinforcement learning (RL)– a topic I had heard a lot about but knew very little. I knew quantitative researchers and practioners were attempting to leverage reinforcement learning for all sorts of problems in financial forecasting and optimization, but that’s about all I had to go off. So, I decided to spend sometime and take the opportunity to obtain valuable guidance in my efforts to understand RL.&lt;/p&gt;

&lt;p&gt;I read through a couple of chapters in Barto and Sutton’s textbook and then decided to take a look at some of the papers for applied RL in finance. These were interesting, but I kept gravitating back to more theory-oriented RL work– not really because I see myself as a theory person, but because that perspective made it a lot easier for me to connect the algorithms to the kinds of questions I care about in financial economics, like robustness, risk, and when the model’s assumptions actually line up with market data or real-world market problems. In my literature search, a piece that stood out to me was a BAIR article on “Learning to Optimize with RL,” which frames optimization algorithms themselves as policies learned via RL.&lt;sup id=&quot;fnref:bair&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bair&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; In parallel, I worked through parts of an online RL theory book that develops the foundations of Markov decision processes, value functions, and convergence guarantees in a clean, abstract way.&lt;sup id=&quot;fnref:rlbook&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:rlbook&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Together, these are the main readings that gave me a mental picture of RL that actually felt natural from a finance perspective: markets as sequential decision problems under uncertainty, algorithms as dynamic trading rules, and theory as a way to check on when those rules should behave sensibly. For the rest of this note, I build the model using the tools and inspiration from those works.&lt;/p&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In financial settings, reinforcement learning is often introduced as a way to learn trading strategies or hedging rules. Here I use the same language to think instead about learning an optimizer (as in Li et al.) for a family of financial optimization problems.&lt;sup id=&quot;fnref:bair:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bair&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; I intentionally design a minimal set-up: (1) start from a one-period stochastic portfolio problem, (2) reinterpret a finite family of optimizers as actions of an RL agent, (3) derive a simple sample-complexity bound for selecting a nearly optimal optimizer, (4) and finally connect this to a small mean-variance optimization experiment where I empirically validate the theoretical result.&lt;/p&gt;

&lt;h3&gt;Portfolio Setup and Mean-variance Utility&lt;/h3&gt;

&lt;p&gt;Let’s take \(d \ge 1\) assets, and let the random vector of one-period returns be \(R \in \mathbb{R}^d\). A portfolio is a weight vector \(w \in \mathbb{R}^d\) that satisfies the budget and non-negativity constraints like so:&lt;/p&gt;

\[\sum_{j=1}^d w_j = 1\]

\[w_j \ge 0 \quad  \forall j\]

&lt;p&gt;The random portfolio return is then \(X(w) = w^{\top} R\). The mean vector and covariance matrix of returns can be written as \(\mu = \mathbb{E}[R] \in \mathbb{R}^d\) and \(\Sigma = \operatorname{Cov}(R) \in \mathbb{R}^{d \times d}\).&lt;/p&gt;

&lt;p&gt;Consider the standard mean-variance objective with risk-aversion parameter \(\lambda &amp;gt; 0\):&lt;/p&gt;

\[U(w) = \mathbb{E}[X(w)] - \frac{\lambda}{2} \operatorname{Var}(X(w))\]

&lt;p&gt;The expectation can be written explicitly as&lt;/p&gt;

\[\begin{aligned}
\mathbb{E}[X(w)]
&amp;amp;= \mathbb{E}[w^{\top} R] \\
&amp;amp;= w^{\top} \mathbb{E}[R] \\
&amp;amp;= w^{\top} \mu
\end{aligned}\]

&lt;p&gt;because \(w\) is deterministic. For the variance, start from the covariance definition \(\Sigma = \mathbb{E}[(R - \mu)(R - \mu)^{\top}]\). Define the centered scalar \(Y = w^{\top} R - w^{\top} \mu = w^{\top}(R - \mu)\), so that&lt;/p&gt;

\[\operatorname{Var}(X(w)) = \mathbb{E}[Y^2]\]

&lt;p&gt;Now expand \(Y^2\) and rewrite it as a quadratic form like so:&lt;/p&gt;

\[\begin{aligned}
Y^2
&amp;amp;= \big(w^{\top}(R - \mu)\big)^2 \\
&amp;amp;= \big(w^{\top}(R - \mu)\big)\big(w^{\top}(R - \mu)\big) \\
&amp;amp;= w^{\top}(R - \mu)(R - \mu)^{\top} w
\end{aligned}\]

&lt;p&gt;Taking expectations and pulling out the deterministic weights gives the following:&lt;/p&gt;

\[\mathbb{E}[Y^2]
= \mathbb{E}\!\big[w^{\top}(R - \mu)(R - \mu)^{\top} w\big]
= w^{\top} \mathbb{E}\!\big[(R - \mu)(R - \mu)^{\top}\big] w
= w^{\top} \Sigma w\]

&lt;p&gt;Thus the variance is \(\operatorname{Var}(X(w)) = w^{\top} \Sigma w\). Plugging in both of the pieces into the utility, I get:&lt;/p&gt;

\[U(w) = w^{\top} \mu - \frac{\lambda}{2} w^{\top} \Sigma w\]

&lt;p&gt;For a fixed pair \((\mu,\Sigma)\), this is a concave quadratic function of \(w\) on the simplex of admissible portfolios \(\mathcal{W}\). Many portfolio problems can generally be seen as variants of this kind of form. For convenience, write \(U(w, \mu, \Sigma) = w^{\top} \mu - \frac{\lambda}{2} w^{\top} \Sigma w\) and think of \((\mu,\Sigma)\) as the parameters that define a particular optimization parameterization \(p = (\mu, \Sigma)\). The corresponding optimal utility is&lt;/p&gt;

\[U^{\star}(p) = \max_{w \in \mathcal{W}} U(w, \mu, \Sigma)\]

&lt;p&gt;In practice, \(\mu\) and \(\Sigma\) are unknown as only samples can be obtained and not the population-level values. Different universes or constraints give different parameterizations \(p\). A meta-optimizer should perform well on average across a distribution of these kinds of parameterizations.&lt;/p&gt;

&lt;h3&gt;A Finite Family of Optimizers as Actions&lt;/h3&gt;

&lt;p&gt;Instead of tuning a single algorithm by hand, now imagine a finite family of \(K\) candidate optimizers,&lt;/p&gt;

\[\mathcal{O} = \{O_1, O_2, \dots, O_K\}\]

&lt;p&gt;where each of the optimizers \(O_k\) takes as input data for a given parameterization \(p\) and outputs a portfolio \(w_k\). For example, \(O_1\) could be projected gradient ascent with a small step size, \(O_2\) could be the same algorithm with a larger step size, and this can go on. To connect this to RL cleanly, consider a simple sequential environment. Say there is an unknown “meta-distribution” \(\mathcal{P}\) over problem parameterizations \(p = (\mu,\Sigma)\). In each case \(i\), nature draws a fresh parameterization&lt;/p&gt;

\[p_i = (\mu_i, \Sigma_i) \sim \mathcal{P}\]

&lt;p&gt;the agent chooses an action \(A_i \in \{1,\dots,K\}\), and action \(A_i = k\) means the optimizer \(O_k\) is applied to problem \(p_i\). Optimizer \(O_k\) outputs a portfolio \(w_{k,i} = O_k(p_i)\), and then out-of-sample (OOS) performance \(R_i = R_{k}(p_i)\) can be evaluated, where \(R_k(p)\) is some reward based on \(U(w_k, \mu, \Sigma)\).&lt;/p&gt;

&lt;p&gt;For the analysis, the rewards are assumed bounded in \([0,1]\):&lt;/p&gt;

\[0 \le R_k(p) \le 1 \quad \forall k, p\]

&lt;p&gt;Note that this is not restrictive– any utility that is real-values should be able to be clipped to \([a,b]\) and then rescaled linearly to \([0,1]\) without changing the ordering of the optimizers. The mean performance of the optimizer \(k\) under the meta-distribution \(\mathcal{P}\) is \(\mu_k = \mathbb{E}_{p \sim \mathcal{P}}[R_k(p)]\), and an index of a best optimizer is \(\mu_{k^{\star}} = \max_{1 \le k \le K} \mu_k\).&lt;/p&gt;

&lt;p&gt;The meta-learning problem is then to use samples from cases across \(i\) to select an index \(\hat{k}\) such that \(\mu_{\hat{k}}\) is close to \(\mu_{k^{\star}}\). This is exactly a stochastic multi-armed bandit in the exploration setting. Each optimizer is an arm and then the reward of arm \(k\) is the random value \(R_k(p)\) when \(p \sim \mathcal{P}\), and \(\mu_k\) is its mean. The only difference from the classical bandit model is that there is a financial interpretation of the reward.&lt;/p&gt;

&lt;h3&gt;A Simple Meta-algorithm and its Uniform Deviation Bound&lt;/h3&gt;

&lt;p&gt;For the analysis, consider the following strategy. For each optimizer \(k\), collect \(n\) independent rewards \(R_{k,1}, R_{k,2}, \dots, R_{k,n}\), where each of the rewards \(R_{k,j}\) is generated by drawing a fresh problem parameterization \(p_{k,j} \sim \mathcal{P}\), running optimizer \(O_k\), and computing its bounded reward. The empirical mean reward of optimizer \(k\) is then the following:&lt;/p&gt;

\[\hat{\mu}_k = \frac{1}{n} \sum_{j=1}^n R_{k,j}\]

&lt;p&gt;and at the end choose \(\hat{k} = \arg\max_{1 \le k \le K} \hat{\mu}_k\) with and break ties arbitarily. The question then is how large \(n\) must be so that we can be sure that, with high probability, the gap \(\mu_{k^{\star}} - \mu_{\hat{k}}\) is small. Use Hoeffding’s inequality for bounded random variables. For a fixed optimizer \(k\), the rewards \(R_{k,1},\dots,R_{k,n}\) are independent and lie in \([0,1]\). Writing \(\hat{\mu}_k = \frac{1}{n} \sum_{j=1}^n R_{k,j}\) and \(\mu_k = \mathbb{E}[R_{k,1}]\), Hoeffding’s inequality states that for every \(\varepsilon &amp;gt; 0\),&lt;/p&gt;

\[\mathbb{P}\big(\|\hat{\mu}_k - \mu_k\| \ge \varepsilon\big) \le 2 \exp(-2 n \varepsilon^2)\]

&lt;p&gt;For each \(k\), then define the deviation event \(E_k = \{ \|\hat{\mu}_k - \mu_k\| \ge \varepsilon \}\), so that \(\mathbb{P}(E_k) \le 2 \exp(-2 n \varepsilon^2)\). The event that some optimizer has a deviation at least \(\varepsilon\) is the union \(\bigcup_{k=1}^K E_k\), and by the union bound,&lt;/p&gt;

\[\mathbb{P}\!\left(\bigcup_{k=1}^K E_k\right) \le 2 K \exp(-2 n \varepsilon^2)\]

&lt;p&gt;Equivalently, with probability at least \(1 - 2 K \exp(-2 n \varepsilon^2)\) there is the event&lt;/p&gt;

\[\mathcal{E} = \{ \|\hat{\mu}_k - \mu_k\| &amp;lt; \varepsilon \quad \forall k = \{1,\dots,K \}\]

&lt;p&gt;meaning that every empirical mean is within \(\varepsilon\) of its true mean.&lt;/p&gt;

&lt;p&gt;On event \(\mathcal{E}\), the optimizer chosen by empirical means is at most \(2 \varepsilon\) worse than the best optimizer in terms of the true mean reward. To see this, fix an outcome where \(\mathcal{E}\) holds. By definition of \(k^{\star}\), \(\mu_{k^{\star}} = \max_{1 \le k \le K} \mu_k\). On \(\mathcal{E}\), the empirical mean of optimizer \(k^{\star}\) satisfies \(\|\hat{\mu}_{k^{\star}} - \mu_{k^{\star}}\| &amp;lt; \varepsilon\), and so \(\mu_{k^{\star}} - \hat{\mu}_{k^{\star}} \le \varepsilon\). For any other optimizer \(k\), the same event then gives \(\|\hat{\mu}_k - \mu_k\| &amp;lt; \varepsilon \Rightarrow \mu_k \le \hat{\mu}_k + \varepsilon\). By construction, \(\hat{k}\) maximizes the empirical means, so \(\hat{\mu}_{\hat{k}} \ge \hat{\mu}_{k^{\star}}\).&lt;/p&gt;

&lt;p&gt;Now decompose the performance gap as&lt;/p&gt;

\[\mu_{k^{\star}} - \mu_{\hat{k}} = (\mu_{k^{\star}} - \hat{\mu}_{k^{\star}}) + (\hat{\mu}_{k^{\star}} - \hat{\mu}_{\hat{k}}) + (\hat{\mu}_{\hat{k}} - \mu_{\hat{k}})\]

&lt;p&gt;On \(\mathcal{E}\), the three terms satisfy \(\mu_{k^{\star}} - \hat{\mu}_{k^{\star}} \le \varepsilon\), \(\hat{\mu}_{k^{\star}} - \hat{\mu}_{\hat{k}} \le 0\), and \(\hat{\mu}_{\hat{k}} - \mu_{\hat{k}} \le \varepsilon\), so&lt;/p&gt;

\[\mu_{k^{\star}} - \mu_{\hat{k}} \le \varepsilon + 0 + \varepsilon = 2 \varepsilon\]

&lt;p&gt;Therefore&lt;/p&gt;

\[\mathbb{P}\big(\mu_{k^{\star}} - \mu_{\hat{k}} &amp;gt; 2 \varepsilon\big)
\le 2 K \exp(-2 n \varepsilon^2)\]

&lt;p&gt;This inequality gives a clean probabilistic guarantee in that the probability that the optimizer selected by empirical mean performance is more than \(2 \varepsilon\) worse than the best optimizer is at most \(2 K \exp(-2 n \varepsilon^2)\).&lt;/p&gt;

&lt;h3&gt;Sample Complexity in Terms of \(\varepsilon\) and \(\delta\)&lt;/h3&gt;

&lt;p&gt;Next, to invert the bound, fix a target confidence level \(1 - \delta\) with \(0 &amp;lt; \delta &amp;lt; 1\).  The goal is to get \(2 K \exp(-2 n \varepsilon^2) \le \delta\). First, start with \(2 K \exp(-2 n \varepsilon^2) \le \delta \Rightarrow \exp(-2 n \varepsilon^2) \le \delta/(2 K)\). Taking natural logs and using the fact that the exponential is monotonic, find that \(-2 n \varepsilon^2 \le \log(\delta/(2 K))\), and multiplying by \(-1\), the inequality gets reversed so \(2 n \varepsilon^2 \ge -\log(\delta/(2 K)) = \log(2 K/\delta)\). Dividing both sides by \(2 \varepsilon^2\) finally gives:&lt;/p&gt;

\[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)\]

&lt;p&gt;Thus, if&lt;/p&gt;

\[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)\]

&lt;p&gt;cases per optimizer and the optimizer with largest empirical mean is selected, then with probability at least \(1 - \delta\) the chosen optimizer \(\hat{k}\) satisfies \(\mu_{k^{\star}} - \mu_{\hat{k}} \le 2 \varepsilon\). The total number of episodes is \(K n\), and the dependence on the number of optimizers is only logarithmic through the factor \(\log(2 K / \delta)\).&lt;/p&gt;

&lt;p&gt;For a small numerical example, consider \(K = 4\), \(\delta = 0.05\), \(\varepsilon = 0.1\). Calculate each component explicitly like so: \(2 K = 2 \times 4 = 8\), \(\frac{2 K}{\delta} = \frac{8}{0.05} = 160\), \(\log\!\left(\frac{2 K}{\delta}\right) = \log(160) \approx 5.0752\). Next, \(\varepsilon^2 = (0.1)^2 = 0.01\), \(2 \varepsilon^2 = 2 \times 0.01 = 0.02\), \(\frac{1}{2 \varepsilon^2} = \frac{1}{0.02} = 50\). Then, putting this together:&lt;/p&gt;

\[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)
\approx 50 \times 5.0752
\approx 253.76\]

&lt;p&gt;so it should be enough to take \(n = 254\) cases per optimizer. The resulting total sample size is \(K n = 4 \times 254 = 1016\). Under such a choice, the bound guarantees that the optimizer selected by the empirical mean performance is within about \(0.2\) of the best possible mean reward, with probability at least \(95\%\).&lt;/p&gt;

&lt;h3&gt;Mean-variance Experiment and Empirical Results&lt;/h3&gt;

&lt;p&gt;To experiment, I implement the meta-optimization set-up synthetically in a world with \(d=5\) assets and four candidate optimizers that are given by projected gradient ascent with step sizes \(\eta \in \{0.01, 0.03, 0.08, 0.15\}\). For each of the problems \(p = (\mu,\Sigma)\), all four of the optimizers are run, their mean–variance utilities are computed, and then I linearly rescale them to \([0,1]\) so that the best optimizer on that problem receives a reward of \(1\), and the worst receives a reward of \(0\). To avoid a degenerate and basically deterministic situation where the best optimizer is almost always identified perfectly, and to better mimic noisy OOS evaluation during meta-learning, I add a small Gaussian perturbation to this rescaled utility and then clip it back to \([0,1]\). This maintains the assumption that rewards are bounded in the analysis with the Hoeffding bound, but it introduces a nontrivial chance of not selecting the best optimizer when \(n\) is small.&lt;/p&gt;

&lt;p&gt;On a large independent test set of problems, the estimated mean rewards of the four optimizers are \((\mu_0,\mu_1,\mu_2,\mu_3) \approx (0.24, 0.47, 0.69, 0.76)\), so the \(\eta = 0.15\) optimizer is best but not far better than \(\eta = 0.08\). For each sample size \(n\), I draw \(n\) new problems per optimizer, compute the empirical means \(\hat{\mu}*k\), choose \(\hat{k} = \arg\max_k \hat{\mu}*k\), and write down the realized performance gap \(\mu*{k^\star} - \mu*{\hat{k}}\). Repeating this 60 times per \(n\) produces an empirical distribution of the gap, which I then compare to the tolerance obtained via the Hoeffding bound:&lt;/p&gt;

\[2\varepsilon_n = 2\sqrt{\frac{1}{2n}\log\left(\frac{2K}{\delta}\right)}
\quad\text{with } K = 4,\ \delta = 0.05\]

&lt;p&gt;The figure below plots the mean gap, its 95th percentile, and the theoretical \(2\varepsilon_n\) curve as functions of \(n\) on a semi-logarithmic scale:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/RL-optimizers-plot1.png&quot; alt=&quot;Meta-optimizer Selection Experiment in the Toy Market&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The exact values are in the table below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Episodes per optimizer \(n\)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Mean gap \(\mathbb{E}[\mu_{k^\star} - \mu_{\hat{k}}]\)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;95th pct. gap&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Theoretical \(2\varepsilon_n\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0496&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2900&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.5930&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0290&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0726&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1264&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0278&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0726&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7965&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0157&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0726&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5632&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;64&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0060&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0726&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.3982&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;128&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0012&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0000&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2816&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;256&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0000&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0000&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1991&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;512&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0000&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0000&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.1408&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Even with the noisy rewards, the mean gap goes toward zero as \(n\) increases, and the 95th-percentile gap decays on the same \(1/\sqrt{n}\) scale as the theoretical \(2\varepsilon_n\) line while staying below it. This indicates that the Hoeffding bound is conservative but also accurate for this meta-optimization problem. More broadly, it can be seen that classical concentration inequalities already give useful sample-complexity guarantees for optimizer selection using RL, as demonstrated in the stylized financial setting here.&lt;/p&gt;

&lt;h3&gt;Notes and References&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:bair&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;BAIR Blog, “Learning to Optimize with RL” (2017). Available at &lt;a href=&quot;https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/&quot;&gt;https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/&lt;/a&gt;. &lt;a href=&quot;#fnref:bair&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:bair:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rlbook&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;em&gt;Reinforcement Learning and Stochastic Optimization&lt;/em&gt; (RL Theory Book). Available at &lt;a href=&quot;https://rltheorybook.github.io/&quot;&gt;https://rltheorybook.github.io/&lt;/a&gt;. &lt;a href=&quot;#fnref:rlbook&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="finance" />
      
        <category term="optimization" />
      
        <category term="machine-learning" />
      

      

      
        <summary type="html">Earlier this year in the spring, I got the chance to take part in Penn’s Directed Reading Program (DRP) in the Math department. I wanted to explore machine learning for time-series data and sequential learning, and eventually settled on reinforcement learning (RL)– a topic I had heard a lot about but knew very little. I knew quantitative researchers and practioners were attempting to leverage reinforcement learning for all sorts of problems in financial forecasting and optimization, but that’s about all I had to go off. So, I decided to spend sometime and take the opportunity to obtain valuable guidance in my efforts to understand RL. I read through a couple of chapters in Barto and Sutton’s textbook and then decided to take a look at some of the papers for applied RL in finance. These were interesting, but I kept gravitating back to more theory-oriented RL work– not really because I see myself as a theory person, but because that perspective made it a lot easier for me to connect the algorithms to the kinds of questions I care about in financial economics, like robustness, risk, and when the model’s assumptions actually line up with market data or real-world market problems. In my literature search, a piece that stood out to me was a BAIR article on “Learning to Optimize with RL,” which frames optimization algorithms themselves as policies learned via RL.1 In parallel, I worked through parts of an online RL theory book that develops the foundations of Markov decision processes, value functions, and convergence guarantees in a clean, abstract way.2 Together, these are the main readings that gave me a mental picture of RL that actually felt natural from a finance perspective: markets as sequential decision problems under uncertainty, algorithms as dynamic trading rules, and theory as a way to check on when those rules should behave sensibly. For the rest of this note, I build the model using the tools and inspiration from those works. Introduction In financial settings, reinforcement learning is often introduced as a way to learn trading strategies or hedging rules. Here I use the same language to think instead about learning an optimizer (as in Li et al.) for a family of financial optimization problems.1 I intentionally design a minimal set-up: (1) start from a one-period stochastic portfolio problem, (2) reinterpret a finite family of optimizers as actions of an RL agent, (3) derive a simple sample-complexity bound for selecting a nearly optimal optimizer, (4) and finally connect this to a small mean-variance optimization experiment where I empirically validate the theoretical result. Portfolio Setup and Mean-variance Utility Let’s take \(d \ge 1\) assets, and let the random vector of one-period returns be \(R \in \mathbb{R}^d\). A portfolio is a weight vector \(w \in \mathbb{R}^d\) that satisfies the budget and non-negativity constraints like so: \[\sum_{j=1}^d w_j = 1\] \[w_j \ge 0 \quad \forall j\] The random portfolio return is then \(X(w) = w^{\top} R\). The mean vector and covariance matrix of returns can be written as \(\mu = \mathbb{E}[R] \in \mathbb{R}^d\) and \(\Sigma = \operatorname{Cov}(R) \in \mathbb{R}^{d \times d}\). Consider the standard mean-variance objective with risk-aversion parameter \(\lambda &amp;gt; 0\): \[U(w) = \mathbb{E}[X(w)] - \frac{\lambda}{2} \operatorname{Var}(X(w))\] The expectation can be written explicitly as \[\begin{aligned} \mathbb{E}[X(w)] &amp;amp;= \mathbb{E}[w^{\top} R] \\ &amp;amp;= w^{\top} \mathbb{E}[R] \\ &amp;amp;= w^{\top} \mu \end{aligned}\] because \(w\) is deterministic. For the variance, start from the covariance definition \(\Sigma = \mathbb{E}[(R - \mu)(R - \mu)^{\top}]\). Define the centered scalar \(Y = w^{\top} R - w^{\top} \mu = w^{\top}(R - \mu)\), so that \[\operatorname{Var}(X(w)) = \mathbb{E}[Y^2]\] Now expand \(Y^2\) and rewrite it as a quadratic form like so: \[\begin{aligned} Y^2 &amp;amp;= \big(w^{\top}(R - \mu)\big)^2 \\ &amp;amp;= \big(w^{\top}(R - \mu)\big)\big(w^{\top}(R - \mu)\big) \\ &amp;amp;= w^{\top}(R - \mu)(R - \mu)^{\top} w \end{aligned}\] Taking expectations and pulling out the deterministic weights gives the following: \[\mathbb{E}[Y^2] = \mathbb{E}\!\big[w^{\top}(R - \mu)(R - \mu)^{\top} w\big] = w^{\top} \mathbb{E}\!\big[(R - \mu)(R - \mu)^{\top}\big] w = w^{\top} \Sigma w\] Thus the variance is \(\operatorname{Var}(X(w)) = w^{\top} \Sigma w\). Plugging in both of the pieces into the utility, I get: \[U(w) = w^{\top} \mu - \frac{\lambda}{2} w^{\top} \Sigma w\] For a fixed pair \((\mu,\Sigma)\), this is a concave quadratic function of \(w\) on the simplex of admissible portfolios \(\mathcal{W}\). Many portfolio problems can generally be seen as variants of this kind of form. For convenience, write \(U(w, \mu, \Sigma) = w^{\top} \mu - \frac{\lambda}{2} w^{\top} \Sigma w\) and think of \((\mu,\Sigma)\) as the parameters that define a particular optimization parameterization \(p = (\mu, \Sigma)\). The corresponding optimal utility is \[U^{\star}(p) = \max_{w \in \mathcal{W}} U(w, \mu, \Sigma)\] In practice, \(\mu\) and \(\Sigma\) are unknown as only samples can be obtained and not the population-level values. Different universes or constraints give different parameterizations \(p\). A meta-optimizer should perform well on average across a distribution of these kinds of parameterizations. A Finite Family of Optimizers as Actions Instead of tuning a single algorithm by hand, now imagine a finite family of \(K\) candidate optimizers, \[\mathcal{O} = \{O_1, O_2, \dots, O_K\}\] where each of the optimizers \(O_k\) takes as input data for a given parameterization \(p\) and outputs a portfolio \(w_k\). For example, \(O_1\) could be projected gradient ascent with a small step size, \(O_2\) could be the same algorithm with a larger step size, and this can go on. To connect this to RL cleanly, consider a simple sequential environment. Say there is an unknown “meta-distribution” \(\mathcal{P}\) over problem parameterizations \(p = (\mu,\Sigma)\). In each case \(i\), nature draws a fresh parameterization \[p_i = (\mu_i, \Sigma_i) \sim \mathcal{P}\] the agent chooses an action \(A_i \in \{1,\dots,K\}\), and action \(A_i = k\) means the optimizer \(O_k\) is applied to problem \(p_i\). Optimizer \(O_k\) outputs a portfolio \(w_{k,i} = O_k(p_i)\), and then out-of-sample (OOS) performance \(R_i = R_{k}(p_i)\) can be evaluated, where \(R_k(p)\) is some reward based on \(U(w_k, \mu, \Sigma)\). For the analysis, the rewards are assumed bounded in \([0,1]\): \[0 \le R_k(p) \le 1 \quad \forall k, p\] Note that this is not restrictive– any utility that is real-values should be able to be clipped to \([a,b]\) and then rescaled linearly to \([0,1]\) without changing the ordering of the optimizers. The mean performance of the optimizer \(k\) under the meta-distribution \(\mathcal{P}\) is \(\mu_k = \mathbb{E}_{p \sim \mathcal{P}}[R_k(p)]\), and an index of a best optimizer is \(\mu_{k^{\star}} = \max_{1 \le k \le K} \mu_k\). The meta-learning problem is then to use samples from cases across \(i\) to select an index \(\hat{k}\) such that \(\mu_{\hat{k}}\) is close to \(\mu_{k^{\star}}\). This is exactly a stochastic multi-armed bandit in the exploration setting. Each optimizer is an arm and then the reward of arm \(k\) is the random value \(R_k(p)\) when \(p \sim \mathcal{P}\), and \(\mu_k\) is its mean. The only difference from the classical bandit model is that there is a financial interpretation of the reward. A Simple Meta-algorithm and its Uniform Deviation Bound For the analysis, consider the following strategy. For each optimizer \(k\), collect \(n\) independent rewards \(R_{k,1}, R_{k,2}, \dots, R_{k,n}\), where each of the rewards \(R_{k,j}\) is generated by drawing a fresh problem parameterization \(p_{k,j} \sim \mathcal{P}\), running optimizer \(O_k\), and computing its bounded reward. The empirical mean reward of optimizer \(k\) is then the following: \[\hat{\mu}_k = \frac{1}{n} \sum_{j=1}^n R_{k,j}\] and at the end choose \(\hat{k} = \arg\max_{1 \le k \le K} \hat{\mu}_k\) with and break ties arbitarily. The question then is how large \(n\) must be so that we can be sure that, with high probability, the gap \(\mu_{k^{\star}} - \mu_{\hat{k}}\) is small. Use Hoeffding’s inequality for bounded random variables. For a fixed optimizer \(k\), the rewards \(R_{k,1},\dots,R_{k,n}\) are independent and lie in \([0,1]\). Writing \(\hat{\mu}_k = \frac{1}{n} \sum_{j=1}^n R_{k,j}\) and \(\mu_k = \mathbb{E}[R_{k,1}]\), Hoeffding’s inequality states that for every \(\varepsilon &amp;gt; 0\), \[\mathbb{P}\big(\|\hat{\mu}_k - \mu_k\| \ge \varepsilon\big) \le 2 \exp(-2 n \varepsilon^2)\] For each \(k\), then define the deviation event \(E_k = \{ \|\hat{\mu}_k - \mu_k\| \ge \varepsilon \}\), so that \(\mathbb{P}(E_k) \le 2 \exp(-2 n \varepsilon^2)\). The event that some optimizer has a deviation at least \(\varepsilon\) is the union \(\bigcup_{k=1}^K E_k\), and by the union bound, \[\mathbb{P}\!\left(\bigcup_{k=1}^K E_k\right) \le 2 K \exp(-2 n \varepsilon^2)\] Equivalently, with probability at least \(1 - 2 K \exp(-2 n \varepsilon^2)\) there is the event \[\mathcal{E} = \{ \|\hat{\mu}_k - \mu_k\| &amp;lt; \varepsilon \quad \forall k = \{1,\dots,K \}\] meaning that every empirical mean is within \(\varepsilon\) of its true mean. On event \(\mathcal{E}\), the optimizer chosen by empirical means is at most \(2 \varepsilon\) worse than the best optimizer in terms of the true mean reward. To see this, fix an outcome where \(\mathcal{E}\) holds. By definition of \(k^{\star}\), \(\mu_{k^{\star}} = \max_{1 \le k \le K} \mu_k\). On \(\mathcal{E}\), the empirical mean of optimizer \(k^{\star}\) satisfies \(\|\hat{\mu}_{k^{\star}} - \mu_{k^{\star}}\| &amp;lt; \varepsilon\), and so \(\mu_{k^{\star}} - \hat{\mu}_{k^{\star}} \le \varepsilon\). For any other optimizer \(k\), the same event then gives \(\|\hat{\mu}_k - \mu_k\| &amp;lt; \varepsilon \Rightarrow \mu_k \le \hat{\mu}_k + \varepsilon\). By construction, \(\hat{k}\) maximizes the empirical means, so \(\hat{\mu}_{\hat{k}} \ge \hat{\mu}_{k^{\star}}\). Now decompose the performance gap as \[\mu_{k^{\star}} - \mu_{\hat{k}} = (\mu_{k^{\star}} - \hat{\mu}_{k^{\star}}) + (\hat{\mu}_{k^{\star}} - \hat{\mu}_{\hat{k}}) + (\hat{\mu}_{\hat{k}} - \mu_{\hat{k}})\] On \(\mathcal{E}\), the three terms satisfy \(\mu_{k^{\star}} - \hat{\mu}_{k^{\star}} \le \varepsilon\), \(\hat{\mu}_{k^{\star}} - \hat{\mu}_{\hat{k}} \le 0\), and \(\hat{\mu}_{\hat{k}} - \mu_{\hat{k}} \le \varepsilon\), so \[\mu_{k^{\star}} - \mu_{\hat{k}} \le \varepsilon + 0 + \varepsilon = 2 \varepsilon\] Therefore \[\mathbb{P}\big(\mu_{k^{\star}} - \mu_{\hat{k}} &amp;gt; 2 \varepsilon\big) \le 2 K \exp(-2 n \varepsilon^2)\] This inequality gives a clean probabilistic guarantee in that the probability that the optimizer selected by empirical mean performance is more than \(2 \varepsilon\) worse than the best optimizer is at most \(2 K \exp(-2 n \varepsilon^2)\). Sample Complexity in Terms of \(\varepsilon\) and \(\delta\) Next, to invert the bound, fix a target confidence level \(1 - \delta\) with \(0 &amp;lt; \delta &amp;lt; 1\). The goal is to get \(2 K \exp(-2 n \varepsilon^2) \le \delta\). First, start with \(2 K \exp(-2 n \varepsilon^2) \le \delta \Rightarrow \exp(-2 n \varepsilon^2) \le \delta/(2 K)\). Taking natural logs and using the fact that the exponential is monotonic, find that \(-2 n \varepsilon^2 \le \log(\delta/(2 K))\), and multiplying by \(-1\), the inequality gets reversed so \(2 n \varepsilon^2 \ge -\log(\delta/(2 K)) = \log(2 K/\delta)\). Dividing both sides by \(2 \varepsilon^2\) finally gives: \[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)\] Thus, if \[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right)\] cases per optimizer and the optimizer with largest empirical mean is selected, then with probability at least \(1 - \delta\) the chosen optimizer \(\hat{k}\) satisfies \(\mu_{k^{\star}} - \mu_{\hat{k}} \le 2 \varepsilon\). The total number of episodes is \(K n\), and the dependence on the number of optimizers is only logarithmic through the factor \(\log(2 K / \delta)\). For a small numerical example, consider \(K = 4\), \(\delta = 0.05\), \(\varepsilon = 0.1\). Calculate each component explicitly like so: \(2 K = 2 \times 4 = 8\), \(\frac{2 K}{\delta} = \frac{8}{0.05} = 160\), \(\log\!\left(\frac{2 K}{\delta}\right) = \log(160) \approx 5.0752\). Next, \(\varepsilon^2 = (0.1)^2 = 0.01\), \(2 \varepsilon^2 = 2 \times 0.01 = 0.02\), \(\frac{1}{2 \varepsilon^2} = \frac{1}{0.02} = 50\). Then, putting this together: \[n \ge \frac{1}{2 \varepsilon^2} \log\!\left(\frac{2 K}{\delta}\right) \approx 50 \times 5.0752 \approx 253.76\] so it should be enough to take \(n = 254\) cases per optimizer. The resulting total sample size is \(K n = 4 \times 254 = 1016\). Under such a choice, the bound guarantees that the optimizer selected by the empirical mean performance is within about \(0.2\) of the best possible mean reward, with probability at least \(95\%\). Mean-variance Experiment and Empirical Results To experiment, I implement the meta-optimization set-up synthetically in a world with \(d=5\) assets and four candidate optimizers that are given by projected gradient ascent with step sizes \(\eta \in \{0.01, 0.03, 0.08, 0.15\}\). For each of the problems \(p = (\mu,\Sigma)\), all four of the optimizers are run, their mean–variance utilities are computed, and then I linearly rescale them to \([0,1]\) so that the best optimizer on that problem receives a reward of \(1\), and the worst receives a reward of \(0\). To avoid a degenerate and basically deterministic situation where the best optimizer is almost always identified perfectly, and to better mimic noisy OOS evaluation during meta-learning, I add a small Gaussian perturbation to this rescaled utility and then clip it back to \([0,1]\). This maintains the assumption that rewards are bounded in the analysis with the Hoeffding bound, but it introduces a nontrivial chance of not selecting the best optimizer when \(n\) is small. On a large independent test set of problems, the estimated mean rewards of the four optimizers are \((\mu_0,\mu_1,\mu_2,\mu_3) \approx (0.24, 0.47, 0.69, 0.76)\), so the \(\eta = 0.15\) optimizer is best but not far better than \(\eta = 0.08\). For each sample size \(n\), I draw \(n\) new problems per optimizer, compute the empirical means \(\hat{\mu}*k\), choose \(\hat{k} = \arg\max_k \hat{\mu}*k\), and write down the realized performance gap \(\mu*{k^\star} - \mu*{\hat{k}}\). Repeating this 60 times per \(n\) produces an empirical distribution of the gap, which I then compare to the tolerance obtained via the Hoeffding bound: \[2\varepsilon_n = 2\sqrt{\frac{1}{2n}\log\left(\frac{2K}{\delta}\right)} \quad\text{with } K = 4,\ \delta = 0.05\] The figure below plots the mean gap, its 95th percentile, and the theoretical \(2\varepsilon_n\) curve as functions of \(n\) on a semi-logarithmic scale: The exact values are in the table below: Episodes per optimizer \(n\) Mean gap \(\mathbb{E}[\mu_{k^\star} - \mu_{\hat{k}}]\) 95th pct. gap Theoretical \(2\varepsilon_n\) 4 0.0496 0.2900 1.5930 8 0.0290 0.0726 1.1264 16 0.0278 0.0726 0.7965 32 0.0157 0.0726 0.5632 64 0.0060 0.0726 0.3982 128 0.0012 0.0000 0.2816 256 0.0000 0.0000 0.1991 512 0.0000 0.0000 0.1408 Even with the noisy rewards, the mean gap goes toward zero as \(n\) increases, and the 95th-percentile gap decays on the same \(1/\sqrt{n}\) scale as the theoretical \(2\varepsilon_n\) line while staying below it. This indicates that the Hoeffding bound is conservative but also accurate for this meta-optimization problem. More broadly, it can be seen that classical concentration inequalities already give useful sample-complexity guarantees for optimizer selection using RL, as demonstrated in the stylized financial setting here. Notes and References BAIR Blog, “Learning to Optimize with RL” (2017). Available at https://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/. &amp;#8617; &amp;#8617;2 Reinforcement Learning and Stochastic Optimization (RL Theory Book). Available at https://rltheorybook.github.io/. &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">On Provable Guarantees of Adversarial Robustness of Mixture of Experts</title>
      
      
      <link href="https://a-sircar1.github.io/2024/05/25/24-MoE-Robustness/" rel="alternate" type="text/html" title="On Provable Guarantees of Adversarial Robustness of Mixture of Experts" />
      
      <published>2024-05-25T06:10:56+00:00</published>
      <updated>2024-05-25T06:10:56+00:00</updated>
      <id>https://a-sircar1.github.io/2024/05/25/24-MoE-Robustness</id>
      <content type="html" xml:base="https://a-sircar1.github.io/2024/05/25/24-MoE-Robustness/">&lt;p&gt;Check out this recent paper I wrote analyzing the robustness of the individual sub-models that determined via a routing architecture, following the general Mixture of Experts (MoE) framework. I’ve been thinking about the benefits sparsity has to offer from multiple lenses, including robustness and interpretability, and I believe the comparison of MoE models with the same number of total parameters as their dense counterparts offer the perfect setting for sparsity studies. This paper is, hopefully, the first in a line of studies I’d like to conduct into the benefits of sparsity.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With its strong performances in accuracy and efficiency, the Mixture of Experts (MoE) paradigm has become extremely popular in several settings, including vision and language-modeling. Although there are several empirical works detailing the accomplishments of numerous variants and applications of MoE, relatively few studies have been conducted in establishing theoretical guarantees for this class of models. Furthermore, the notion of having several independent expert sub-modules perform inference on specialized portions of an input space assigned by a routing mechanism demands an analysis of the robustness of such a procedure. Notably, adversarial robustness is a key property desired in any kind of predictive model. In this work, we specifically wonder whether the MoE architecture lends itself to classical adversarial defense techniques. To this extent, we develop the &lt;em&gt;robust MoE classifier&lt;/em&gt; that takes advantage of randomized smoothing to increase adversarial robustness. We theoretically show that the robust MoE classifier can achieve lower Lipschitz constants than dense, standard neural network counterparts that implement randomized smoothing. Furthermore, we provide a provable bound on the size of the \(\ell_2\)-norm ball in which our algorithm will be robust. Next, we perform an empirical study of the performance of our model compared to several baseline models in the presence of adversarial attacks and discuss a modified FGSM attack against models that exhibit the MoE architecture. Finally, we end with a discussion on the potential for theoretical guarantees to be made regarding the robustness of MoE given datasets that exhibit specific features.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Arnab Sircar</name>
          
          
        </author>
      

      
        <category term="machine-learning" />
      

      

      
        <summary type="html">Check out this recent paper I wrote analyzing the robustness of the individual sub-models that determined via a routing architecture, following the general Mixture of Experts (MoE) framework. I’ve been thinking about the benefits sparsity has to offer from multiple lenses, including robustness and interpretability, and I believe the comparison of MoE models with the same number of total parameters as their dense counterparts offer the perfect setting for sparsity studies. This paper is, hopefully, the first in a line of studies I’d like to conduct into the benefits of sparsity. &amp;nbsp; Abstract With its strong performances in accuracy and efficiency, the Mixture of Experts (MoE) paradigm has become extremely popular in several settings, including vision and language-modeling. Although there are several empirical works detailing the accomplishments of numerous variants and applications of MoE, relatively few studies have been conducted in establishing theoretical guarantees for this class of models. Furthermore, the notion of having several independent expert sub-modules perform inference on specialized portions of an input space assigned by a routing mechanism demands an analysis of the robustness of such a procedure. Notably, adversarial robustness is a key property desired in any kind of predictive model. In this work, we specifically wonder whether the MoE architecture lends itself to classical adversarial defense techniques. To this extent, we develop the robust MoE classifier that takes advantage of randomized smoothing to increase adversarial robustness. We theoretically show that the robust MoE classifier can achieve lower Lipschitz constants than dense, standard neural network counterparts that implement randomized smoothing. Furthermore, we provide a provable bound on the size of the \(\ell_2\)-norm ball in which our algorithm will be robust. Next, we perform an empirical study of the performance of our model compared to several baseline models in the presence of adversarial attacks and discuss a modified FGSM attack against models that exhibit the MoE architecture. Finally, we end with a discussion on the potential for theoretical guarantees to be made regarding the robustness of MoE given datasets that exhibit specific features.</summary>
      

      
      
    </entry>
  
  
</feed>
